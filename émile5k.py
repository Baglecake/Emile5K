# -*- coding: utf-8 -*-
"""Émile5K.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19jDcUsvXJKvw7HwEgPYV7MqeL4NshysU

# 1. Utilities
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile utilities.py
# """
# Utilities and Helper Functions for Émile-2 Simulation
# ----------------------------------------------------
# Core constants and utility functions used throughout the simulation.
# """
# import logging
# import math
# import numpy as np
# import torch
# import torch.nn.functional as F
# import traceback
# from typing import Union, List, Dict, Optional, Tuple, Any
# from qiskit import QuantumCircuit
# from qiskit_aer import AerSimulator
# from qiskit.quantum_info import Statevector
# from qiskit_aer.library import SaveStatevector
# 
# # Configure logging
# logging.basicConfig(
#     level=logging.INFO,
#     format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
# )
# logger = logging.getLogger("emile4.utilities")
# 
# # ==========================
# # Global Constants
# # ==========================
# # Core simulation parameters
# NUM_QUBITS_PER_AGENT = 4
# DECOHERENCE_RATE = 0.01
# MINIMUM_COHERENCE_FLOOR = 0.0001
# MOMENTUM_DECAY = 0.7
# DISTINCTION_ANCHOR_WEIGHT = 0.2
# 
# # Surplus and distinction parameters
# SURPLUS_ADJUSTMENT_RATE = 0.05
# MAX_SURPLUS = 10.0
# EXPULSION_RECOVERY_RATE = 0.02
# SURPLUS_THRESHOLD = 1.5
# TARGET_DISTINCTION = 0.7
# PHASE_SCALING_FACTOR = 0.3
# SURPLUS_RECYCLE_FRACTION = 0.7
# COLLAPSE_DISSIPATION_THRESHOLD = 0.35
# COLLAPSE_DISSIPATION_RATE = 0.02  # Added missing constant
# CORE_DISTINCTION_UPDATE_RATE = 0.01
# INSTABILITY_GRACE_PERIOD = 3
# 
# # Learning and training parameters
# LEARNING_RATE = 1e-3
# LEARNING_RATE_MIN = 1e-5
# LEARNING_RATE_MAX = 1e-3
# WEIGHT_DECAY = 0.005
# GRADIENT_CLIP_VALUE = 1.0
# REWARD_SCALING = 1.5
# EVOLUTION_TIME = 0.1
# 
# # QPE (Quantum Phase Estimation) precision
# QPE_PRECISION_QUBITS = 4
# 
# # Maximum entropy placeholder
# MAX_ENTROPY = np.log(2 ** NUM_QUBITS_PER_AGENT)
# 
# # Device configuration
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 
# # Transformer parameters
# HIDDEN_DIM = 20  # Added missing constant
# NUM_TRANSFORMER_HEADS = 4  # Added missing constant
# NUM_TRANSFORMER_LAYERS = 2  # Added missing constant
# 
# # ==========================
# # Custom Exceptions
# # ==========================
# class QuantumCircuitError(Exception):
#     """Error during quantum circuit initialization or operation."""
#     pass
# 
# class TensorShapeError(Exception):
#     """Error when tensor shapes don't match expected dimensions."""
#     pass
# 
# class ValueConversionError(Exception):
#     """Error when converting between numeric types."""
#     pass
# 
# # ==========================
# # Utility Functions
# # ==========================
# 
# def _initialize_circuit(num_qubits: int) -> Tuple[QuantumCircuit, Optional[AerSimulator]]:
#     """Initialize quantum circuit and simulator."""
#     try:
#         logger.info(f"Initializing quantum circuit with {num_qubits} qubits")
#         qc = QuantumCircuit(num_qubits, name="agent_circuit")
# 
#         try:
#             simulator = AerSimulator(method='statevector')
#         except Exception as sim_err:
#             logger.warning(f"Error initializing AerSimulator: {sim_err}. Falling back to default.")
#             simulator = None
# 
#         logger.info("Quantum circuit and simulator initialized successfully.")
#         return qc, simulator
#     except Exception as e:
#         logger.error(f"Error in _initialize_circuit: {e}")
#         traceback.print_exc()
#         raise QuantumCircuitError(f"Failed to initialize quantum circuit: {str(e)}")
# 
# def ensure_real(value: Any, default: float = 0.0) -> float:
#     """
#     Convert value to real float, handling complex numbers and arrays.
# 
#     Args:
#         value: Value to convert
#         default: Default value if conversion fails
# 
#     Returns:
#         Real float value
#     """
#     try:
#         if value is None:
#             return default
#         if isinstance(value, (complex, np.complex64, np.complex128)):
#             return float(np.real(value))
#         elif isinstance(value, (list, np.ndarray)):
#             if len(value) == 0:
#                 return default
#             return float(np.real(value[0]))
#         return float(value)
#     except (TypeError, ValueError, IndexError) as e:
#         logger.debug(f"Value conversion error: {e}, using default {default}")
#         return default
# 
# def adapt_tensor_shape(x: torch.Tensor, expected_dim: int = 3,
#                        expected_last_dim: int = 20) -> torch.Tensor:
#     """
#     Adapt tensor shape to expected dimensions.
# 
#     Removes extra singleton dimensions (except batch) and ensures the last
#     dimension matches expected_last_dim by padding or trimming.
# 
#     Args:
#         x: Input tensor
#         expected_dim: Expected number of dimensions
#         expected_last_dim: Expected size of last dimension
# 
#     Returns:
#         Tensor with appropriate shape
#     """
#     try:
#         # If x is a scalar, unsqueeze it
#         if x.dim() == 0:
#             x = x.unsqueeze(0)
# 
#         # Save original batch size
#         batch_size = x.size(0)
# 
#         # Squeeze dimensions beyond the batch dimension
#         new_shape = [batch_size]
#         for i in range(1, x.dim()):
#             if x.size(i) != 1:
#                 new_shape.append(x.size(i))
# 
#         # If nothing remains after batch dimension, add sequence dimension of size 1
#         if len(new_shape) == 1:
#             new_shape.append(1)
# 
#         # Try to reshape the tensor
#         try:
#             x = x.view(*new_shape)
#         except RuntimeError as e:
#             logger.warning(f"Error reshaping tensor from {x.shape} to {new_shape}: {e}")
#             # If view fails, try reshape as a fallback
#             try:
#                 x = x.reshape(batch_size, -1)
#             except RuntimeError:
#                 pass
# 
#         # Ensure tensor is n-dimensional
#         if x.dim() == 2:
#             # If it is [B, feature], add a sequence dimension
#             x = x.unsqueeze(1)
#         elif x.dim() > expected_dim:
#             # If more than expected dims, collapse dims 1 to n-1 into one
#             shape = x.shape
#             x = x.view(shape[0], -1, shape[-1])
# 
#         # Ensure last dimension equals expected_last_dim
#         current_last_dim = x.size(-1)
#         if current_last_dim < expected_last_dim:
#             padding = (0, expected_last_dim - current_last_dim)
#             x = F.pad(x, padding)
#         elif current_last_dim > expected_last_dim:
#             x = x[..., :expected_last_dim]
# 
#         return x
#     except Exception as e:
#         logger.error(f"Failed to adapt tensor shape: {e}")
#         raise TensorShapeError(f"Failed to adapt tensor shape: {str(e)}")
# 
# def update_momentum(old_value: float, new_sample: float,
#                    decay: float = MOMENTUM_DECAY) -> float:
#     """
#     Update value using exponential moving average.
# 
#     Args:
#         old_value: Previous momentum value
#         new_sample: New sample value
#         decay: Momentum decay factor (default: MOMENTUM_DECAY)
# 
#     Returns:
#         Updated momentum value
#     """
#     return float(decay * old_value + (1 - decay) * new_sample)
# 
# def to_float(x: Union[float, int, np.number, torch.Tensor],
#             default: float = 0.0) -> float:
#     """
#     Convert various numeric types to float.
# 
#     Args:
#         x: Value to convert
#         default: Default value if conversion fails
# 
#     Returns:
#         Float value
#     """
#     try:
#         if isinstance(x, torch.Tensor):
#             return float(x.detach().cpu().item())
#         return float(x)
#     except (TypeError, ValueError) as e:
#         logger.debug(f"Float conversion error: {e}, using default {default}")
#         return default
# 
# def compute_phase_coherence(phases: Optional[List[float]] = None,
#                            default: float = MINIMUM_COHERENCE_FLOOR) -> float:
#     """
#     Compute phase coherence from a list of phases.
# 
#     Args:
#         phases: List of phase values
#         default: Default value if computation fails
# 
#     Returns:
#         Phase coherence value in range [MINIMUM_COHERENCE_FLOOR, 1.0]
#     """
#     if not isinstance(phases, list) or not phases:
#         logger.debug("Invalid input to compute_phase_coherence, using default")
#         return default
# 
#     try:
#         # Filter and convert values to float
#         cleaned = np.array([to_float(p) for p in phases if isinstance(p, (int, float))])
#         if cleaned.size == 0:
#             return default
# 
#         # Compute coherence using complex phase representation
#         complex_phases = np.exp(1j * cleaned)
#         coherence = float(np.abs(np.mean(complex_phases)))
#         return max(coherence, MINIMUM_COHERENCE_FLOOR)
#     except Exception as e:
#         logger.warning(f"Error in compute_phase_coherence: {e}")
#         return default
# 
# def compute_normalized_entropy(probabilities: Union[List[float], np.ndarray]) -> float:
#     """
#     Compute normalized entropy from probability distribution.
# 
#     Args:
#         probabilities: Probability distribution
# 
#     Returns:
#         Normalized entropy in range [0, 1]
#     """
#     try:
#         from scipy.stats import entropy
# 
#         # Convert to numpy array and normalize
#         probabilities = np.array(probabilities, dtype=np.float64)
# 
#         # Handle potential NaN or infinite values
#         probabilities = np.nan_to_num(probabilities, nan=0.0, posinf=0.0, neginf=0.0)
# 
#         # Check for empty or all-zero array
#         if probabilities.size == 0 or np.sum(probabilities) <= 1e-10:
#             return 1.0
# 
#         # Normalize probabilities safely
#         probabilities = probabilities / max(np.sum(probabilities), 1e-10)
# 
#         # Calculate entropy and normalize
#         entropy_value = float(entropy(probabilities, base=2))
#         max_possible_entropy = float(np.log2(len(probabilities))) if len(probabilities) > 0 else 1.0
# 
#         # Avoid division by zero
#         if max_possible_entropy < 1e-10:
#             return 1.0
# 
#         return entropy_value / max_possible_entropy
#     except Exception as e:
#         logger.warning(f"Error computing normalized entropy: {e}")
#         return 1.0  # Return maximum entropy on error
# 
# def compute_context_similarity(ctx1: Dict, ctx2: Dict) -> float:
#     """
#     Compute cosine similarity between context dictionaries.
# 
#     Args:
#         ctx1: First context dictionary
#         ctx2: Second context dictionary
# 
#     Returns:
#         Similarity score between 0 and 1
#     """
#     try:
#         common_keys = set(ctx1.keys()) & set(ctx2.keys())
#         if not common_keys:
#             return 0.0
# 
#         vec1, vec2 = [], []
#         for key in common_keys:
#             try:
#                 vec1.append(to_float(ctx1[key]))
#                 vec2.append(to_float(ctx2[key]))
#             except Exception:
#                 continue
# 
#         if not vec1:
#             return 0.0
# 
#         v1 = np.array(vec1)
#         v2 = np.array(vec2)
# 
#         norm1 = np.linalg.norm(v1)
#         norm2 = np.linalg.norm(v2)
# 
#         if norm1 == 0 or norm2 == 0:
#             return 0.0
# 
#         cosine_sim = np.dot(v1, v2) / (norm1 * norm2)
#         similarity = (cosine_sim + 1) / 2.0
# 
#         # Weight by proportion of common keys
#         total_keys = max(len(ctx1), len(ctx2))
#         key_ratio = len(common_keys) / total_keys if total_keys > 0 else 1.0
# 
#         return similarity * key_ratio
#     except Exception as e:
#         logger.warning(f"Error in compute_context_similarity: {e}")
#         return 0.0
# 
# def local_operator(i: int, n: int, op: np.ndarray) -> np.ndarray:
#     """
#     Create local quantum operator on specified qubit.
# 
#     Args:
#         i: Target qubit index
#         n: Total number of qubits
#         op: Single-qubit operator (2x2 matrix)
# 
#     Returns:
#         Full n-qubit operator
#     """
#     if i < 0 or i >= n:
#         raise ValueError(f"Qubit index {i} out of range [0, {n-1}]")
# 
#     ops = [np.eye(2) for _ in range(n)]
#     ops[i] = op
# 
#     result = ops[0]
#     for k in range(1, n):
#         result = np.kron(result, ops[k])
# 
#     return result
# 
# def two_qubit_operator(i: int, j: int, n: int, op: np.ndarray) -> np.ndarray:
#     """
#     Create two-qubit operator acting on specified qubits.
# 
#     Args:
#         i: First qubit index
#         j: Second qubit index
#         n: Total number of qubits
#         op: Two-qubit operator (4x4 matrix)
# 
#     Returns:
#         Full n-qubit operator
#     """
#     if i < 0 or i >= n or j < 0 or j >= n or i == j:
#         raise ValueError(f"Invalid qubit indices: i={i}, j={j}, n={n}")
#     if op.shape != (4, 4):
#         raise ValueError(f"Operator must be 4x4, got {op.shape}")
# 
#     # Create n-qubit identity operator
#     result = np.eye(2**n)
# 
#     # Calculate the indices for the 2-qubit subspace
#     # This is a more general approach than the original function
#     subspace_indices = []
#     for k in range(2**n):
#         binary = format(k, f'0{n}b')
#         if (i < j and binary[i] + binary[j] in ['00', '01', '10', '11']) or \
#            (i > j and binary[j] + binary[i] in ['00', '01', '10', '11']):
#             subspace_indices.append(k)
# 
#     # For each pair of basis states in the 2-qubit subspace
#     for ii, idx1 in enumerate(subspace_indices):
#         for jj, idx2 in enumerate(subspace_indices):
#             # Apply the corresponding element of the 2-qubit operator
#             result[idx1, idx2] = op[ii, jj]
# 
#     return result
# 
# def reshape_input_tensor(tensor: torch.Tensor, expected_shape: Tuple[int, ...]) -> torch.Tensor:
#     """
#     Reshape tensor to expected shape, with graceful fallback.
# 
#     Args:
#         tensor: Input tensor
#         expected_shape: Expected tensor shape
# 
#     Returns:
#         Reshaped tensor
#     """
#     try:
#         # Try direct view operation first
#         return tensor.view(expected_shape)
#     except RuntimeError:
#         logger.warning(f"Cannot view tensor from {tensor.shape} to {expected_shape}")
# 
#         # If element count matches, try reshape
#         if tensor.numel() == np.prod(expected_shape):
#             try:
#                 return tensor.reshape(expected_shape)
#             except RuntimeError as e:
#                 logger.warning(f"Reshape failed: {e}")
# 
#         # Try to preserve batch dimension and final dimension
#         try:
#             return tensor.view(-1, expected_shape[-1])
#         except RuntimeError:
#             logger.warning(f"Failed to reshape tensor to {expected_shape}")
# 
#         # Return original tensor as fallback
#         return tensor

"""# 2. Data Classes"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile data_classes.py
# 
# """
# Data Classes for Émile-2 Simulation
# -----------------------------------
# Core data structures that encapsulate state and ensure type safety.
# """
# import logging
# import torch
# import numpy as np
# from dataclasses import dataclass, field
# from typing import Dict, List, Optional, Union, Any
# import traceback
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.data_classes")
# 
# # Import necessary constants
# from utilities import MINIMUM_COHERENCE_FLOOR
# 
# 
# @dataclass
# class SurplusState:
#     """Container for surplus state information with stability tracking."""
#     values: Dict[str, float] = field(
#         default_factory=lambda: {'basal': 1.0, 'cognitive': 1.0, 'predictive': 1.0, 'ontological': 1.0}
#     )
#     accumulation_rate: Dict[str, float] = field(
#         default_factory=lambda: {'basal': 0.01, 'cognitive': 0.01, 'predictive': 0.01, 'ontological': 0.01}
#     )
#     stability: float = 1.0
#     quantum_coupling: float = 1.0
#     stability_momentum: float = 0.0
#     last_expulsion: float = 0.0
#     recycled_surplus: Dict[str, float] = field(default_factory=dict)
# 
#     def __post_init__(self):
#         """Validate and initialize all values."""
#         # Required keys for complete state
#         required_keys = {'basal', 'cognitive', 'predictive', 'ontological'}
# 
#         # Ensure all required keys exist in values and accumulation_rate
#         for key in required_keys:
#             if key not in self.values:
#                 logger.warning(f"Missing required key '{key}' in values, initializing to 1.0")
#                 self.values[key] = 1.0
#             if key not in self.accumulation_rate:
#                 logger.warning(f"Missing required key '{key}' in accumulation_rate, initializing to 0.01")
#                 self.accumulation_rate[key] = 0.01
# 
#         # Ensure all values are floats
#         try:
#             self.values = {k: float(v) for k, v in self.values.items()}
#         except (TypeError, ValueError) as e:
#             logger.error(f"Error converting values to float: {e}")
#             self.values = {k: 1.0 for k in required_keys}
# 
#         try:
#             self.accumulation_rate = {k: float(v) for k, v in self.accumulation_rate.items()}
#         except (TypeError, ValueError) as e:
#             logger.error(f"Error converting accumulation_rate to float: {e}")
#             self.accumulation_rate = {k: 0.01 for k in required_keys}
# 
#         # Ensure numeric values are floats
#         try:
#             self.stability = float(self.stability)
#         except (TypeError, ValueError):
#             logger.warning("Invalid stability value, defaulting to 1.0")
#             self.stability = 1.0
# 
#         try:
#             self.quantum_coupling = float(self.quantum_coupling)
#         except (TypeError, ValueError):
#             logger.warning("Invalid quantum_coupling value, defaulting to 1.0")
#             self.quantum_coupling = 1.0
# 
#         try:
#             self.stability_momentum = float(self.stability_momentum)
#         except (TypeError, ValueError):
#             logger.warning("Invalid stability_momentum value, defaulting to 0.0")
#             self.stability_momentum = 0.0
# 
#         try:
#             self.last_expulsion = float(self.last_expulsion)
#         except (TypeError, ValueError):
#             logger.warning("Invalid last_expulsion value, defaulting to 0.0")
#             self.last_expulsion = 0.0
# 
#         # Ensure recycled_surplus is a dictionary with float values
#         if not isinstance(self.recycled_surplus, dict):
#             logger.warning("Invalid recycled_surplus type, defaulting to empty dict")
#             self.recycled_surplus = {}
#         else:
#             try:
#                 self.recycled_surplus = {k: float(v) for k, v in self.recycled_surplus.items()}
#             except (TypeError, ValueError):
#                 logger.warning("Error converting recycled_surplus values to float, using empty dict")
#                 self.recycled_surplus = {}
# 
#     def validate(self) -> bool:
#         """Validate SurplusState fields."""
#         try:
#             # Validate types
#             if not isinstance(self.values, dict):
#                 logger.error("SurplusState.values is not a dict")
#                 return False
#             if not isinstance(self.accumulation_rate, dict):
#                 logger.error("SurplusState.accumulation_rate is not a dict")
#                 return False
#             if not isinstance(self.stability, float):
#                 logger.error(f"SurplusState.stability is not a float: {type(self.stability)}")
#                 return False
#             if not isinstance(self.quantum_coupling, float):
#                 logger.error(f"SurplusState.quantum_coupling is not a float: {type(self.quantum_coupling)}")
#                 return False
# 
#             # Validate required keys
#             required_keys = {'basal', 'cognitive', 'predictive', 'ontological'}
#             for key in required_keys:
#                 if key not in self.values:
#                     logger.error(f"SurplusState missing key: {key}")
#                     return False
#                 if not isinstance(self.values[key], (int, float)):
#                     logger.error(f"Invalid type for {key}: {type(self.values[key])}")
#                     return False
# 
#             # Validate value ranges (optional, based on domain knowledge)
#             for key, value in self.values.items():
#                 if value < 0:
#                     logger.warning(f"Negative value for {key}: {value}")
# 
#             if self.stability < 0 or self.stability > 1.0:
#                 logger.warning(f"Stability outside [0,1] range: {self.stability}")
# 
#             if self.quantum_coupling < 0 or self.quantum_coupling > 1.0:
#                 logger.warning(f"Quantum coupling outside [0,1] range: {self.quantum_coupling}")
# 
#             return True
#         except Exception as e:
#             logger.error(f"Error in SurplusState validation: {e}")
#             return False
# 
#     def copy(self) -> 'SurplusState':
#         """Create a deep copy of the surplus state with proper type handling."""
#         try:
#             # Handle values dictionary
#             new_values = {}
#             for k, v in self.values.items():
#                 if callable(v):  # Skip if it's a method
#                     continue
#                 try:
#                     new_values[k] = float(v)
#                 except (TypeError, ValueError):
#                     logger.warning(f"Invalid value for {k}, using default")
#                     new_values[k] = 1.0
# 
#             # Handle accumulation rates
#             new_accumulation_rate = {}
#             for k, v in self.accumulation_rate.items():
#                 if callable(v):  # Skip if it's a method
#                     continue
#                 try:
#                     new_accumulation_rate[k] = float(v)
#                 except (TypeError, ValueError):
#                     logger.warning(f"Invalid accumulation rate for {k}, using default")
#                     new_accumulation_rate[k] = 0.01
# 
#             # Handle recycled surplus
#             new_recycled_surplus = {}
#             for k, v in self.recycled_surplus.items():
#                 if callable(v):  # Skip if it's a method
#                     continue
#                 try:
#                     new_recycled_surplus[k] = float(v)
#                 except (TypeError, ValueError):
#                     logger.warning(f"Invalid recycled surplus for {k}, using default")
#                     new_recycled_surplus[k] = 0.0
# 
#             # Create new instance with properly typed values
#             return SurplusState(
#                 values=new_values,
#                 accumulation_rate=new_accumulation_rate,
#                 stability=float(self.stability) if not callable(self.stability) else 1.0,
#                 quantum_coupling=float(self.quantum_coupling) if not callable(self.quantum_coupling) else 1.0,
#                 stability_momentum=float(self.stability_momentum) if not callable(self.stability_momentum) else 0.0,
#                 last_expulsion=float(self.last_expulsion) if not callable(self.last_expulsion) else 0.0,
#                 recycled_surplus=new_recycled_surplus
#             )
# 
#         except Exception as e:
#             logger.error(f"Error copying SurplusState: {e}")
#             # Return a new default instance if copying fails
#             return SurplusState()
# 
#     def total_surplus(self) -> float:
#         """Calculate total surplus with proper type handling."""
#         try:
#             if not isinstance(self.values, dict):
#                 logger.warning("Values is not a dictionary")
#                 return 0.0
# 
#             total = 0.0
#             for key, value in self.values.items():
#                 try:
#                     total += float(value)
#                 except (TypeError, ValueError):
#                     logger.warning(f"Could not convert value for {key} to float")
# 
#             return total
#         except Exception as e:
#             logger.error(f"Error calculating total surplus: {e}")
#             return 0.0
# 
# 
# @dataclass
# class TransformerOutput:
#     """Container for transformer outputs with quantum-aware processing."""
#     prediction: torch.Tensor
#     phase_prediction: Optional[torch.Tensor] = None
#     value_estimate: Optional[torch.Tensor] = None
#     attention_weights: Dict[str, torch.Tensor] = field(default_factory=dict)
#     entropy: Optional[torch.Tensor] = None
#     coherence_estimate: Optional[torch.Tensor] = None
# 
#     def __post_init__(self):
#         """Ensure all tensors are properly initialized and on correct device."""
#         try:
#             # Ensure prediction is a tensor
#             if not isinstance(self.prediction, torch.Tensor):
#                 logger.warning(f"Prediction is not a tensor, converting from {type(self.prediction)}")
#                 try:
#                     self.prediction = torch.tensor(self.prediction, dtype=torch.float32)
#                 except Exception as e:
#                     logger.error(f"Could not convert prediction to tensor: {e}")
#                     self.prediction = torch.tensor(0.0)
# 
#             # Get device from prediction tensor
#             device = self.prediction.device
# 
#             # Handle phase prediction
#             if self.phase_prediction is None:
#                 self.phase_prediction = torch.zeros_like(self.prediction)
#             elif not isinstance(self.phase_prediction, torch.Tensor):
#                 try:
#                     self.phase_prediction = torch.tensor(self.phase_prediction, device=device)
#                 except Exception as e:
#                     logger.error(f"Could not convert phase_prediction to tensor: {e}")
#                     self.phase_prediction = torch.zeros_like(self.prediction)
# 
#             # Handle value estimate
#             if self.value_estimate is None:
#                 self.value_estimate = torch.zeros_like(self.prediction)
#             elif not isinstance(self.value_estimate, torch.Tensor):
#                 try:
#                     self.value_estimate = torch.tensor(self.value_estimate, device=device)
#                 except Exception as e:
#                     logger.error(f"Could not convert value_estimate to tensor: {e}")
#                     self.value_estimate = torch.zeros_like(self.prediction)
# 
#             # Handle entropy
#             if self.entropy is None:
#                 self.entropy = torch.tensor(0.0, device=device)
#             elif not isinstance(self.entropy, torch.Tensor):
#                 try:
#                     self.entropy = torch.tensor(self.entropy, device=device)
#                 except Exception as e:
#                     logger.error(f"Could not convert entropy to tensor: {e}")
#                     self.entropy = torch.tensor(0.0, device=device)
# 
#             # Handle coherence estimate
#             if self.coherence_estimate is None:
#                 self.coherence_estimate = torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
#             elif not isinstance(self.coherence_estimate, torch.Tensor):
#                 try:
#                     self.coherence_estimate = torch.tensor(self.coherence_estimate, device=device)
#                 except Exception as e:
#                     logger.error(f"Could not convert coherence_estimate to tensor: {e}")
#                     self.coherence_estimate = torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
# 
#             # Ensure attention weights are proper tensors
#             if not isinstance(self.attention_weights, dict):
#                 logger.warning(f"Attention weights is not a dict, initializing empty dict")
#                 self.attention_weights = {}
#             else:
#                 for key, value in list(self.attention_weights.items()):
#                     if not isinstance(value, torch.Tensor):
#                         try:
#                             self.attention_weights[key] = torch.tensor(value, device=device)
#                         except Exception as e:
#                             logger.error(f"Could not convert attention weight {key} to tensor: {e}")
#                             del self.attention_weights[key]
# 
#         except Exception as e:
#             logger.error(f"Error in TransformerOutput initialization: {e}")
#             # Set safe default values, ensuring device is preserved
#             device = getattr(self.prediction, 'device', None)
#             if device is None:
#                 device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 
#             self.prediction = torch.tensor(0.0, device=device)
#             self.phase_prediction = torch.tensor(0.0, device=device)
#             self.value_estimate = torch.tensor(0.0, device=device)
#             self.attention_weights = {}
#             self.entropy = torch.tensor(0.0, device=device)
#             self.coherence_estimate = torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
# 
#     def validate(self) -> bool:
#         """Validate transformer output state."""
#         try:
#             # Check prediction tensor
#             if not isinstance(self.prediction, torch.Tensor):
#                 logger.error("Invalid prediction type")
#                 return False
# 
#             # Check tensor dimensions
#             if self.prediction.dim() > 3:
#                 logger.error(f"Invalid prediction dimensions: {self.prediction.dim()}")
#                 return False
# 
#             # Check for NaN values
#             if torch.isnan(self.prediction).any():
#                 logger.error("NaN values in prediction")
#                 return False
# 
#             # Check phase prediction
#             if self.phase_prediction is not None:
#                 if not isinstance(self.phase_prediction, torch.Tensor):
#                     logger.error("Invalid phase prediction type")
#                     return False
#                 if torch.isnan(self.phase_prediction).any():
#                     logger.error("NaN values in phase prediction")
#                     return False
# 
#             # Check value estimate
#             if self.value_estimate is not None:
#                 if not isinstance(self.value_estimate, torch.Tensor):
#                     logger.error("Invalid value estimate type")
#                     return False
#                 if torch.isnan(self.value_estimate).any():
#                     logger.error("NaN values in value estimate")
#                     return False
# 
#             # Check attention weights
#             if not isinstance(self.attention_weights, dict):
#                 logger.error("Invalid attention weights type")
#                 return False
# 
#             for key, value in self.attention_weights.items():
#                 if not isinstance(value, torch.Tensor):
#                     logger.error(f"Invalid attention weight tensor for {key}")
#                     return False
#                 if torch.isnan(value).any():
#                     logger.error(f"NaN values in attention weights for {key}")
#                     return False
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error validating transformer output: {e}")
#             return False
# 
#     @property
#     def device(self) -> torch.device:
#         """Get the device of the prediction tensor."""
#         return self.prediction.device
# 
#     def to(self, device: torch.device) -> 'TransformerOutput':
#         """Move all tensors to specified device."""
#         try:
#             self.prediction = self.prediction.to(device)
# 
#             if self.phase_prediction is not None:
#                 self.phase_prediction = self.phase_prediction.to(device)
# 
#             if self.value_estimate is not None:
#                 self.value_estimate = self.value_estimate.to(device)
# 
#             if self.entropy is not None:
#                 self.entropy = self.entropy.to(device)
# 
#             if self.coherence_estimate is not None:
#                 self.coherence_estimate = self.coherence_estimate.to(device)
# 
#             # Move attention weights
#             for k, v in self.attention_weights.items():
#                 if isinstance(v, torch.Tensor):
#                     self.attention_weights[k] = v.to(device)
# 
#             return self
# 
#         except Exception as e:
#             logger.error(f"Error moving tensors to device: {e}")
#             return self
# 
#     def get_prediction_value(self) -> float:
#         """Safely extract prediction value as float."""
#         try:
#             if self.prediction is None:
#                 return 0.0
# 
#             # Handle different tensor shapes
#             if self.prediction.dim() == 0:  # Scalar
#                 return self.prediction.item()
#             elif self.prediction.dim() == 1:  # Vector
#                 return self.prediction[0].item()
#             elif self.prediction.dim() == 2:  # Matrix
#                 return self.prediction[0, 0].item()
#             elif self.prediction.dim() == 3:  # 3D tensor
#                 return self.prediction[0, 0, 0].item()
#             else:
#                 return self.prediction.mean().item()
#         except Exception as e:
#             logger.error(f"Error extracting prediction value: {e}")
#             return 0.0

"""# 3. Base Quantum"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile base_quantum.py
# """
# Base Quantum Module for Émile-2 Simulation
# ------------------------------------------
# Foundational quantum state functionality for the simulation.
# """
# import logging
# import numpy as np
# from typing import Dict, Optional
# from qiskit import QuantumCircuit
# from qiskit_aer import AerSimulator
# from qiskit.quantum_info import Statevector
# from qiskit_aer.library import SaveStatevector
# import traceback
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.base_quantum")
# 
# # Import necessary constants
# from utilities import MINIMUM_COHERENCE_FLOOR
# 
# class BaseQuantumState:
#     """
#     Base class for quantum state handling.
# 
#     Provides basic initialization and metrics calculation for quantum states.
#     """
#     def __init__(self, num_qubits: int = 4):
#         """
#         Initialize quantum state with the specified number of qubits.
# 
#         Args:
#             num_qubits: Number of qubits in the system
#         """
#         self.num_qubits = num_qubits
#         self.minimum_coherence = MINIMUM_COHERENCE_FLOOR
# 
#         # Initialize quantum circuit
#         self.qc = QuantumCircuit(num_qubits)
#         logger.debug(f"Initialized quantum circuit with {num_qubits} qubits")
# 
#         # Initialize simulator
#         try:
#             self.simulator = AerSimulator(method='statevector')
#             logger.debug("AerSimulator initialized successfully")
#         except Exception as e:
#             logger.warning(f"Error initializing simulator: {e}")
#             self.simulator = None
# 
#         # Initialize statevector
#         try:
#             self.statevector = Statevector.from_label('0' * num_qubits)
#             logger.debug(f"Statevector initialized to |{'0' * num_qubits}⟩")
#         except Exception as e:
#             logger.error(f"Error initializing statevector: {e}")
#             self.statevector = None
# 
#         # Set initial phase and metrics
#         self.phase = 0.0
#         self.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#     def get_basic_metrics(self) -> Dict[str, float]:
#         """
#         Calculate basic quantum metrics from the current state.
# 
#         Returns:
#             Dictionary containing phase, phase_coherence, and normalized_entropy
#         """
#         try:
#             # Handle invalid statevector
#             if self.statevector is None:
#                 logger.warning("Statevector is None, returning default metrics")
#                 return {
#                     'phase': 0.0,
#                     'phase_coherence': self.minimum_coherence,
#                     'normalized_entropy': 0.0
#                 }
# 
#             # Extract probabilities from statevector
#             if isinstance(self.statevector, np.ndarray):
#                 probs = np.abs(self.statevector) ** 2
#             else:
#                 probs = np.abs(np.array(self.statevector.data)) ** 2
# 
#             # Calculate entropy
#             entropy = self._calculate_entropy(probs)
#             max_entropy = np.log2(len(probs))
#             normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0
# 
#             # Calculate coherence as complement of normalized entropy
#             coherence = max(self.minimum_coherence, 1.0 - normalized_entropy)
# 
#             return {
#                 'phase': float(self.phase),
#                 'phase_coherence': float(coherence),
#                 'normalized_entropy': float(normalized_entropy),
#                 'probabilities': probs.tolist()  # Add probabilities for advanced analysis
#             }
# 
#         except Exception as e:
#             logger.error(f"Error getting basic metrics: {e}")
#             return {
#                 'phase': 0.0,
#                 'phase_coherence': self.minimum_coherence,
#                 'normalized_entropy': 0.0
#             }
# 
#     def _calculate_entropy(self, probabilities: np.ndarray) -> float:
#         """
#         Calculate the von Neumann entropy from probability distribution.
# 
#         Args:
#             probabilities: Probability distribution from statevector
# 
#         Returns:
#             Entropy value
#         """
#         try:
#             # Add small epsilon to avoid log(0)
#             epsilon = np.finfo(float).eps
#             probabilities = probabilities + epsilon
#             probabilities = probabilities / np.sum(probabilities)  # Renormalize
# 
#             # Calculate entropy
#             entropy = -np.sum(probabilities * np.log2(probabilities))
#             return float(entropy)
# 
#         except Exception as e:
#             logger.error(f"Error calculating entropy: {e}")
#             return 0.0
# 
#     def execute_circuit(self, circuit: QuantumCircuit, shots: int = 1024) -> Dict[str, int]:
#         """
#         Execute a quantum circuit and return measurement results.
# 
#         Args:
#             circuit: Quantum circuit to execute
#             shots: Number of measurement shots
# 
#         Returns:
#             Dictionary of measurement results
#         """
#         try:
#             if self.simulator is None:
#                 logger.error("Simulator is None, cannot execute circuit")
#                 return {}
# 
#             # Execute circuit
#             result = self.simulator.run(circuit, shots=shots).result()
# 
#             # Return counts
#             if result.success:
#                 return dict(result.get_counts())
#             else:
#                 logger.error(f"Circuit execution failed: {result.status}")
#                 return {}
# 
#         except Exception as e:
#             logger.error(f"Error executing circuit: {e}")
#             return {}
# 
#     def reset_to_ground_state(self) -> bool:
#         """
#         Reset quantum state to the ground state.
# 
#         Returns:
#             True if reset successful, False otherwise
#         """
#         try:
#             # Create new circuit and statevector
#             self.qc = QuantumCircuit(self.num_qubits)
#             self.statevector = Statevector.from_label('0' * self.num_qubits)
#             self.phase = 0.0
#             self.phase_coherence = self.minimum_coherence
# 
#             logger.info("Reset to ground state successful")
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error resetting to ground state: {e}")
#             return False
# 
#     def is_valid(self) -> bool:
#         """
#         Check if quantum state is valid and ready for operations.
# 
#         Returns:
#             True if state is valid, False otherwise
#         """
#         try:
#             # Check quantum circuit
#             if not isinstance(self.qc, QuantumCircuit):
#                 logger.error("Invalid quantum circuit")
#                 return False
# 
#             # Check statevector
#             if self.statevector is None:
#                 logger.error("Statevector is None")
#                 return False
# 
#             # Check statevector normalization
#             if isinstance(self.statevector, np.ndarray):
#                 state_data = self.statevector
#             else:
#                 state_data = np.array(self.statevector.data)
# 
#             norm = np.linalg.norm(state_data)
#             if not np.isclose(norm, 1.0, atol=1e-6):
#                 logger.error(f"Statevector not normalized: norm = {norm}")
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error validating quantum state: {e}")
#             return False
# 
#     def duplicate(self) -> Optional['BaseQuantumState']:
#         """
#         Create a copy of the current quantum state.
# 
#         Returns:
#             New BaseQuantumState instance with same values, or None if duplication fails
#         """
#         try:
#             # Create new instance
#             new_state = BaseQuantumState(self.num_qubits)
# 
#             # Copy statevector
#             if isinstance(self.statevector, np.ndarray):
#                 new_state.statevector = np.copy(self.statevector)
#             elif hasattr(self.statevector, 'copy'):
#                 new_state.statevector = self.statevector.copy()
# 
#             # Copy phase and coherence
#             new_state.phase = self.phase
#             new_state.phase_coherence = self.phase_coherence
#             new_state.minimum_coherence = self.minimum_coherence
# 
#             return new_state
# 
#         except Exception as e:
#             logger.error(f"Error duplicating quantum state: {e}")
#             return None
# 
#

"""# 4. Core Quantum"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile core_quantum.py
# """
# Core Quantum Management and State
# ---------------------------------
# Advanced quantum state management for Émile-4 simulation with enhanced
# stability and error handling.
# """
# 
# import logging
# import math
# import traceback
# import numpy as np
# import random
# from collections import deque
# from typing import Optional, Dict, Tuple, List, Any, Union
# import time
# import scipy.linalg
# 
# from qiskit import QuantumCircuit, transpile
# from qiskit.quantum_info import Statevector
# from qiskit_aer import AerSimulator
# from qiskit_aer.noise import NoiseModel, amplitude_damping_error, phase_damping_error
# from qiskit_aer.library import SaveStatevector
# 
# from base_quantum import BaseQuantumState
# from data_classes import SurplusState
# from utilities import (
#     DECOHERENCE_RATE,
#     MINIMUM_COHERENCE_FLOOR,
#     MOMENTUM_DECAY,
#     compute_phase_coherence,
#     update_momentum,
#     ensure_real
# )
# 
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.core_quantum")
# 
# 
# def create_noise_model() -> NoiseModel:
#     """
#     Create noise model for quantum simulations.
# 
#     Returns:
#         NoiseModel with amplitude and phase damping errors
#     """
#     try:
#         noise_model = NoiseModel()
#         error_amp = amplitude_damping_error(DECOHERENCE_RATE)
#         error_phase = phase_damping_error(DECOHERENCE_RATE)
# 
#         # Add errors for common gates
#         for gate in ['id', 'u1', 'u2', 'u3']:
#             noise_model.add_all_qubit_quantum_error(error_amp, [gate])
#             noise_model.add_all_qubit_quantum_error(error_phase, [gate])
# 
#         return noise_model
#     except Exception as e:
#         logger.error(f"Error creating noise model: {e}")
#         return NoiseModel()
# 
# 
# class SaveStatevectorWrapper:
#     """
#     Wrapper for SaveStatevector instruction with proper error handling.
#     """
#     def __init__(self, num_qubits: int):
#         """
#         Initialize SaveStatevector wrapper.
# 
#         Args:
#             num_qubits: Number of qubits in the system
#         """
#         self.instruction = None
#         try:
#             self.instruction = SaveStatevector(num_qubits)
#         except Exception as e:
#             logger.warning(f"Could not initialize SaveStatevector: {e}")
#             logger.warning("This may affect statevector operations")
# 
#     def apply(self, qc: QuantumCircuit) -> bool:
#         """
#         Apply SaveStatevector instruction to circuit.
# 
#         Args:
#             qc: Quantum circuit to apply instruction to
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         if self.instruction is None:
#             logger.warning("SaveStatevector instruction is missing. State will not be saved.")
#             return False
# 
#         try:
#             qc.append(self.instruction, qc.qubits)
#             return True
#         except Exception as e:
#             logger.error(f"Error applying SaveStatevector: {e}")
#             return False
# 
# 
# class EnhancedQuantumState(BaseQuantumState):
#     """
#     Enhanced quantum state management with improved coherence and stability.
#     """
#     def __init__(self, agent=None, num_qubits: int = 4):
#         """
#         Initialize enhanced quantum state.
# 
#         Args:
#             agent: Reference to parent agent (optional)
#             num_qubits: Number of qubits in the system
#         """
#         # Call parent initializer
#         super().__init__(num_qubits)
# 
#         # Store agent reference for callbacks
#         self.agent = agent
# 
#         # Initialize quantum parameters
#         self.phase = 0.0
#         self.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#         # Initialize tracking metrics
#         self.coherence_momentum = 0.0
#         self.coherence_history = deque(maxlen=100)
#         self.evolution_history = deque(maxlen=100)
#         self.phase_history = deque(maxlen=100)
#         self.measurement_history = deque(maxlen=100)
# 
#         # Initialize recovery tracking
#         self.recovery_attempts = 0
#         self.max_recovery_attempts = 3
# 
#         # Initialize noise model
#         try:
#             self.noise_model = create_noise_model()
#             logger.debug("Noise model initialized successfully")
#         except Exception as e:
#             logger.warning(f"Error initializing noise model: {e}")
#             self.noise_model = None
# 
#         # Ensure initial state is prepared
#         self._prepare_ground_state_with_coherence()
# 
#         # Store initial metrics
#         try:
#             initial_metrics = self.get_quantum_metrics()
#             self.coherence_history.append(initial_metrics['phase_coherence'])
#             self.phase_history.append(initial_metrics['phase'])
#             logger.debug(f"Initial metrics: coherence={initial_metrics['phase_coherence']:.4f}, phase={initial_metrics['phase']:.4f}")
#         except Exception as e:
#             logger.error(f"Error storing initial metrics: {e}")
#             # Set default values
#             self.coherence_history.append(self.phase_coherence)
#             self.phase_history.append(self.phase)
# 
#     def get_coherence_variance(self) -> float:
#         """
#         Calculate variance of recent coherence values.
# 
#         Returns:
#             Float representing variance of coherence over recent history
#         """
#         try:
#             # Ensure coherence_history exists
#             if not hasattr(self, 'coherence_history'):
#                 self.coherence_history = deque(maxlen=100)
#                 # Populate with some slightly varied initial values if empty
#                 if len(self.coherence_history) == 0:
#                     base_coherence = self.phase_coherence
#                     for _ in range(5):
#                         variation = base_coherence * (1.0 + 0.02 * (random.random() - 0.5))
#                         self.coherence_history.append(variation)
# 
#             # Calculate variance with minimum sample size check
#             if len(self.coherence_history) >= 2:
#                 coherence_variance = float(np.var(list(self.coherence_history)))
# 
#                 # Add small noise to prevent exactly zero variance
#                 if coherence_variance < 1e-6:
#                     coherence_variance = 1e-6 + np.random.random() * 1e-5
# 
#                 return coherence_variance
#             else:
#                 # Default non-zero value if not enough history
#                 return 0.001
# 
#         except Exception as e:
#             logger.error(f"Error calculating coherence variance: {e}")
#             return 0.001  # Safe default
# 
#     def ensure_minimum_mutation(self):
#         """Ensure the quantum state undergoes at least some mutation to prevent stagnation."""
#         try:
#             # Apply a very small random rotation to all qubits
#             for qubit in range(self.num_qubits):
#                 # Vary the rotation angle for each qubit
#                 random_angle = (0.01 + 0.05 * random.random()) * np.pi
#                 # Randomly choose rotation axis (rx, ry, or rz)
#                 rotation_choice = random.choice(['rx', 'ry', 'rz'])
# 
#                 if rotation_choice == 'rx':
#                     self.quantum_state.apply_gate('rx', [qubit], {'theta': random_angle})
#                 elif rotation_choice == 'ry':
#                     # Use rz with equivalent params if ry not available
#                     self.quantum_state.apply_gate('rz', [qubit], {'phi': random_angle})
#                 else:
#                     self.quantum_state.apply_gate('rz', [qubit], {'phi': random_angle})
# 
#             # Add a small amount of phase shift that varies each time
#             phase_shift = 0.02 * np.pi * (random.random() - 0.5)
#             self.quantum_state.apply_phase_shift(phase_shift)
# 
#             # Update state
#             self.quantum_state.update_state()
# 
#             # Log the mutation
#             if hasattr(self, 'logger'):
#                 self.logger.debug(f"Applied minimum quantum mutation with phase shift: {phase_shift:.4f}")
#         except Exception as e:
#             if hasattr(self, 'logger'):
#                 self.logger.error(f"Error ensuring minimum mutation: {e}")
#             else:
#                 print(f"Error ensuring minimum mutation: {e}")
# 
#     def _prepare_ground_state_with_coherence(self) -> bool:
#         """
#         Prepare initial quantum state with minimum coherence.
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             # Create new circuit
#             self.qc = QuantumCircuit(self.num_qubits)
#             logger.debug(f"New quantum circuit created with {self.num_qubits} qubits")
# 
#             # Initialize statevector if invalid
#             if self.statevector is None or not isinstance(self.statevector, (Statevector, np.ndarray)):
#                 logger.warning("Statevector is invalid, reinitializing to |0⟩ state")
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#             # Initialize quantum parameters
#             self.phase_coherence = MINIMUM_COHERENCE_FLOOR
#             self.phase = 0.0
# 
#             # Apply Hadamard gates to create superposition
#             for q in range(self.num_qubits):
#                 self.qc.h(q)
#             logger.debug(f"Applied Hadamard gates to all {self.num_qubits} qubits")
# 
#             # Add state saving instruction
#             save_sv = SaveStatevectorWrapper(self.num_qubits)
#             save_sv.apply(self.qc)
# 
#             # Execute circuit with error handling
#             if self.simulator:
#                 try:
#                     # Transpile circuit
#                     transpiled = transpile(self.qc, self.simulator)
# 
#                     # Run simulation with noise model
#                     result = self.simulator.run(
#                         transpiled,
#                         noise_model=self.noise_model if hasattr(self, 'noise_model') else None
#                     ).result()
# 
#                     # Extract statevector
#                     if 'statevector' in result.data():
#                         self.statevector = result.get_statevector()
#                         logger.debug("Statevector extracted successfully from simulation result")
# 
#                         # Validate and normalize statevector
#                         if isinstance(self.statevector, Statevector):
#                             state_data = np.array(self.statevector.data)
#                         else:
#                             state_data = np.array(self.statevector)
# 
#                         # Check normalization
#                         norm = np.linalg.norm(state_data)
#                         if not np.isclose(norm, 1.0, atol=1e-6):
#                             logger.warning(f"Statevector not normalized (norm={norm}). Renormalizing.")
#                             state_data /= norm
#                             self.statevector = Statevector(state_data)
#                     else:
#                         logger.warning("No statevector found in simulation result. Using default.")
#                         self.statevector = Statevector.from_label('0' * self.num_qubits)
#                 except Exception as e:
#                     logger.error(f"Simulation error: {e}")
#                     logger.error("Using default state initialization")
#                     self.statevector = Statevector.from_label('0' * self.num_qubits)
#             else:
#                 logger.warning("Simulator is not available. Using default initialization.")
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#             # Update quantum metrics
#             self.update_phase_coherence()
#             self._update_state_metrics()
#             logger.debug(f"After initialization: coherence={self.phase_coherence:.4f}, phase={self.phase:.4f}")
# 
#             # Store initial state in history
#             if hasattr(self, 'evolution_history'):
#                 self.evolution_history.append({
#                     'time': 0,
#                     'statevector': self.statevector.copy() if isinstance(self.statevector, Statevector) else Statevector(self.statevector),
#                     'phase_coherence': self.phase_coherence,
#                     'phase': self.phase
#                 })
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error in ground state preparation: {e}")
#             traceback.print_exc()
#             return self._attempt_state_recovery()
# 
#     def _attempt_state_recovery(self) -> bool:
#         """
#         Attempt recovery from failed state operations.
# 
#         Returns:
#             True if recovery successful, False otherwise
#         """
#         try:
#             self.recovery_attempts += 1
#             logger.warning(f"Attempting state recovery (attempt {self.recovery_attempts}/{self.max_recovery_attempts})")
# 
#             # Reset circuit
#             self.qc = QuantumCircuit(self.num_qubits)
# 
#             # Reset to basic state
#             self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#             # Apply basic Hadamard to all qubits
#             for q in range(self.num_qubits):
#                 self.qc.h(q)
# 
#             # Update metrics with safe defaults
#             self.phase_coherence = self.minimum_coherence
#             self.phase = 0.0
# 
#             logger.info("Basic state recovery complete")
# 
#             # Reset recovery counter if successful
#             if self.recovery_attempts >= self.max_recovery_attempts:
#                 logger.warning("Maximum recovery attempts reached")
#                 self.recovery_attempts = 0
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error in state recovery: {e}")
# 
#             # Set absolute minimum working state as last resort
#             self.statevector = Statevector.from_label('0' * self.num_qubits)
#             self.phase_coherence = self.minimum_coherence
#             self.phase = 0.0
# 
#             return False
# 
#     def update_state(self) -> bool:
#         """
#         Update quantum state with enhanced error handling and recovery.
# 
#         Returns:
#             True if update successful, False otherwise
#         """
#         try:
#             # Create new circuit
#             self.qc = QuantumCircuit(self.num_qubits)
# 
#             # Validate statevector
#             if self.statevector is None or not isinstance(self.statevector, (Statevector, np.ndarray)):
#                 logger.warning("Invalid statevector detected. Resetting to |0⟩ state.")
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#             # Extract state data
#             if isinstance(self.statevector, Statevector):
#                 state_data = self.statevector.data
#             else:
#                 state_data = self.statevector
# 
#             # Initialize circuit to current state
#             self.qc.initialize(list(state_data), range(self.num_qubits))
#             logger.debug("Initialized quantum circuit to current state")
# 
#             # Add state saving instruction
#             save_sv = SaveStatevectorWrapper(self.num_qubits)
#             save_sv.apply(self.qc)
# 
#             # Execute circuit with simulator
#             if self.simulator:
#                 try:
#                     # Transpile circuit
#                     transpiled = transpile(self.qc, self.simulator)
# 
#                     # Run simulation
#                     result = self.simulator.run(
#                         transpiled,
#                         noise_model=self.noise_model if hasattr(self, 'noise_model') else None,
#                         shots=1
#                     ).result()
# 
#                     # Process result
#                     if 'statevector' in result.data():
#                         self.statevector = result.get_statevector()
#                         logger.debug("Updated statevector from simulation result")
# 
#                         # Update quantum metrics
#                         self.update_phase_coherence()
#                         self._update_state_metrics()
# 
#                         # Store state in history
#                         if hasattr(self, 'evolution_history'):
#                             self.evolution_history.append({
#                                 'time': len(self.evolution_history),
#                                 'statevector': self.statevector.copy() if isinstance(self.statevector, Statevector) else Statevector(self.statevector),
#                                 'phase_coherence': self.phase_coherence,
#                                 'phase': self.phase
#                             })
# 
#                         return True
#                     else:
#                         logger.warning("No statevector found. Falling back to recovery.")
#                         return self._attempt_state_recovery()
#                 except Exception as e:
#                     logger.error(f"Error in circuit execution: {e}")
#                     return self._attempt_state_recovery()
#             else:
#                 logger.warning("Simulator unavailable. Using fallback statevector.")
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
#                 self.update_phase_coherence()
#                 self._update_state_metrics()
#                 return True
# 
#         except Exception as e:
#             logger.error(f"Error updating quantum state: {e}")
#             traceback.print_exc()
#             return self._attempt_state_recovery()
# 
#     def reinforce_coherence(self, qc: QuantumCircuit,
#                            distinction_variance: float,
#                            phase_coherence: float) -> bool:
#         """
#         Reinforce quantum coherence based on distinction and coherence metrics.
# 
#         Args:
#             qc: Quantum circuit to modify
#             distinction_variance: Variance in distinction level
#             phase_coherence: Current phase coherence
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             # Ensure inputs are real floats
#             phase_coherence = ensure_real(phase_coherence, MINIMUM_COHERENCE_FLOOR)
#             distinction_variance = ensure_real(distinction_variance, 0.0)
# 
#             # Target coherence for the system
#             target_coherence = 0.8  # High coherence goal
# 
#             # Calculate correction parameters
#             coherence_error = target_coherence - phase_coherence
#             base_angle = float((np.pi / 8) * np.sign(coherence_error))
#             variance_factor = min(1.0, distinction_variance / 0.02)
# 
#             # Update momentum with type safety
#             self.coherence_momentum = update_momentum(
#                 self.coherence_momentum,
#                 coherence_error
#             )
# 
#             # Calculate final angle with momentum influence
#             angle = base_angle * variance_factor * (1.0 + 0.1 * self.coherence_momentum)
#             angle = ensure_real(angle, 0.0)
# 
#             # Apply quantum operations
#             for q in range(self.num_qubits):
#                 qc.rz(0.1 * angle, q)  # Small phase adjustment
#                 qc.rx(angle, q)        # Main rotation
# 
#             # Track optimization
#             if hasattr(self, 'optimization_history'):
#                 self.optimization_history.append({
#                     'type': 'coherence_reinforcement',
#                     'angle': float(angle),
#                     'coherence': float(phase_coherence),
#                     'momentum': float(self.coherence_momentum),
#                     'timestamp': time.time()
#                 })
# 
#             logger.debug(f"Applied coherence reinforcement: angle={angle:.4f}, momentum={self.coherence_momentum:.4f}")
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error in coherence reinforcement: {e}")
#             return False
# 
#     def _update_state_metrics(self) -> None:
#         """Update and store quantum state evolution metrics."""
#         try:
#             # Extract probabilities from statevector
#             if isinstance(self.statevector, Statevector):
#                 probs = np.abs(np.array(self.statevector.data))**2
#             else:
#                 probs = np.abs(self.statevector)**2
# 
#             # Calculate entropy
#             entropy_val = self._calculate_entropy(probs)
# 
#             # Handle potential complex values in phase_coherence
#             coherence_real = ensure_real(self.phase_coherence, MINIMUM_COHERENCE_FLOOR)
#             coherence_imag = 0.0
#             if isinstance(self.phase_coherence, complex):
#                 coherence_imag = float(self.phase_coherence.imag)
# 
#             # Store metrics in evolution history
#             self.evolution_history.append({
#                 'time': len(self.evolution_history),
#                 'entropy': float(entropy_val),
#                 'coherence_real': coherence_real,
#                 'coherence_imag': coherence_imag,
#                 'coherence_magnitude': abs(self.phase_coherence),
#                 'phase': float(self.phase)
#             })
# 
#             logger.debug(f"Updated state metrics: entropy={entropy_val:.4f}, coherence={coherence_real:.4f}, phase={self.phase:.4f}")
# 
#         except Exception as e:
#             logger.error(f"Error updating state metrics: {e}")
#             # Add safe default values
#             self.evolution_history.append({
#                 'time': len(self.evolution_history),
#                 'entropy': 0.0,
#                 'coherence': float(self.phase_coherence),
#                 'phase': float(self.phase)
#             })
# 
#     def _calculate_entropy(self, probabilities: np.ndarray) -> float:
#         """
#         Calculate the von Neumann entropy from probability distribution.
# 
#         Args:
#             probabilities: Probability distribution from statevector
# 
#         Returns:
#             Entropy value
#         """
#         try:
#             # Add small epsilon to avoid log(0)
#             epsilon = np.finfo(float).eps
#             # Handle case where all probabilities are zero or contain NaN values
#             probabilities = np.nan_to_num(probabilities, nan=0.0)
# 
#             # Check if sum is zero
#             total_prob = np.sum(probabilities)
#             if total_prob < epsilon:
#                 return 0.0  # Return zero entropy for invalid distribution
# 
#             # Normalize properly with safeguard against division by zero
#             probabilities = probabilities / total_prob
# 
#             # Calculate entropy
#             entropy = -np.sum(probabilities * np.log2(probabilities + epsilon))
#             return float(entropy)
# 
#         except Exception as e:
#             logger.error(f"Error calculating entropy: {e}")
#             return 0.0
# 
#     def apply_gate(self, gate: str, qubits: List[int], params: Optional[Dict] = None) -> bool:
#         """
#         Apply quantum gate to circuit with error handling.
# 
#         Args:
#             gate: Gate name ('h', 'x', 'rz', 'rx', 'cx')
#             qubits: List of qubit indices
#             params: Additional parameters for parameterized gates
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         if params is None:
#             params = {}
# 
#         try:
#             # Validate qubits
#             if not qubits or not all(isinstance(q, int) and 0 <= q < self.num_qubits for q in qubits):
#                 logger.warning(f"Invalid qubits {qubits} for gate {gate}. Skipping operation.")
#                 return False
# 
#             # Create new circuit for gate application
#             gate_circuit = QuantumCircuit(self.num_qubits)
# 
#             # Validate statevector
#             if self.statevector is None or not isinstance(self.statevector, Statevector):
#                 logger.warning("Statevector is None. Resetting to |0⟩ state.")
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#             # Initialize to current state
#             gate_circuit.initialize(list(self.statevector.data), range(self.num_qubits))
# 
#             # Apply requested gate
#             if gate == 'h':
#                 for q in qubits:
#                     gate_circuit.h(q)
#                 logger.debug(f"Applied Hadamard gate to qubits {qubits}")
#             elif gate == 'x':
#                 for q in qubits:
#                     gate_circuit.x(q)
#                 logger.debug(f"Applied X gate to qubits {qubits}")
#             elif gate == 'rz':
#                 phi = ensure_real(params.get('phi', 0.0), 0.0)
#                 for q in qubits:
#                     gate_circuit.rz(phi, q)
#                 logger.debug(f"Applied RZ gate with phi={phi:.4f} to qubits {qubits}")
#             elif gate == 'rx':
#                 theta = ensure_real(params.get('theta', 0.0), 0.0)
#                 for q in qubits:
#                     gate_circuit.rx(theta, q)
#                 logger.debug(f"Applied RX gate with theta={theta:.4f} to qubits {qubits}")
#             elif gate == 'cx' and len(qubits) >= 2:
#                 gate_circuit.cx(qubits[0], qubits[1])
#                 logger.debug(f"Applied CX gate from qubit {qubits[0]} to {qubits[1]}")
#             else:
#                 logger.warning(f"Unsupported gate: {gate}")
#                 return False
# 
#             # Add state saving instruction
#             save_sv = SaveStatevectorWrapper(self.num_qubits)
#             if save_sv.instruction is not None:
#                 gate_circuit.append(save_sv.instruction, gate_circuit.qubits)
# 
#             # Execute circuit
#             if self.simulator is not None:
#                 try:
#                     transpiled = transpile(gate_circuit, self.simulator)
#                     result = self.simulator.run(transpiled, noise_model=self.noise_model).result()
# 
#                     if 'statevector' in result.data():
#                         self.statevector = result.get_statevector()
#                         return True
#                     else:
#                         logger.warning("No statevector returned after gate application. Attempting recovery.")
#                         return self._attempt_state_recovery()
#                 except Exception as e:
#                     logger.error(f"Error applying {gate}: {e}")
#                     return self._attempt_state_recovery()
#             else:
#                 logger.warning(f"Simulator unavailable while applying {gate}. Using fallback state.")
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
#                 self.update_phase_coherence()
#                 self._update_state_metrics()
#                 return True
# 
#         except Exception as e:
#             logger.error(f"Error applying gate '{gate}' on qubits {qubits}: {e}")
#             return self._attempt_state_recovery()
# 
#     def apply_phase_shift(self, angle: float) -> bool:
#         """
#         Apply phase shift to quantum state.
# 
#         Args:
#             angle: Phase shift angle
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             # Ensure angle is a real float
#             angle = ensure_real(angle, 0.0)
# 
#             # Create phase shift circuit
#             phase_circuit = QuantumCircuit(self.num_qubits)
# 
#             # Initialize to current state
#             if isinstance(self.statevector, np.ndarray):
#                 current_state = Statevector(self.statevector)
#             else:
#                 current_state = self.statevector
# 
#             phase_circuit.initialize(list(current_state.data), range(self.num_qubits))
# 
#             # Apply phase shift
#             phase_circuit.rz(angle, 0)
#             logger.debug(f"Applied phase shift of {angle:.4f} to qubit 0")
# 
#             # Add state saving instruction
#             save_sv = SaveStatevectorWrapper(self.num_qubits)
#             if save_sv.instruction is not None:
#                 phase_circuit.append(save_sv.instruction, phase_circuit.qubits)
# 
#             # Execute circuit
#             if self.simulator is not None:
#                 transpiled = transpile(phase_circuit, self.simulator)
#                 result = self.simulator.run(transpiled, noise_model=self.noise_model).result()
# 
#                 if 'statevector' in result.data():
#                     self.statevector = result.get_statevector()
#                     self.phase = (self.phase + angle) % (2 * np.pi)
# 
#                     if hasattr(self, 'phase_history'):
#                         self.phase_history.append(self.phase)
# 
#                     # Update coherence
#                     self.update_phase_coherence()
#                     self._update_state_metrics()
#                     return True
#                 else:
#                     logger.error("No statevector in phase shift result")
#                     return self._attempt_state_recovery()
#             else:
#                 logger.error("Simulator not available")
#                 return self._attempt_state_recovery()
# 
#         except Exception as e:
#             logger.error(f"Error applying phase shift: {e}")
#             return self._attempt_state_recovery()
# 
#     def get_quantum_metrics(self) -> Dict[str, float]:
#         """
#         Get comprehensive quantum metrics with robust error handling.
# 
#         Returns:
#             Dictionary containing phase, phase_coherence, and normalized_entropy
#         """
#         try:
#             # Ensure phase coherence is initialized
#             if not hasattr(self, 'phase_coherence') or self.phase_coherence is None:
#                 self.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#             # Get statevector probabilities
#             if isinstance(self.statevector, np.ndarray):
#                 probs = np.abs(self.statevector) ** 2
#             elif hasattr(self.statevector, 'data'):
#                 probs = np.abs(np.array(self.statevector.data)) ** 2
#             else:
#                 logger.warning("Invalid statevector, reinitializing...")
#                 from qiskit.quantum_info import Statevector
#                 self.statevector = Statevector.from_label('0' * self.num_qubits)
#                 probs = np.abs(np.array(self.statevector.data)) ** 2
# 
#             # Calculate entropy
#             entropy_val = self._calculate_entropy(probs)
#             max_entropy = np.log2(2**self.num_qubits)
#             normalized_entropy = entropy_val / max_entropy if max_entropy > 0 else 0.0
# 
#             # Get phase information
#             if not hasattr(self, 'phase'):
#                 self.phase = 0.0
# 
#             # Calculate phase distinction
#             if hasattr(self, 'phase_history') and len(self.phase_history) > 0:
#                 mean_phase = float(np.mean(list(self.phase_history)))
#                 phase_distinction = float(abs(self.phase - mean_phase))
#             else:
#                 phase_distinction = 0.0
# 
#             # Calculate mean coherence
#             if hasattr(self, 'coherence_history') and len(self.coherence_history) > 0:
#                 mean_coherence = float(np.mean(list(self.coherence_history)))
#             else:
#                 mean_coherence = float(self.phase_coherence)
# 
#             # Calculate phase stability
#             if hasattr(self, 'phase_history') and len(self.phase_history) > 0:
#                 phase_stability = float(np.std(list(self.phase_history)))
#             else:
#                 phase_stability = 1.0
# 
#             # Calculate coherence distinction and quantum coupling
#             coherence_distinction = float(np.clip(self.phase_coherence, MINIMUM_COHERENCE_FLOOR, 1.0))
#             quantum_coupling = float(np.clip(self.phase_coherence * (1.0 - normalized_entropy), 0.0, 1.0))
#             quantum_surplus_coupling = float(np.clip(mean_coherence * (1.0 - normalized_entropy), 0.0, 1.0))
#             stability = float(np.clip((mean_coherence + (1.0 - normalized_entropy)) / 2, 0.0, 1.0))
# 
#             # Calculate coherence variance using the dedicated method
#             coherence_variance = self.get_coherence_variance()
# 
#             # Prepare metrics dictionary with explicit type conversion
#             metrics = {
#                 'phase': float(self.phase % (2*np.pi)),
#                 'phase_coherence': float(self.phase_coherence),
#                 'entropy': float(entropy_val),
#                 'normalized_entropy': float(normalized_entropy),
#                 'mean_coherence': float(mean_coherence),
#                 'phase_stability': float(phase_stability),
#                 'phase_distinction': float(phase_distinction),
#                 'coherence_distinction': coherence_distinction,
#                 'quantum_coupling': quantum_coupling,
#                 'quantum_surplus_coupling': quantum_surplus_coupling,
#                 'stability': stability,
#                 'coherence_variance': coherence_variance  # Add coherence variance to metrics
#             }
# 
#             # Validate metrics
#             for key, value in metrics.items():
#                 if not isinstance(value, float) or math.isnan(value) or math.isinf(value):
#                     logger.warning(f"Invalid {key} value: {value}, using default")
#                     if key in ['phase_coherence', 'mean_coherence', 'coherence_distinction']:
#                         metrics[key] = float(MINIMUM_COHERENCE_FLOOR)
#                     elif key in ['quantum_coupling', 'quantum_surplus_coupling', 'stability']:
#                         metrics[key] = 1.0
#                     elif key == 'coherence_variance':  # Special handling for coherence_variance
#                         metrics[key] = 0.001  # Default small non-zero value
#                     else:
#                         metrics[key] = 0.0
# 
#             return metrics
# 
#         except Exception as e:
#             logger.error(f"Error computing quantum metrics: {e}")
#             traceback.print_exc()
# 
#             # Import random here to ensure it's available
#             import random
# 
#             # Return safe default metrics
#             return {
#                 'phase': random.uniform(0.0, 2*np.pi),
#                 'phase_coherence': float(MINIMUM_COHERENCE_FLOOR),
#                 'entropy': 0.0,
#                 'normalized_entropy': 0.0,
#                 'mean_coherence': float(MINIMUM_COHERENCE_FLOOR),
#                 'phase_stability': 1.0,
#                 'phase_distinction': 0.0,
#                 'coherence_distinction': float(MINIMUM_COHERENCE_FLOOR),
#                 'quantum_coupling': 1.0,
#                 'quantum_surplus_coupling': 1.0,
#                 'stability': 1.0,
#                 'coherence_variance': 0.001  # Include default coherence_variance in fallback
#             }
# 
#     def compute_quantum_surplus_coupling(self, surplus_state: Any) -> float:
#         """
#         Compute coupling strength between quantum state and surplus values.
# 
#         Args:
#             surplus_state: Either a SurplusState object or a dictionary of surplus values
# 
#         Returns:
#             Coupling strength in range [0.1, 1.0]
#         """
#         try:
#             # Get quantum metrics
#             metrics = self.get_quantum_metrics()
# 
#             # Get key components from quantum metrics
#             coherence_factor = max(metrics['phase_coherence'], self.minimum_coherence)
#             entropy_factor = 1.0 - metrics['normalized_entropy']
#             phase_factor = metrics.get('phase_distinction', 0.5)
# 
#             # Process the surplus_state input
#             if isinstance(surplus_state, SurplusState):
#                 surplus_values = surplus_state.values
#             elif isinstance(surplus_state, dict):
#                 surplus_values = surplus_state
#             else:
#                 logger.warning("Invalid surplus state type; using default values")
#                 surplus_values = {
#                     'basal': 1.0,
#                     'cognitive': 1.0,
#                     'predictive': 1.0,
#                     'ontological': 1.0
#                 }
# 
#             # Calculate the total surplus
#             surplus_total = sum(surplus_values.values())
#             if surplus_total <= 0:
#                 logger.warning("Total surplus is zero or negative; using fallback value")
#                 surplus_total = 0.1  # Prevent division by zero or negative totals
# 
#             # Compute the coupling
#             coupling = (0.4 * coherence_factor + 0.3 * entropy_factor + 0.3 * phase_factor) / (1.0 + surplus_total)
# 
#             # Return the coupling clipped between 0.1 and 1.0
#             return float(np.clip(coupling, 0.1, 1.0))
# 
#         except Exception as e:
#             logger.error(f"Error computing quantum surplus coupling: {e}")
#             return 0.5
# 
#     def get_quantum_distinction_metrics(self) -> Dict[str, float]:
#         """
#         Get distinction-related quantum metrics.
# 
#         Returns:
#             Dictionary of distinction metrics
#         """
#         try:
#             # Get full quantum metrics
#             quantum_metrics = self.get_quantum_metrics()
# 
#             # Calculate distinction factor from phase history
#             distinction_factor = 0.0
#             if hasattr(self, 'phase_history') and self.phase_history:
#                 avg_phase = np.mean(list(self.phase_history))
#                 distinction_factor = abs(quantum_metrics['phase'] - avg_phase)
# 
#             # Calculate field resistance based on entropy
#             field_resistance = np.exp(-quantum_metrics['normalized_entropy'])
# 
#             # Calculate ontological stability combining coherence and distinction
#             ontological_stability = (0.7 * quantum_metrics['phase_coherence'] +
#                                      0.3 * (1.0 - distinction_factor))
# 
#             # Return distinction metrics
#             return {
#                 'phase_distinction': float(distinction_factor),
#                 'field_resistance': float(field_resistance),
#                 'ontological_stability': float(ontological_stability),
#                 'coherence_distinction': float(quantum_metrics.get('phase_coherence', 0.0))
#             }
# 
#         except Exception as e:
#             logger.error(f"Error computing quantum distinction metrics: {e}")
#             return {
#                 'phase_distinction': 0.0,
#                 'field_resistance': 0.0,
#                 'ontological_stability': self.minimum_coherence,
#                 'coherence_distinction': self.minimum_coherence
#             }
# 
#     def update_phase_coherence(self) -> float:
#         """
#         Update phase coherence measure from the current statevector.
# 
#         Returns:
#             Updated phase coherence value
#         """
#         try:
#             # Validate statevector
#             if self.statevector is None:
#                 logger.warning("Statevector is None. Defaulting phase coherence.")
#                 self.phase_coherence = self.minimum_coherence
#                 return self.phase_coherence
# 
#             # Extract numerical array from statevector
#             if isinstance(self.statevector, Statevector):
#                 state_array = np.array(self.statevector.data, dtype=np.complex128)
#             else:
#                 state_array = np.array(self.statevector, dtype=np.complex128)
# 
#             # Handle potential NaN values
#             state_array = np.nan_to_num(state_array, nan=0.0)
# 
#             # Form the density matrix
#             rho = np.outer(state_array, np.conj(state_array))
# 
#             # Validate density matrix
#             if rho.shape[0] == 0:
#                 logger.warning("Density matrix is empty. Defaulting phase coherence.")
#                 self.phase_coherence = self.minimum_coherence
#                 return self.phase_coherence
# 
#             # Calculate coherence from off-diagonal elements
#             diag_mask = np.eye(rho.shape[0], dtype=bool)
#             off_diag_mask = ~diag_mask
# 
#             # Sum the absolute values of off-diagonal elements
#             off_diag_sum = np.sum(np.abs(rho[off_diag_mask]))
# 
#             # Count off-diagonal elements
#             count = np.sum(off_diag_mask)
# 
#             # Handle edge cases
#             if count == 0 or np.isnan(off_diag_sum):
#                 logger.warning("No valid coherence data. Using minimum coherence.")
#                 self.phase_coherence = self.minimum_coherence
#             else:
#                 # Ensure real value and proper bounds
#                 coherence = float(np.real(off_diag_sum / count))
# 
#                 # Add small noise to encourage variation
#                 noise = (np.random.random() - 0.5) * 0.01
#                 coherence += noise
# 
#                 self.phase_coherence = max(min(coherence, 1.0), self.minimum_coherence)
# 
#             # Ensure coherence_history attribute exists
#             if not hasattr(self, 'coherence_history'):
#                 self.coherence_history = deque(maxlen=100)
# 
#             # Store in history
#             self.coherence_history.append(self.phase_coherence)
# 
#             # Print debug info for coherence tracking
#             if len(self.coherence_history) > 1:
#                 recent_coherence = list(self.coherence_history)[-5:]
#                 coherence_variance = np.var(recent_coherence)
#                 logger.debug(f"Recent coherence values: {recent_coherence}")
#                 logger.debug(f"Coherence variance: {coherence_variance}")
# 
#             return self.phase_coherence
# 
#         except Exception as e:
#             logger.error(f"Error in update_phase_coherence: {e}")
#             self.phase_coherence = self.minimum_coherence
#             return self.phase_coherence
# 
#     def measure_state(self, shots: int = 1024) -> Dict[str, int]:
#         """
#         Measure the quantum state and return measurement results.
# 
#         Args:
#             shots: Number of measurement shots
# 
#         Returns:
#             Dictionary of measurement outcomes and counts
#         """
#         try:
#             # Create measurement circuit
#             measure_circuit = QuantumCircuit(self.num_qubits, self.num_qubits)
# 
#             # Initialize to current state
#             if isinstance(self.statevector, np.ndarray):
#                 current_state = Statevector(self.statevector)
#             else:
#                 current_state = self.statevector
# 
#             measure_circuit.initialize(list(current_state.data), range(self.num_qubits))
# 
#             # Add measurement operations
#             for q in range(self.num_qubits):
#                 measure_circuit.measure(q, q)
# 
#             # Execute circuit
#             if self.simulator is not None:
#                 transpiled = transpile(measure_circuit, self.simulator)
#                 result = self.simulator.run(transpiled, shots=shots).result()
# 
#                 if result.success:
#                     counts = dict(result.get_counts())
# 
#                     # Store in measurement history
#                     if hasattr(self, 'measurement_history'):
#                         self.measurement_history.append({
#                             'time': len(self.measurement_history),
#                             'counts': counts,
#                             'shots': shots
#                         })
# 
#                     return counts
#                 else:
#                     logger.error(f"Measurement failed: {result.status}")
#                     return {}
#             else:
#                 logger.error("Simulator not available for measurement")
#                 return {}
# 
#         except Exception as e:
#             logger.error(f"Error in state measurement: {e}")
#             return {}
# 
#     def compute_overlap(self, target_state: Union[Statevector, np.ndarray]) -> float:
#         """
#         Compute overlap between current state and target state.
# 
#         Args:
#             target_state: Target quantum state
# 
#         Returns:
#             Fidelity (overlap squared) between states
#         """
#         try:
#             # Ensure current statevector is valid
#             if self.statevector is None:
#                 logger.warning("Current statevector is None. Cannot compute overlap.")
#                 return 0.0
# 
#             # Convert current state to numpy array
#             if isinstance(self.statevector, Statevector):
#                 current_array = np.array(self.statevector.data, dtype=np.complex128)
#             else:
#                 current_array = np.array(self.statevector, dtype=np.complex128)
# 
#             # Convert target state to numpy array
#             if isinstance(target_state, Statevector):
#                 target_array = np.array(target_state.data, dtype=np.complex128)
#             else:
#                 target_array = np.array(target_state, dtype=np.complex128)
# 
#             # Ensure dimensions match
#             if current_array.shape != target_array.shape:
#                 logger.error(f"State dimensions do not match: {current_array.shape} vs {target_array.shape}")
#                 return 0.0
# 
#             # Compute overlap
#             overlap = np.abs(np.vdot(current_array, target_array)) ** 2
#             return float(overlap)
# 
#         except Exception as e:
#             logger.error(f"Error computing state overlap: {e}")
#             return 0.0
# 
#     def apply_custom_unitary(self, unitary_matrix: np.ndarray, qubits: List[int]) -> bool:
#         """
#         Apply custom unitary operation to specified qubits.
# 
#         Args:
#             unitary_matrix: Unitary matrix to apply
#             qubits: Qubits to apply operation to
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             # Validate qubits
#             if not qubits or not all(0 <= q < self.num_qubits for q in qubits):
#                 logger.error(f"Invalid qubits: {qubits}")
#                 return False
# 
#             # Validate unitary matrix
#             matrix_size = 2 ** len(qubits)
#             if unitary_matrix.shape != (matrix_size, matrix_size):
#                 logger.error(f"Unitary matrix size mismatch: {unitary_matrix.shape} vs ({matrix_size}, {matrix_size})")
#                 return False
# 
#             # Create custom unitary circuit
#             custom_circuit = QuantumCircuit(self.num_qubits)
# 
#             # Initialize to current state
#             if isinstance(self.statevector, np.ndarray):
#                 current_state = Statevector(self.statevector)
#             else:
#                 current_state = self.statevector
# 
#             custom_circuit.initialize(list(current_state.data), range(self.num_qubits))
# 
#             # Apply custom unitary
#             # For single qubit unitary
#             if len(qubits) == 1:
#                 custom_circuit.unitary(unitary_matrix, qubits[0], label='custom')
#             # For multi-qubit unitary
#             else:
#                 custom_circuit.unitary(unitary_matrix, qubits, label='custom')
# 
#             # Add state saving instruction
#             save_sv = SaveStatevectorWrapper(self.num_qubits)
#             save_sv.apply(custom_circuit)
# 
#             # Execute circuit
#             if self.simulator is not None:
#                 transpiled = transpile(custom_circuit, self.simulator)
#                 result = self.simulator.run(transpiled, noise_model=self.noise_model).result()
# 
#                 if 'statevector' in result.data():
#                     self.statevector = result.get_statevector()
#                     self.update_phase_coherence()
#                     self._update_state_metrics()
#                     return True
#                 else:
#                     logger.error("No statevector returned after custom unitary")
#                     return self._attempt_state_recovery()
#             else:
#                 logger.error("Simulator not available for custom unitary")
#                 return False
# 
#         except Exception as e:
#             logger.error(f"Error applying custom unitary: {e}")
#             return False
# 
#     def get_evolution_history(self) -> List[Dict]:
#         """
#         Get the history of quantum state evolution.
# 
#         Returns:
#             List of state evolution records
#         """
#         try:
#             if not hasattr(self, 'evolution_history') or not self.evolution_history:
#                 return []
# 
#             # Create a simplified version for return (avoiding statevector copies)
#             history = []
#             for entry in self.evolution_history:
#                 history_entry = {k: v for k, v in entry.items() if k != 'statevector'}
#                 history.append(history_entry)
# 
#             return history
# 
#         except Exception as e:
#             logger.error(f"Error retrieving evolution history: {e}")
#             return []
# 
#     def is_stable(self) -> bool:
#         """
#         Check if the quantum state is stable based on recent history.
# 
#         Returns:
#             True if state is stable, False otherwise
#         """
#         try:
#             # Check if we have enough history
#             if not hasattr(self, 'coherence_history') or len(self.coherence_history) < 10:
#                 return True  # Assume stable if not enough history
# 
#             # Calculate stability metrics
#             recent_coherence = list(self.coherence_history)[-10:]
#             coherence_variance = np.var(recent_coherence)
# 
#             # A state is stable if coherence variance is small
#             stability_threshold = 0.01
#             return coherence_variance < stability_threshold
# 
#         except Exception as e:
#             logger.error(f"Error checking stability: {e}")
#             return False
# 
#     def get_state_representation(self) -> Dict[str, Any]:
#         """
#         Get a comprehensive representation of the current quantum state.
# 
#         Returns:
#             Dictionary with state details
#         """
#         try:
#             # Get basic metrics
#             metrics = self.get_quantum_metrics()
# 
#             # Calculate additional state representations
#             if isinstance(self.statevector, Statevector):
#                 probabilities = np.abs(np.array(self.statevector.data))**2
#             else:
#                 probabilities = np.abs(self.statevector)**2
# 
#             # Find most probable states
#             top_states = []
#             for i, prob in enumerate(probabilities):
#                 if prob > 0.01:  # Only include states with significant probability
#                     state_label = format(i, f'0{self.num_qubits}b')
#                     top_states.append({
#                         'state': state_label,
#                         'probability': float(prob),
#                         'amplitude': float(np.abs(self.statevector[i])),
#                         'phase': float(np.angle(self.statevector[i]))
#                     })
# 
#             # Sort by probability
#             top_states.sort(key=lambda x: x['probability'], reverse=True)
# 
#             # Return comprehensive state representation
#             return {
#                 'metrics': metrics,
#                 'top_states': top_states[:8],  # Limit to top 8 states
#                 'num_qubits': self.num_qubits,
#                 'coherence': float(self.phase_coherence),
#                 'timestamp': time.time()
#             }
# 
#         except Exception as e:
#             logger.error(f"Error getting state representation: {e}")
#             return {
#                 'metrics': {},
#                 'top_states': [],
#                 'num_qubits': self.num_qubits,
#                 'coherence': float(self.minimum_coherence),
#                 'timestamp': time.time()
#             }
# 
# 
# # Main execution for testing
# if __name__ == "__main__":
#     # Create a quantum state
#     print("Initializing quantum state...")
#     quantum_state = EnhancedQuantumState(num_qubits=4)
# 
#     # Apply some gates
#     print("\nApplying Hadamard gates...")
#     for q in range(quantum_state.num_qubits):
#         quantum_state.apply_gate('h', [q])
# 
#     # Apply a phase shift
#     print("\nApplying phase shift...")
#     quantum_state.apply_phase_shift(0.5)
# 
#     # Get and print metrics
#     metrics = quantum_state.get_quantum_metrics()
#     print("\nQuantum Metrics:")
#     for key, value in metrics.items():
#         print(f"  {key}: {value:.4f}")
# 
#     # Measure the state
#     print("\nMeasuring quantum state...")
#     measurement = quantum_state.measure_state(shots=1024)
#     print("\nMeasurement Results:")
#     for state, count in measurement.items():
#         print(f"  |{state}⟩: {count} ({count/1024:.4f})")
#

"""# 5. Cognitive Structures"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile cognitive_structures.py
# """
# Cognitive Structures Module for Émile-2 Simulation
# --------------------------------------------------
# Implements recursive cognitive structures that enable emergence through
# adaptive layering and ontological modeling.
# """
# import logging
# import numpy as np
# import time
# from collections import deque
# from typing import Dict, List, Tuple, Optional, Union, Any
# import traceback
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.cognitive_structures")
# 
# # Import constants
# from utilities import (
#     TARGET_DISTINCTION,
#     CORE_DISTINCTION_UPDATE_RATE,
#     MOMENTUM_DECAY,
#     DISTINCTION_ANCHOR_WEIGHT,
#     COLLAPSE_DISSIPATION_THRESHOLD,
#     COLLAPSE_DISSIPATION_RATE,
#     ensure_real
# )
# 
# # Constants for cognitive structure dynamics
# STABILITY_DECAY_RATE = 0.99
# COLLAPSE_RESISTANCE_BASE = 0.8
# COLLAPSE_RESISTANCE_DECAY = 0.95
# COLLAPSE_RESISTANCE_UPDATE_RATE = 0.05
# MIN_COLLAPSE_RESISTANCE = 0.1
# MAX_COLLAPSE_RESISTANCE = 2.0
# MIN_STABILITY = 0.1
# MAX_STABILITY = 1.0
# MIN_ADAPTATION_RATE = 0.001
# MAX_ADAPTATION_RATE = 0.1
# SIGMOID_STEEPNESS = 5.0
# SIGMOID_MIDPOINT = 0.5
# STRENGTH_MOMENTUM_INFLUENCE = 0.1
# MAX_HISTORY_LENGTH = 1000
# STABILITY_WEIGHT = 0.6  # For weighted average in feedback calculation
# COHERENCE_WEIGHT = 0.4  # For weighted average in feedback calculation
# 
# 
# class RecursiveCognitiveStructuring:
#     """
#     Multi-layer cognitive structure with recursive updating and collapse prevention.
# 
#     Manages a hierarchical cognitive structure where each layer tracks attributes
#     like strength, stability, quantum coupling, and core identity.
#     Enhanced with adaptive feedback strength based on stability and coherence.
#     """
#     def __init__(self, num_layers: int = 3):
#         """
#         Initialize recursive cognitive structure with improved layer differentiation.
# 
#         Args:
#             num_layers: Number of cognitive layers
#         """
#         try:
#             # Move constants from utilities.py to instance variables for dynamic adjustment
#             self.target_distinction = 0.7  # Default from TARGET_DISTINCTION
#             self.core_distinction_update_rate = 0.01  # Default from CORE_DISTINCTION_UPDATE_RATE
#             self.stability_decay = 0.99  # Default from STABILITY_DECAY_RATE
#             self.distinction_anchor_weight = 0.2  # Default from DISTINCTION_ANCHOR_WEIGHT
#             self.collapse_dissipation_threshold = 0.35  # Default from COLLAPSE_DISSIPATION_THRESHOLD
#             self.collapse_dissipation_rate = 0.02  # Default from COLLAPSE_DISSIPATION_RATE
# 
#             # Additional constants moved from the class code to instance variables
#             self.collapse_resistance_base = 0.8  # Default from COLLAPSE_RESISTANCE_BASE
#             self.collapse_resistance_decay = 0.95  # Default from COLLAPSE_RESISTANCE_DECAY
#             self.collapse_resistance_update_rate = 0.05  # Default from COLLAPSE_RESISTANCE_UPDATE_RATE
#             self.min_collapse_resistance = 0.1  # Default from MIN_COLLAPSE_RESISTANCE
#             self.max_collapse_resistance = 2.0  # Default from MAX_COLLAPSE_RESISTANCE
#             self.stability_weight = 0.6  # Default from STABILITY_WEIGHT
#             self.coherence_weight = 0.4  # Default from COHERENCE_WEIGHT
# 
#             # Add momentum values for dynamic constant adjustment
#             self.threshold_momentum = 0.0
#             self.rate_momentum = 0.0
#             self.constants_adaptation_rate = 0.01
# 
#             # Track constant history
#             self.constants_history = deque(maxlen=100)
# 
#             # Initialize layers with more differentiated characteristics
#             self.layers = []
#             for i in range(num_layers):
#                 # More varied initialization values to break symmetry and promote dynamics
#                 layer = {
#                     'strength': 1.0 + 0.2 * i + 0.1 * np.random.random(),  # Increasing by depth with randomness
#                     'stability': 0.3 + 0.1 * i + 0.05 * np.random.random(),  # Start with lower stability
#                     'quantum_coupling': 0.3 + 0.2 * i + 0.05 * np.random.random(),  # More variance by layer
#                     'adaptation_rate': 0.01 * (0.8 ** i) * (1.0 + 0.2 * np.random.random()),  # Decreasing with randomness
#                     'base_feedback_strength': 0.3 + 0.2 * i + 0.1 * np.random.random(),  # Higher variability
#                     'feedback_strength': 0.2 ** (num_layers - i - 1) * (1.0 + 0.1 * np.random.random()),
#                     'collapse_resistance': 0.3 + 0.2 * i + 0.1 * np.random.random(),
#                     'core_identity': 1.2 ** (num_layers - i - 1) * (1.0 + 0.05 * np.random.random())
#                 }
#                 self.layers.append(layer)
# 
#             # History tracking with limited length to avoid memory issues
#             self.history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.adaptation_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.collapse_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.identity_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.distinction_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.feedback_history = deque(maxlen=MAX_HISTORY_LENGTH)
# 
#             # Initialize feedback matrix for asymmetric inter-layer feedback
#             self.feedback_matrix = np.zeros((num_layers, num_layers))
#             for i in range(num_layers):
#                 for j in range(num_layers):
#                     if i != j:  # No self-feedback
#                         # More varied initial feedback strengths
#                         self.feedback_matrix[i, j] = 0.2 ** abs(i - j) * (1.0 + 0.1 * np.random.random())
# 
#             # Dynamic parameters
#             self.coupling_momentum = 0.0
#             self.feedback_momentum = np.zeros(num_layers)
#             self.identity_momentum = 0.0
#             self.identity_preservation_rate = self.core_distinction_update_rate
#             self.adaptation_momentum = np.zeros(num_layers)
# 
#             # State tracking
#             self.metrics = {
#                 'last_update_time': time.time(),
#                 'updates_count': 0,
#                 'mean_strength': 1.0,
#                 'mean_stability': 0.3,  # Start with lower stability to encourage dynamics
#                 'collapse_probability': 0.0,
#                 'mean_feedback': 0.0
#             }
# 
#             # Record initial constants state
#             self._record_constants_state("initialization")
# 
#             logger.info(f"Initialized RecursiveCognitiveStructuring with {num_layers} layers")
# 
#         except Exception as e:
#             logger.error(f"Error initializing RecursiveCognitiveStructuring: {e}")
#             # Create minimal fallback structure
#             self.layers = [{
#                 'strength': 1.0,
#                 'stability': 0.3,
#                 'quantum_coupling': 0.3,
#                 'adaptation_rate': 0.01,
#                 'base_feedback_strength': 0.5,
#                 'feedback_strength': 0.2,
#                 'collapse_resistance': 0.3,
#                 'core_identity': 1.2
#             }]
#             self.history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.adaptation_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.collapse_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.identity_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.distinction_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.feedback_history = deque(maxlen=MAX_HISTORY_LENGTH)
#             self.feedback_matrix = np.zeros((1, 1))
#             self.collapse_threshold = 0.35
#             self.coupling_momentum = 0.0
#             self.feedback_momentum = np.zeros(1)
#             self.adaptation_momentum = np.zeros(1)
# 
#             # Initialize dynamic constants with default values
#             self.target_distinction = 0.7
#             self.core_distinction_update_rate = 0.01
#             self.stability_decay = 0.99
#             self.distinction_anchor_weight = 0.2
#             self.collapse_dissipation_threshold = 0.35
#             self.collapse_dissipation_rate = 0.02
#             self.collapse_resistance_base = 0.8
#             self.collapse_resistance_decay = 0.95
#             self.collapse_resistance_update_rate = 0.05
#             self.min_collapse_resistance = 0.1
#             self.max_collapse_resistance = 2.0
#             self.stability_weight = 0.6
#             self.coherence_weight = 0.4
# 
#     def _record_constants_state(self, event_type: str) -> None:
#         """Record the current state of all dynamic constants for tracking changes over time."""
#         try:
#             constants_state = {
#                 'target_distinction': self.target_distinction,
#                 'core_distinction_update_rate': self.core_distinction_update_rate,
#                 'stability_decay': self.stability_decay,
#                 'distinction_anchor_weight': self.distinction_anchor_weight,
#                 'collapse_dissipation_threshold': self.collapse_dissipation_threshold,
#                 'collapse_dissipation_rate': self.collapse_dissipation_rate,
#                 'collapse_resistance_base': self.collapse_resistance_base,
#                 'collapse_resistance_decay': self.collapse_resistance_decay,
#                 'collapse_resistance_update_rate': self.collapse_resistance_update_rate,
#                 'stability_weight': self.stability_weight,
#                 'coherence_weight': self.coherence_weight,
#                 'event_type': event_type,
#                 'timestamp': time.time()
#             }
#             self.constants_history.append(constants_state)
#         except Exception as e:
#             logger.error(f"Error recording constants state: {e}")
# 
#     def update_dynamic_constants(self,
#                               phase_coherence: float,
#                               distinction_level: float,
#                               prediction_error: float,
#                               mean_stability: float) -> None:
#         """
#         Update dynamic constants based on system performance and behavior.
# 
#         Args:
#             phase_coherence: Current phase coherence
#             distinction_level: Current distinction level
#             prediction_error: Current prediction error
#             mean_stability: Mean stability across layers
#         """
#         try:
#             # Store previous values for tracking changes
#             prev_threshold = self.collapse_dissipation_threshold
#             prev_update_rate = self.core_distinction_update_rate
# 
#             # 1. Update collapse dissipation threshold based on mean stability
#             # If system is very stable, can raise the threshold (less likely to trigger dissipation)
#             if mean_stability > 0.7 and self.metrics['collapse_probability'] < 0.2:
#                 threshold_adjustment = 0.01 * self.constants_adaptation_rate * (mean_stability - 0.5)
# 
#                 # Update momentum
#                 self.threshold_momentum = update_momentum(
#                     self.threshold_momentum,
#                     threshold_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.collapse_dissipation_threshold += threshold_adjustment + 0.1 * self.threshold_momentum
#                 self.collapse_dissipation_threshold = np.clip(self.collapse_dissipation_threshold, 0.25, 0.5)
# 
#                 logger.info(f"Increased collapse_dissipation_threshold to {self.collapse_dissipation_threshold:.4f}")
# 
#             # If system is unstable, lower the threshold (more likely to trigger dissipation)
#             elif mean_stability < 0.4 or self.metrics['collapse_probability'] > 0.6:
#                 threshold_adjustment = -0.01 * self.constants_adaptation_rate
# 
#                 # Update momentum
#                 self.threshold_momentum = update_momentum(
#                     self.threshold_momentum,
#                     threshold_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.collapse_dissipation_threshold += threshold_adjustment + 0.1 * self.threshold_momentum
#                 self.collapse_dissipation_threshold = np.clip(self.collapse_dissipation_threshold, 0.25, 0.5)
# 
#                 logger.info(f"Decreased collapse_dissipation_threshold to {self.collapse_dissipation_threshold:.4f}")
# 
#             # 2. Update core distinction update rate based on distinction level and prediction error
#             # When prediction error is high, faster updates may be needed
#             if prediction_error > 0.5 and phase_coherence > 0.6:
#                 rate_adjustment = 0.001 * self.constants_adaptation_rate * prediction_error
# 
#                 # Update momentum
#                 self.rate_momentum = update_momentum(
#                     self.rate_momentum,
#                     rate_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.core_distinction_update_rate += rate_adjustment + 0.05 * self.rate_momentum
#                 self.core_distinction_update_rate = np.clip(self.core_distinction_update_rate, 0.005, 0.05)
# 
#                 logger.info(f"Increased core_distinction_update_rate to {self.core_distinction_update_rate:.4f}")
# 
#             # When system is working well, can use slower updates
#             elif prediction_error < 0.2 and mean_stability > 0.7:
#                 rate_adjustment = -0.001 * self.constants_adaptation_rate
# 
#                 # Update momentum
#                 self.rate_momentum = update_momentum(
#                     self.rate_momentum,
#                     rate_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.core_distinction_update_rate += rate_adjustment + 0.05 * self.rate_momentum
#                 self.core_distinction_update_rate = np.clip(self.core_distinction_update_rate, 0.005, 0.05)
# 
#                 logger.info(f"Decreased core_distinction_update_rate to {self.core_distinction_update_rate:.4f}")
# 
#             # 3. Update target distinction based on current distinction level
#             distinction_delta = distinction_level - self.target_distinction
#             if abs(distinction_delta) > 0.2 and mean_stability > 0.6:
#                 # Move target distinction toward actual distinction if stable
#                 self.target_distinction += 0.01 * self.constants_adaptation_rate * distinction_delta
#                 self.target_distinction = np.clip(self.target_distinction, 0.3, 0.9)
# 
#                 logger.info(f"Adjusted target_distinction to {self.target_distinction:.4f}")
# 
#             # 4. Update stability and coherence weights based on which is more effective
#             # Compare recent stability-driven vs. coherence-driven adaptation success
#             # This would require more detailed tracking, simplified version here:
#             if phase_coherence > mean_stability and prediction_error < 0.3:
#                 # Coherence seems more reliable
#                 self.coherence_weight = min(0.6, self.coherence_weight + 0.01)
#                 self.stability_weight = 1.0 - self.coherence_weight
#                 logger.info(f"Adjusted weights: coherence={self.coherence_weight:.2f}, stability={self.stability_weight:.2f}")
#             elif mean_stability > phase_coherence and prediction_error < 0.3:
#                 # Stability seems more reliable
#                 self.stability_weight = min(0.8, self.stability_weight + 0.01)
#                 self.coherence_weight = 1.0 - self.stability_weight
#                 logger.info(f"Adjusted weights: coherence={self.coherence_weight:.2f}, stability={self.stability_weight:.2f}")
# 
#             # Record significant constant changes
#             if (abs(self.collapse_dissipation_threshold - prev_threshold) > 0.02 or
#                 abs(self.core_distinction_update_rate - prev_update_rate) > 0.002):
#                 self._record_constants_state("significant_update")
# 
#             # Make sure identity_preservation_rate is kept in sync with core_distinction_update_rate
#             self.identity_preservation_rate = self.core_distinction_update_rate
# 
#         except Exception as e:
#             logger.error(f"Error updating cognitive structure dynamic constants: {e}")
# 
# 
#     def _sigmoid(self, x: float) -> float:
#         """
#         Helper function to calculate sigmoid for smooth transitions.
# 
#         Args:
#             x: Input value
# 
#         Returns:
#             Sigmoid of input value
#         """
#         try:
#             return 1.0 / (1.0 + np.exp(-x))
#         except OverflowError:
#             # Handle extreme values
#             return 0.0 if x < 0 else 1.0
#         except Exception as e:
#             logger.error(f"Error in sigmoid calculation: {e}")
#             return 0.5  # Safe default
# 
#     def update(self, phase_coherence: float, distinction_level: float,
#            surplus: Dict[str, float], prediction_error: float,
#            quantum_metrics: Dict[str, float]) -> bool:
#         """
#         Update the recursive cognitive structure based on current metrics.
#         Includes adaptive feedback strength based on stability and coherence.
# 
#         Args:
#             phase_coherence: Quantum phase coherence
#             distinction_level: Current distinction level
#             surplus: Dictionary of surplus values
#             prediction_error: Error in prediction
#             quantum_metrics: Dictionary of quantum metrics
# 
#         Returns:
#             True if update successful, False otherwise
#         """
#         try:
#             # Validate inputs
#             if not self._validate_cognitive_update(phase_coherence, distinction_level, surplus, prediction_error):
#                 logger.warning("Invalid inputs for cognitive update")
#                 return False
# 
#             # Apply safe casting to float values
#             phase_coherence = ensure_real(phase_coherence, 0.5)
#             distinction_level = ensure_real(distinction_level, 0.5)
#             prediction_error = ensure_real(prediction_error, 0.0)
# 
#             # Extract additional metrics for enhanced behavior
#             normalized_entropy = quantum_metrics.get('normalized_entropy', 0.5)
#             phase_distinction = quantum_metrics.get('phase_distinction', 0.0)
#             stability = quantum_metrics.get('stability', 1.0)
#             quantum_coupling = quantum_metrics.get('quantum_coupling', 1.0)
#             quantum_surplus_coupling = quantum_metrics.get('quantum_surplus_coupling', 1.0)
# 
#             # Compute base adaptation strength with enhanced metrics
#             base_strength = phase_coherence * distinction_level * (1.0 - normalized_entropy)
# 
#             # Get quantum influence factor - enhanced with multiple metrics
#             quantum_factor = (1.0 - normalized_entropy) * (0.5 + 0.5 * quantum_coupling)
# 
#             # Update coupling momentum
#             self.coupling_momentum = MOMENTUM_DECAY * self.coupling_momentum + (1 - MOMENTUM_DECAY) * (quantum_factor - 0.5)
# 
#             # Calculate adaptation strength with quantum influence
#             adaptation_strength = (
#                 base_strength *
#                 (1.0 + quantum_factor) *
#                 (1.0 + 0.5 * phase_coherence) *
#                 (1.0 + self.coupling_momentum)
#             )
# 
#             # Calculate per-layer stability and update the feedback matrix
#             self._update_feedback_matrix(phase_coherence, quantum_metrics)
# 
#             # Update each layer
#             for i, layer in enumerate(self.layers):
#                 # Calculate depth factor (deeper layers update more slowly)
#                 depth_factor = 0.7 ** i
# 
#                 # Enhanced strength update using more metrics
#                 strength_update = (
#                     prediction_error *
#                     (1 - distinction_level) *
#                     phase_coherence *
#                     depth_factor *
#                     layer.get('collapse_resistance', 0.3) *
#                     (1.0 + 0.1 * phase_distinction)  # Add phase distinction influence
#                 )
# 
#                 # Update adaptation momentum
#                 self.adaptation_momentum[i] = (
#                     MOMENTUM_DECAY * self.adaptation_momentum[i] +
#                     (1 - MOMENTUM_DECAY) * strength_update
#                 )
# 
#                 # Update strength with momentum influence
#                 layer['strength'] += (
#                     strength_update * adaptation_strength +
#                     STRENGTH_MOMENTUM_INFLUENCE * self.adaptation_momentum[i]
#                 )
# 
#                 # Enhanced stability update with quantum metrics influence
#                 stability_update = (
#                     phase_coherence * 0.01 +
#                     (1 - prediction_error) * 0.01 +
#                     stability * 0.005  # Add overall stability influence
#                 )
# 
#                 layer['stability'] = np.clip(
#                     layer['stability'] * self.stability_decay +
#                     stability_update * (phase_coherence * depth_factor),
#                     MIN_STABILITY, MAX_STABILITY
#                 )
# 
#                 # Enhanced quantum coupling update
#                 coupling_update = quantum_metrics.get('phase_coherence', 0.5) * (1 - quantum_metrics.get('normalized_entropy', 0.5))
#                 layer['quantum_coupling'] = 0.9 * layer['quantum_coupling'] + 0.1 * coupling_update
# 
#                 # Update adaptation rate with quantum surplus coupling
#                 layer['adaptation_rate'] = np.clip(
#                     layer['adaptation_rate'] * (1.0 + 0.1 * (quantum_surplus_coupling - 0.5)),
#                     MIN_ADAPTATION_RATE, MAX_ADAPTATION_RATE
#                 )
# 
#                 # === Enhanced Adaptive Feedback Strength (now per-layer) ===
#                 # Use weighted average for combined metric with more inputs
#                 combined_metric = (
#                     self.stability_weight * layer['stability'] +
#                     self.coherence_weight * phase_coherence +
#                     0.1 * (1.0 - normalized_entropy)  # Add entropy influence
#                 )
# 
#                 # Scale and shift the sigmoid for smooth adaptation
#                 adaptive_feedback = layer['base_feedback_strength'] * self._sigmoid(
#                     SIGMOID_STEEPNESS * (combined_metric - SIGMOID_MIDPOINT)
#                 )
# 
#                 layer['feedback_strength'] = np.clip(adaptive_feedback, 0.1, 1.0)
# 
#                 # Update feedback momentum - used for tracking, not for computation
#                 self.feedback_momentum[i] = (
#                     MOMENTUM_DECAY * self.feedback_momentum[i] +
#                     (1 - MOMENTUM_DECAY) * strength_update
#                 )
# 
#                 # Enhanced collapse resistance update
#                 collapse_update = (
#                     self.collapse_resistance_base *
#                     (1.0 + layer['stability']) *
#                     (1.0 + layer['quantum_coupling']) *
#                     (1.0 + 0.1 * quantum_surplus_coupling)  # Add quantum surplus coupling influence
#                 )
# 
#                 layer['collapse_resistance'] = np.clip(
#                    layer['collapse_resistance'] * self.collapse_resistance_decay +
#                    self.collapse_resistance_update_rate * collapse_update,
#                    self.min_collapse_resistance, self.max_collapse_resistance
#                 )
# 
#                 # Enhanced core identity update with inter-layer feedback
#                 identity_update = (
#                     distinction_level *
#                     layer['stability'] *
#                     layer['quantum_coupling'] *
#                     (1.0 + 0.1 * phase_distinction)  # Add phase distinction influence
#                 )
# 
#                 # Enhanced core identity update with inter-layer feedback
#                 core_identity_update = identity_update
#                 for j, other_layer in enumerate(self.layers):
#                     if i != j:  # Don't provide feedback to itself
#                         # Get feedback strength from matrix (asymmetric)
#                         feedback_strength = self.feedback_matrix[j, i]
#                         # Add feedback contribution
#                         core_identity_update += (
#                             other_layer['strength'] *
#                             other_layer['stability'] *
#                             feedback_strength *
#                             0.01  # Small weight factor to prevent overinfluence
#                         )
# 
#                 # Apply the core identity update - using dynamic update rate
#                 layer['core_identity'] = (
#                     (1 - self.identity_preservation_rate) * layer['core_identity'] +
#                     self.identity_preservation_rate * core_identity_update
#                 )
# 
#             # Track layer history
#             self.history.append([layer.copy() for layer in self.layers])
# 
#             # Track identity history
#             self.identity_history.append([layer['core_identity'] for layer in self.layers])
# 
#             # Track feedback history
#             self.feedback_history.append({
#                 'mean_feedback': np.mean([layer['feedback_strength'] for layer in self.layers]),
#                 'feedback_matrix': self.feedback_matrix.copy(),
#                 'timestamp': time.time()
#             })
# 
#             # Update metrics
#             self.metrics.update({
#                 'last_update_time': time.time(),
#                 'updates_count': self.metrics['updates_count'] + 1,
#                 'mean_strength': np.mean([layer['strength'] for layer in self.layers]),
#                 'mean_stability': np.mean([layer['stability'] for layer in self.layers]),
#                 'mean_feedback': np.mean([layer['feedback_strength'] for layer in self.layers]),
#                 'quantum_influence': quantum_factor
#             })
# 
#             # Track distinction level
#             self.track_history(distinction_level)
# 
#             # Update dynamic constants
#             self.update_dynamic_constants(
#                 phase_coherence=phase_coherence,
#                 distinction_level=distinction_level,
#                 prediction_error=prediction_error,
#                 mean_stability=self.metrics['mean_stability']
#             )
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error in cognitive update: {e}")
#             return False
# 
#     def get_constants_history(self) -> List[Dict]:
#        """
#        Get history of dynamic constant changes.
# 
#        Returns:
#            List of constant state snapshots with timestamps
#        """
#        try:
#            return list(self.constants_history)
#        except Exception as e:
#            logger.error(f"Error getting constants history: {e}")
#            return []
# 
#     def get_dynamic_constants(self) -> Dict[str, float]:
#        """
#        Return current values of all dynamic constants.
# 
#        Returns:
#            Dictionary with current constant values
#        """
#        try:
#            return {
#                'target_distinction': self.target_distinction,
#                'core_distinction_update_rate': self.core_distinction_update_rate,
#                'stability_decay': self.stability_decay,
#                'distinction_anchor_weight': self.distinction_anchor_weight,
#                'collapse_dissipation_threshold': self.collapse_dissipation_threshold,
#                'collapse_dissipation_rate': self.collapse_dissipation_rate,
#                'collapse_resistance_base': self.collapse_resistance_base,
#                'collapse_resistance_decay': self.collapse_resistance_decay,
#                'collapse_resistance_update_rate': self.collapse_resistance_update_rate,
#                'identity_preservation_rate': self.identity_preservation_rate,
#                'stability_weight': self.stability_weight,
#                'coherence_weight': self.coherence_weight
#            }
#        except Exception as e:
#            logger.error(f"Error getting dynamic constants: {e}")
#            return {}
# 
#     def _update_feedback_matrix(self, phase_coherence: float, quantum_metrics: Dict[str, float]) -> None:
#         """
#         Update the feedback matrix for asymmetric inter-layer feedback.
# 
#         Args:
#             phase_coherence: Current phase coherence
#             quantum_metrics: Dictionary of quantum metrics
#         """
#         try:
#             num_layers = len(self.layers)
# 
#             # Calculate a global coherence factor
#             coherence_factor = phase_coherence * (1.0 - quantum_metrics.get('normalized_entropy', 0.5))
# 
#             # Update each pair's feedback strength
#             for i in range(num_layers):
#                 for j in range(num_layers):
#                     if i != j:  # No self-feedback
#                         # Get source and target layer stability
#                         source_stability = self.layers[i]['stability']
#                         target_stability = self.layers[j]['stability']
# 
#                         # Calculate combined metric with weighted stability and distance factor
#                         distance_factor = 1.0 / (1.0 + abs(i - j))
#                         combined_metric = (
#                             STABILITY_WEIGHT * (source_stability * target_stability) +
#                             COHERENCE_WEIGHT * coherence_factor
#                         ) * distance_factor
# 
#                         # Update matrix value with sigmoid
#                         self.feedback_matrix[i, j] = self.layers[i]['base_feedback_strength'] * self._sigmoid(
#                             SIGMOID_STEEPNESS * (combined_metric - SIGMOID_MIDPOINT)
#                         )
# 
#         except Exception as e:
#             logger.error(f"Error updating feedback matrix: {e}")
# 
#     def predict_cognitive_collapse(self) -> float:
#         """
#         Predict the probability of cognitive collapse.
# 
#         Returns:
#             Collapse probability (0.0 to 1.0)
#         """
#         try:
#             if not self.layers:
#                 return 0.0
# 
#             # Calculate collapse factors from all layers
#             avg_strength = np.mean([layer['strength'] * layer.get('collapse_resistance', 0.3) for layer in self.layers])
#             avg_stability = np.mean([layer['stability'] for layer in self.layers])
#             avg_coupling = np.mean([layer['quantum_coupling'] for layer in self.layers])
# 
#             # Calculate individual collapse factors
#             stability_factor = 1.0 - avg_stability
#             strength_factor = 1.0 - avg_strength
#             coupling_factor = 1.0 - avg_coupling
# 
#             # Weighted combination of factors
#             collapse_prob = (
#                 0.3 * stability_factor +
#                 0.2 * strength_factor +
#                 0.2 * coupling_factor
#             )
# 
#             # Scale by collapse resistance
#             collapse_prob *= (1.0 - self.collapse_resistance_base)
# 
#             # Ensure probability is in valid range
#             collapse_prob = float(np.clip(collapse_prob, 0, 1))
# 
#             # Track history
#             self.collapse_history.append(collapse_prob)
# 
#             # Update metrics
#             self.metrics['collapse_probability'] = collapse_prob
# 
#             return collapse_prob
# 
#         except Exception as e:
#             logger.error(f"Error predicting collapse: {e}")
#             return 0.0
# 
#     def get_cognitive_state(self) -> Dict[str, float]:
#         """
#         Get the current cognitive state summary.
# 
#         Returns:
#             Dictionary of cognitive state metrics
#         """
#         try:
#             # Calculate mean values across layers
#             avg_strength = np.mean([layer['strength'] for layer in self.layers])
#             avg_stability = np.mean([layer['stability'] for layer in self.layers])
#             avg_coupling = np.mean([layer['quantum_coupling'] for layer in self.layers])
#             avg_identity = np.mean([layer['core_identity'] for layer in self.layers])
#             avg_resistance = np.mean([layer.get('collapse_resistance', 0.3) for layer in self.layers])
#             avg_feedback = np.mean([layer['feedback_strength'] for layer in self.layers])
# 
#             # Calculate feedback matrix metrics
#             feedback_matrix_avg = np.mean(self.feedback_matrix)
#             feedback_matrix_max = np.max(self.feedback_matrix)
#             feedback_matrix_min = np.min(self.feedback_matrix[self.feedback_matrix > 0])  # Min of non-zero elements
# 
#             # Generate comprehensive state summary
#             state = {
#                 'mean_strength': float(avg_strength),
#                 'mean_stability': float(avg_stability),
#                 'mean_coupling': float(avg_coupling),
#                 'mean_identity': float(avg_identity),
#                 'mean_collapse_resistance': float(avg_resistance),
#                 'mean_feedback_strength': float(avg_feedback),
#                 'feedback_matrix_avg': float(feedback_matrix_avg),
#                 'feedback_matrix_max': float(feedback_matrix_max),
#                 'feedback_matrix_min': float(feedback_matrix_min),
#                 'collapse_probability': float(self.predict_cognitive_collapse()),
#                 'quantum_influence': float(avg_coupling * avg_stability),
#                 'adaptation_momentum': float(np.mean(self.feedback_momentum)),
#                 'coupling_momentum': float(self.coupling_momentum),
#                 'identity_coherence': float(np.std([layer['core_identity'] for layer in self.layers]))
#             }
# 
#             # Add per-layer information
#             for i, layer in enumerate(self.layers):
#                 state[f'layer_{i}_strength'] = float(layer['strength'])
#                 state[f'layer_{i}_stability'] = float(layer['stability'])
#                 state[f'layer_{i}_coupling'] = float(layer['quantum_coupling'])
#                 state[f'layer_{i}_identity'] = float(layer['core_identity'])
#                 state[f'layer_{i}_resistance'] = float(layer.get('collapse_resistance', 0.3))
#                 state[f'layer_{i}_feedback'] = float(layer['feedback_strength'])
# 
#             return state
# 
#         except Exception as e:
#             logger.error(f"Error getting cognitive state: {e}")
#             # Return minimal state info
#             return {
#                 'mean_strength': 1.0,
#                 'mean_stability': 0.1,
#                 'mean_coupling': 0.3,
#                 'collapse_probability': 0.0,
#                 'quantum_influence': 0.3
#             }
# 
#     def dissipate_collapse(self, surplus_values: Dict[str, float]) -> Dict[str, float]:
#         """
#         Dissipate cognitive collapse by recycling surplus.
# 
#         Args:
#             surplus_values: Dictionary of surplus values
# 
#         Returns:
#             Dictionary of recycled surplus values
#         """
#         try:
#             # Calculate collapse probability
#             collapse_prob = self.predict_cognitive_collapse()
# 
#             # Check if collapse prevention is needed
#             if collapse_prob < self.collapse_threshold:
#                 return {}
# 
#             logger.info(f"Dissipating collapse: probability={collapse_prob:.4f}")
# 
#             # Calculate recycling fractions based on collapse probability
#             recycle_fraction = COLLAPSE_DISSIPATION_RATE * (collapse_prob / self.collapse_threshold)
# 
#             # Initialize recycled surplus
#             recycled = {}
# 
#             # Recycle surplus values
#             for key, value in surplus_values.items():
#                 recycle_amount = value * recycle_fraction
#                 recycled[key] = recycle_amount
# 
#             # Update feedback for each layer
#             for layer in self.layers:
#                 layer['collapse_resistance'] = min(
#                     layer['collapse_resistance'] * (1.0 + 0.1 * recycle_fraction),
#                     MAX_COLLAPSE_RESISTANCE
#                 )
# 
#             logger.info(f"Recycled surplus: {recycled}")
#             return recycled
# 
#         except Exception as e:
#             logger.error(f"Error dissipating collapse: {e}")
#             return {}
# 
#     def _validate_cognitive_update(self, phase_coherence: float,
#                                    distinction_level: float,
#                                    surplus: Dict[str, float],
#                                    prediction_error: float) -> bool:
#         """
#         Validate that the inputs for cognitive update are within expected ranges.
# 
#         Args:
#             phase_coherence: Quantum phase coherence
#             distinction_level: Current distinction level
#             surplus: Dictionary of surplus values
#             prediction_error: Error in prediction
# 
#         Returns:
#             True if inputs are valid, False otherwise
#         """
#         try:
#             # Validate value ranges
#             if not 0 <= phase_coherence <= 1:
#                 logger.warning(f"Invalid phase coherence: {phase_coherence}")
#                 return False
# 
#             if not 0 <= distinction_level <= 1:
#                 logger.warning(f"Invalid distinction level: {distinction_level}")
#                 return False
# 
#             # Validate surplus keys
#             required_keys = ['basal', 'cognitive', 'predictive', 'ontological']
#             if not all(key in surplus for key in required_keys):
#                 logger.warning(f"Missing surplus keys: {surplus.keys()}")
#                 return False
# 
#             # Validate prediction error
#             if not isinstance(prediction_error, (int, float)) or prediction_error < 0:
#                 logger.warning(f"Invalid prediction error: {prediction_error}")
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error validating cognitive update: {e}")
#             return False
# 
#     def _update_stability_metrics(self) -> None:
#         """Update cognitive stability metrics from recent adaptation history."""
#         try:
#             if not hasattr(self, 'stability_metrics'):
#                 self.stability_metrics = {
#                     'collapse_probability': 0.0,
#                     'mean_stability': 0.1,
#                     'adaptation_efficiency': 1.0
#                 }
# 
#             # Update stability metrics
#             stabilities = [layer['stability'] for layer in self.layers]
#             self.stability_metrics['mean_stability'] = float(np.mean(stabilities))
# 
#             # Update adaptation efficiency
#             recent_changes = list(self.adaptation_history)[-10:] if self.adaptation_history else []
#             if recent_changes:
#                 efficiency = np.mean([change.get('success', 0.0) for change in recent_changes])
#                 self.stability_metrics['adaptation_efficiency'] = float(efficiency)
# 
#         except Exception as e:
#             logger.error(f"Error updating stability metrics: {e}")
# 
#     def track_history(self, distinction_level: float) -> None:
#         """
#         Track historical distinction levels.
# 
#         Args:
#             distinction_level: Current distinction level to track
#         """
#         try:
#             if not self.layers:
#                 logger.warning("No layers available to track distinction history")
#                 return
# 
#             # Validate input
#             if not 0 <= distinction_level <= 1:
#                 logger.warning(f"Invalid distinction level for tracking: {distinction_level}")
#                 distinction_level = min(max(distinction_level, 0.0), 1.0)
# 
#             # Append to history
#             self.distinction_history.append(float(distinction_level))
# 
#         except Exception as e:
#             logger.error(f"Error tracking distinction history: {e}")
# 
#     def get_layer(self, index: int) -> Dict[str, float]:
#         """
#         Get layer information by index.
# 
#         Args:
#             index: Layer index (0 to num_layers-1)
# 
#         Returns:
#             Layer dictionary or empty dict if index invalid
#         """
#         try:
#             if 0 <= index < len(self.layers):
#                 return self.layers[index].copy()
#             else:
#                 logger.warning(f"Invalid layer index: {index}")
#                 return {}
# 
#         except Exception as e:
#             logger.error(f"Error getting layer: {e}")
#             return {}
# 
#     def get_historical_metrics(self) -> Dict[str, List]:
#         """
#         Get historical metrics for analysis.
# 
#         Returns:
#             Dictionary of historical metric lists
#         """
#         try:
#             # Extract relevant history for analysis
#             history_metrics = {}
# 
#             # Get strength history
#             if self.history:
#                 strength_history = [
#                     np.mean([layer['strength'] for layer in snapshot])
#                     for snapshot in self.history
#                 ]
#                 history_metrics['strength'] = list(strength_history)
# 
#             # Get stability history
#             if self.history:
#                 stability_history = [
#                     np.mean([layer['stability'] for layer in snapshot])
#                     for snapshot in self.history
#                 ]
#                 history_metrics['stability'] = list(stability_history)
# 
#             # Get collapse history
#             if self.collapse_history:
#                 history_metrics['collapse_probability'] = list(self.collapse_history)
# 
#             # Get distinction history
#             if self.distinction_history:
#                 history_metrics['distinction'] = list(self.distinction_history)
# 
#             # Get feedback strength history
#             if self.feedback_history:
#                 history_metrics['feedback_strength'] = [entry['mean_feedback'] for entry in self.feedback_history]
# 
#             # Get inter-layer feedback metrics
#             if self.feedback_history:
#                 # Calculate average feedback matrix similarity (as a measure of asymmetry)
#                 matrix_symmetry = []
#                 for entry in self.feedback_history:
#                     if 'feedback_matrix' in entry:
#                         matrix = entry['feedback_matrix']
#                         # Calculate symmetry as similarity between matrix and its transpose
#                         diff = np.abs(matrix - matrix.T)
#                         symmetry = 1.0 - (np.sum(diff) / (np.sum(matrix) + 1e-10))
#                         matrix_symmetry.append(symmetry)
# 
#                 if matrix_symmetry:
#                     history_metrics['feedback_matrix_symmetry'] = matrix_symmetry
# 
#             return history_metrics
# 
#         except Exception as e:
#             logger.error(f"Error getting historical metrics: {e}")
#             return {}
# 
#     def reset_to_baseline(self) -> bool:
#         """
#         Reset cognitive structure to baseline state.
# 
#         Returns:
#             True if reset successful, False otherwise
#         """
#         try:
#             logger.info("Resetting cognitive structure to baseline")
# 
#             # Reset each layer with original initialization pattern
#             num_layers = len(self.layers)
#             for i, layer in enumerate(self.layers):
#                 layer['strength'] = 1.0 + 0.2*i  # Increasing by depth
#                 layer['stability'] = 0.1 + 0.05 * i  # Increasing by depth
#                 layer['quantum_coupling'] = 0.3 + 0.1 * i  # Increasing by depth
#                 layer['adaptation_rate'] = 0.01 * (0.8 ** i)  # Decreasing by depth
#                 layer['base_feedback_strength'] = 0.5  # Base value
#                 layer['feedback_strength'] = 0.2 ** (num_layers - i - 1)  # Initial value
#                 layer['collapse_resistance'] = 0.3 + 0.2 * i
#                 layer['core_identity'] = 1.2 ** (num_layers - i - 1)  # Increasing by depth
# 
#             # Reset momentum
#             self.coupling_momentum = 0.0
#             self.feedback_momentum = np.zeros(len(self.layers))
#             self.identity_momentum = 0.0
#             self.adaptation_momentum = np.zeros(len(self.layers))
# 
#             # Reset feedback matrix
#             for i in range(num_layers):
#                 for j in range(num_layers):
#                     if i != j:  # No self-feedback
#                         self.feedback_matrix[i, j] = 0.2 ** abs(i - j)  # Decaying with distance
# 
#             # Track reset event
#             self.adaptation_history.append({
#                 'event': 'reset',
#                 'timestamp': time.time()
#             })
# 
#             logger.info("Reset complete")
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error resetting cognitive structure: {e}")
#             return False
# 
#     def get_feedback_matrix_visualization(self) -> Dict[str, Any]:
#         """
#         Prepare feedback matrix data for visualization.
# 
#         Returns:
#             Dictionary with matrix data and metadata
#         """
#         try:
#             # Get current matrix
#             matrix_data = self.feedback_matrix.tolist()
# 
#             # Get layer properties for context
#             layer_stability = [layer['stability'] for layer in self.layers]
#             layer_strength = [layer['strength'] for layer in self.layers]
# 
#             # Calculate properties
#             symmetry = 1.0 - (np.sum(np.abs(self.feedback_matrix - self.feedback_matrix.T)) /
#                              (np.sum(self.feedback_matrix) + 1e-10))
# 
#             feedback_influence = []
#             for i in range(len(self.layers)):
#                 # How much this layer influences others
#                 outgoing = np.sum(self.feedback_matrix[i, :])
#                 # How much this layer is influenced by others
#                 incoming = np.sum(self.feedback_matrix[:, i])
#                 feedback_influence.append({
#                     'layer': i,
#                     'outgoing': float(outgoing),
#                     'incoming': float(incoming),
#                     'net': float(outgoing - incoming)
#                 })
# 
#             return {
#                 'matrix': matrix_data,
#                 'symmetry': float(symmetry),
#                 'layer_stability': layer_stability,
#                 'layer_strength': layer_strength,
#                 'feedback_influence': feedback_influence,
#                 'timestamp': time.time()
#             }
# 
#         except Exception as e:
#             logger.error(f"Error preparing feedback matrix visualization: {e}")
#             return {
#                 'error': str(e),
#                 'matrix': [[0.0]]
#             }
# 
#     def _update_metrics(self) -> None:
#         """Update directionality and strength metrics based on the causality matrix."""
#         try:
#             self.directionality = float(np.mean(np.abs(np.diff(self.causality_matrix, axis=0))))
#             self.strength = float(np.mean(np.abs(self.causality_matrix)))
#         except Exception as e:
#             print(f"Error updating metrics: {e}")
#             self.directionality = 0.0
#             self.strength = 0.0
# 
# # For testing the module
# if __name__ == "__main__":
#     import random
# 
#     # Create recursive cognitive structure
#     cognitive = RecursiveCognitiveStructuring(num_layers=3)
# 
#     # Run a series of updates with random values
#     print("Running cognitive updates...")
#     for i in range(10):
#         # Generate random test values
#         phase_coherence = random.uniform(0.3, 0.9)
#         distinction_level = random.uniform(0.4, 0.8)
#         surplus = {
#             'basal': random.uniform(0.5, 1.5),
#             'cognitive': random.uniform(0.5, 1.5),
#             'predictive': random.uniform(0.5, 1.5),
#             'ontological': random.uniform(0.5, 1.5)
#         }
#         prediction_error = random.uniform(0.0, 0.3)
#         quantum_metrics = {
#             'phase_coherence': phase_coherence,
#             'normalized_entropy': random.uniform(0.1, 0.5)
#         }
# 
#         # Update cognitive structure
#         cognitive.update(phase_coherence, distinction_level, surplus, prediction_error, quantum_metrics)
# 
#         # Get and print cognitive state
#         state = cognitive.get_cognitive_state()
#         print(f"\nUpdate {i+1}:")
#         print(f"  Strength: {state['mean_strength']:.4f}")
#         print(f"  Stability: {state['mean_stability']:.4f}")
#         print(f"  Collapse Probability: {state['collapse_probability']:.4f}")
# 
#         # Check for collapse prevention
#         if state['collapse_probability'] > 0.5:
#             print("  Dissipating collapse...")
#             recycled = cognitive.dissipate_collapse(surplus)
#             print(f"  Recycled surplus: {recycled}")
# 
# 
#

"""# 6. Memory Field"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile memory_field.py
# """
# Memory Field Module for Émile-2 Simulation
# ------------------------------------------
# Implements hierarchical memory structures and ontological field dynamics
# for emergent behavior.
# """
# import logging
# import time
# import numpy as np
# from collections import deque, defaultdict
# from typing import Any, Dict, List, Optional, Tuple, Union
# import torch
# import traceback
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.memory_field")
# 
# # Import from other modules
# from utilities import (
#     MOMENTUM_DECAY,
#     update_momentum,
#     compute_phase_coherence,
#     compute_context_similarity
# )
# from data_classes import SurplusState
# 
# # =============================================================================
# # RecursiveDistinctionMemory
# # =============================================================================
# class RecursiveDistinctionMemory:
#     """
#     Hierarchical memory system for storing and retrieving quantum state information.
# 
#     Implements multiple memory levels with different temporal scales and prioritization,
#     along with memory consolidation and retrieval mechanisms.
#     """
#     def __init__(self, max_size: int = 10000, hierarchy_levels: int = 4):
#         """
#         Initialize the recursive memory structure.
# 
#         Args:
#             max_size: Maximum number of entries in memory
#             hierarchy_levels: Number of hierarchical levels in memory
#         """
#         # Create a list of deques, one per hierarchy level
#         self.memory = [deque(maxlen=max(max_size // hierarchy_levels, 100)) for _ in range(hierarchy_levels)]
# 
#         # Memory configuration
#         self.importance_thresholds = [0.3, 0.5, 0.7, 0.9][:hierarchy_levels]  # thresholds per level
#         self.retrieval_weights = [1.0, 0.8, 0.6, 0.4][:hierarchy_levels]      # weights for retrieval
#         self.consolidation_counters = [0] * hierarchy_levels
#         self.consolidation_threshold = 100
# 
#         # Memory statistics and tracking
#         self.access_counts = [0] * hierarchy_levels
#         self.consolidation_history = []
#         self.retrieval_stats = {'hits': 0, 'misses': 0}
#         self.memory_stability = 1.0
#         self.consolidation_momentum = np.zeros(hierarchy_levels)
# 
#         # Metrics
#         self.metrics = {
#             'total_entries': 0,
#             'last_importance': 0.0,
#             'last_retrieval_time': 0.0,
#             'memory_utilization': [0.0] * hierarchy_levels
#         }
# 
#         logger.info(f"Initialized RecursiveDistinctionMemory with {hierarchy_levels} levels")
# 
#     def store(self, phase_coherence: float, distinction_level: float,
#               surplus_state: Any, importance: Optional[float] = None) -> bool:
#         """
#         Store information in memory with hierarchical organization.
# 
#         Args:
#             phase_coherence: Quantum phase coherence
#             distinction_level: Current distinction level
#             surplus_state: SurplusState object or dict of surplus values
#             importance: Optional importance score (calculated if None)
# 
#         Returns:
#             True if storage successful, False otherwise
#         """
#         try:
#             # Process surplus_state: convert SurplusState to dict if needed
#             if isinstance(surplus_state, SurplusState):
#                 surplus_values = surplus_state.values
#             elif isinstance(surplus_state, dict):
#                 surplus_values = surplus_state
#             else:
#                 logger.warning(f"Invalid surplus_state type: {type(surplus_state)}, using default")
#                 surplus_values = {
#                     'basal': 1.0,
#                     'cognitive': 1.0,
#                     'predictive': 1.0,
#                     'ontological': 1.0
#                 }
# 
#             # Create a deep copy of surplus values to prevent shared references
#             surplus_copy = {k: float(v) for k, v in surplus_values.items()}
# 
#             # Ensure values are of correct type
#             try:
#                 phase_coherence = float(phase_coherence)
#                 distinction_level = float(distinction_level)
#             except (TypeError, ValueError) as e:
#                 logger.warning(f"Type conversion error: {e}, using default values")
#                 phase_coherence = 0.5
#                 distinction_level = 0.5
# 
#             # Compute importance if not provided
#             if importance is None:
#                 importance = self._calculate_importance(phase_coherence, distinction_level, surplus_copy)
# 
#             # Create memory entry
#             entry = {
#                 'phase_coherence': phase_coherence,
#                 'distinction_level': distinction_level,
#                 'surplus_state': surplus_copy,
#                 'importance': float(importance),
#                 'timestamp': time.time(),
#                 'stability': float(self.memory_stability)
#             }
# 
#             # Store entry in appropriate levels based on importance
#             stored_levels = []
#             for level in range(len(self.memory)):
#                 if importance > self.importance_thresholds[level]:
#                     self.memory[level].append(entry)
#                     self.access_counts[level] += 1
#                     stored_levels.append(level)
# 
#             # Update metrics
#             self.metrics['total_entries'] += 1
#             self.metrics['last_importance'] = importance
#             for level in range(len(self.memory)):
#                 self.metrics['memory_utilization'][level] = len(self.memory[level]) / self.memory[level].maxlen
# 
#             # Check for consolidation
#             self._check_consolidation()
# 
#             logger.debug(f"Stored memory entry with importance {importance:.4f} in levels {stored_levels}")
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error storing in memory: {e}")
#             return False
# 
#     def retrieve_recent(self, steps: int = 10, level: int = 0) -> List[Tuple[float, float, Dict[str, float]]]:
#         """
#         Retrieve recent memory entries from a specified level.
# 
#         Adapts to distinction trends by using deeper memory levels when distinction is volatile.
# 
#         Args:
#             steps: Number of recent entries to retrieve
#             level: Memory hierarchy level (0 is most recent)
# 
#         Returns:
#             List of (phase_coherence, distinction_level, surplus_state) tuples
#         """
#         try:
#             # Record retrieval time
#             self.metrics['last_retrieval_time'] = time.time()
# 
#             # Validate level index
#             if level >= len(self.memory) or not self.memory[level]:
#                 logger.warning(f"Invalid level {level} or empty memory")
#                 self.retrieval_stats['misses'] += 1
#                 default_entry = (1.0, 1.0, {'basal': 1.0, 'cognitive': 1.0, 'predictive': 1.0, 'ontological': 1.0})
#                 return [default_entry] * steps
# 
#             # Convert deque to list before slicing for better safety
#             memory_list = list(self.memory[level])
# 
#             # Analyze distinction volatility
#             if len(memory_list) < steps:
#                 distinction_values = [entry['distinction_level'] for entry in memory_list]
#             else:
#                 distinction_values = [entry['distinction_level'] for entry in memory_list[-steps:]]
# 
#             # Compute distinction variance to detect volatility
#             distinction_variance = np.var(distinction_values) if len(distinction_values) > 1 else 0.0
# 
#             # If distinction variance is high, retrieve from deeper levels
#             if distinction_variance > 0.15 and level < len(self.memory) - 1:
#                 logger.debug(f"High distinction variance {distinction_variance:.4f}, retrieving from level {level+1}")
#                 return self.retrieve_recent(steps, level + 1)
# 
#             # Retrieve most recent entries
#             entries = memory_list[-steps:]
# 
#             # Pad with last entry if needed
#             if len(entries) < steps and entries:
#                 entries.extend([entries[-1]] * (steps - len(entries)))
# 
#             # Update statistics
#             self.access_counts[level] += 1
#             self.retrieval_stats['hits'] += 1
# 
#             # Extract and return the required tuple format
#             result = [(
#                 entry['phase_coherence'],
#                 entry['distinction_level'],
#                 entry['surplus_state']
#             ) for entry in entries]
# 
#             return result
# 
#         except Exception as e:
#             logger.error(f"Error retrieving from memory: {e}")
#             # Return safe defaults
#             default_entry = (1.0, 1.0, {'basal': 1.0, 'cognitive': 1.0, 'predictive': 1.0, 'ontological': 1.0})
#             return [default_entry] * steps
# 
#     def retrieve_by_similarity(self, current_state: Dict[str, float],
#                               threshold: float = 0.8) -> List[Dict]:
#         """
#         Retrieve memories similar to the current state.
# 
#         Args:
#             current_state: Dictionary representing the current state
#             threshold: Similarity threshold for retrieval
# 
#         Returns:
#             List of similar memory entries with added similarity and level information
#         """
#         try:
#             similar_memories = []
# 
#             # Validate current state
#             if not isinstance(current_state, dict) or not current_state:
#                 logger.warning("Invalid current state")
#                 return []
# 
#             # Search all memory levels
#             for level in range(len(self.memory)):
#                 for entry in self.memory[level]:
#                     # Skip entries with missing surplus_state
#                     if 'surplus_state' not in entry or not isinstance(entry['surplus_state'], dict):
#                         continue
# 
#                     # Compute similarity
#                     similarity = compute_context_similarity(current_state, entry['surplus_state'])
# 
#                     # Filter by threshold
#                     if similarity > threshold:
#                         # Add similarity and level info to the entry
#                         similar_entry = entry.copy()
#                         similar_entry['similarity'] = similarity
#                         similar_entry['level'] = level
#                         similar_memories.append(similar_entry)
# 
#             # Sort results by similarity and importance
#             similar_memories.sort(key=lambda x: (x['similarity'], x.get('importance', 0.0)), reverse=True)
# 
#             logger.debug(f"Retrieved {len(similar_memories)} similar memories")
#             return similar_memories
# 
#         except Exception as e:
#             logger.error(f"Error in similarity retrieval: {e}")
#             return []
# 
#     def _calculate_importance(self, phase_coherence: float,
#                               distinction_level: float,
#                               surplus_state: Dict[str, float]) -> float:
#         """
#         Calculate an importance score for a memory entry.
# 
#         Args:
#             phase_coherence: Quantum phase coherence
#             distinction_level: Distinction level
#             surplus_state: Dictionary of surplus values
# 
#         Returns:
#             Importance score (0.0 to 1.0)
#         """
#         try:
#             # Define weights for different components
#             coherence_weight = 0.3
#             distinction_weight = 0.3
#             surplus_weight = 0.4
# 
#             # Process surplus factor (lower surplus means higher importance)
#             surplus_values = list(surplus_state.values())
#             surplus_factor = 1.0 / (1.0 + np.mean(surplus_values))
# 
#             # Add stability bonus
#             stability_bonus = 0.1 * self.memory_stability
# 
#             # Calculate combined importance score
#             importance = (
#                 coherence_weight * phase_coherence +
#                 distinction_weight * distinction_level +
#                 surplus_weight * surplus_factor +
#                 stability_bonus
#             )
# 
#             # Ensure result is in valid range
#             return float(np.clip(importance, 0, 1))
# 
#         except Exception as e:
#             logger.error(f"Error calculating importance: {e}")
#             return 0.5
# 
#     def _compute_state_similarity(self, state1: Dict[str, float],
#                                   state2: Dict[str, float]) -> float:
#         """
#         Compute cosine similarity between two state dictionaries.
# 
#         Args:
#             state1: First state dictionary
#             state2: Second state dictionary
# 
#         Returns:
#             Similarity score (0.0 to 1.0)
#         """
#         try:
#             # Find common keys
#             common_keys = set(state1.keys()) & set(state2.keys())
#             if not common_keys:
#                 return 0.0
# 
#             # Build vectors from common keys
#             vec1 = []
#             vec2 = []
#             for key in common_keys:
#                 try:
#                     vec1.append(float(state1[key]))
#                     vec2.append(float(state2[key]))
#                 except (TypeError, ValueError):
#                     # Skip keys with invalid values
#                     continue
# 
#             # Handle empty vectors
#             if not vec1:
#                 return 0.0
# 
#             # Convert to numpy arrays
#             v1 = np.array(vec1)
#             v2 = np.array(vec2)
# 
#             # Calculate norms
#             norm1 = np.linalg.norm(v1)
#             norm2 = np.linalg.norm(v2)
# 
#             # Handle zero norms
#             if norm1 == 0 or norm2 == 0:
#                 return 0.0
# 
#             # Calculate cosine similarity
#             dot_product = np.dot(v1, v2)
#             cosine_sim = dot_product / (norm1 * norm2)
# 
#             # Scale to [0, 1] range
#             similarity = (cosine_sim + 1) / 2.0
# 
#             # Weight by proportion of common keys
#             total_keys = max(len(state1), len(state2))
#             key_ratio = len(common_keys) / total_keys if total_keys > 0 else 1.0
# 
#             return float(similarity * key_ratio)
# 
#         except Exception as e:
#             logger.error(f"Error computing state similarity: {e}")
#             return 0.0
# 
#     def _check_consolidation(self) -> None:
#         """
#         Check if memory consolidation is needed and perform if necessary.
# 
#         Adjusts consolidation frequency based on surplus fluctuations.
#         """
#         try:
#             for level in range(len(self.memory) - 1):  # Skip last level
#                 self.consolidation_counters[level] += 1
# 
#                 # Adjust threshold based on average surplus in this level
#                 if self.memory[level]:
#                     avg_surplus = np.mean([
#                         sum(entry['surplus_state'].values())
#                         for entry in self.memory[level]
#                     ])
#                     adaptive_threshold = self.consolidation_threshold * (
#                         1.2 if avg_surplus > 2.0 else 0.8
#                     )
#                 else:
#                     adaptive_threshold = self.consolidation_threshold
# 
#                 # Consolidate if threshold reached
#                 if self.consolidation_counters[level] >= adaptive_threshold:
#                     self._consolidate_memory(level)
#                     self.consolidation_counters[level] = 0
# 
#         except Exception as e:
#             logger.error(f"Error checking consolidation: {e}")
# 
#     def _consolidate_memory(self, level: int) -> None:
#         """
#         Consolidate memories from one level to the next.
# 
#         Args:
#             level: Level to consolidate from (to level+1)
#         """
#         try:
#             # Skip if this is already the highest level
#             if level >= len(self.memory) - 1:
#                 return
# 
#             # Get memories at this level
#             memories = list(self.memory[level])
#             if not memories:
#                 return
# 
#             # Sort memories by importance and stability
#             memories.sort(key=lambda x: (x['importance'], x.get('stability', 0.0)), reverse=True)
# 
#             # Track redundancy (how often the same distinction state appears)
#             distinction_counts = {}
#             for entry in memories:
#                 # Create key from coherence and distinction (rounded to reduce noise)
#                 distinction_state = (
#                     round(entry['phase_coherence'], 2),
#                     round(entry['distinction_level'], 2)
#                 )
#                 distinction_counts[distinction_state] = distinction_counts.get(distinction_state, 0) + 1
# 
#             # Remove redundant distinction states (keep only a few of each)
#             filtered_memories = []
#             for entry in memories:
#                 distinction_state = (
#                     round(entry['phase_coherence'], 2),
#                     round(entry['distinction_level'], 2)
#                 )
# 
#                 if distinction_counts[distinction_state] > 5:
#                     # Reduce count for next entry with this state
#                     distinction_counts[distinction_state] -= 1
#                 else:
#                     # Keep entries with unique or rare states
#                     filtered_memories.append(entry)
# 
#             # Calculate how many memories to consolidate
#             consolidation_count = max(
#                 int(len(filtered_memories) * 0.25 * (1.0 + 0.1 * self.consolidation_momentum[level])),
#                 1
#             )
# 
#             # Select top memories for consolidation
#             top_memories = filtered_memories[:consolidation_count]
# 
#             # Move selected memories to next level if important enough
#             consolidated_count = 0
#             for memory in top_memories:
#                 if memory['importance'] > self.importance_thresholds[level + 1]:
#                     self.memory[level + 1].append(memory)
#                     consolidated_count += 1
# 
#             # Record consolidation event
#             self.consolidation_history.append({
#                 'level': level,
#                 'consolidated_count': consolidated_count,
#                 'timestamp': time.time(),
#                 'momentum': self.consolidation_momentum[level]
#             })
# 
#             logger.debug(f"Consolidated {consolidated_count} memories from level {level} to {level+1}")
# 
#         except Exception as e:
#             logger.error(f"Error consolidating memory: {e}")
# 
#     def get_memory_stats(self) -> Dict[str, Any]:
#         """
#         Get statistics about memory usage and performance.
# 
#         Returns:
#             Dictionary of memory statistics
#         """
#         try:
#             # Calculate memory statistics
#             stats = {
#                 'memory_usage': [len(m) for m in self.memory],
#                 'memory_capacity': [m.maxlen for m in self.memory],
#                 'usage_percentage': [len(m) / m.maxlen if m.maxlen else 0 for m in self.memory],
#                 'access_counts': self.access_counts.copy(),
#                 'retrieval_hits': self.retrieval_stats['hits'],
#                 'retrieval_misses': self.retrieval_stats['misses'],
#                 'hit_rate': (self.retrieval_stats['hits'] /
#                              (self.retrieval_stats['hits'] + self.retrieval_stats['misses'])
#                              if (self.retrieval_stats['hits'] + self.retrieval_stats['misses']) > 0 else 0),
#                 'consolidation_events': len(self.consolidation_history),
#                 'memory_stability': self.memory_stability,
#                 'consolidation_momentum': self.consolidation_momentum.tolist(),
#                 'avg_importance': []
#             }
# 
#             # Calculate average importance per level
#             for level in range(len(self.memory)):
#                 if self.memory[level]:
#                     avg_imp = np.mean([m['importance'] for m in self.memory[level]])
#                     stats['avg_importance'].append(float(avg_imp))
#                 else:
#                     stats['avg_importance'].append(0.0)
# 
#             # Add recent consolidation events
#             if self.consolidation_history:
#                 recent_events = self.consolidation_history[-5:]
#                 stats['recent_consolidations'] = [
#                     {
#                         'level': event['level'],
#                         'count': event['consolidated_count'],
#                         'time_ago': time.time() - event['timestamp']
#                     }
#                     for event in recent_events
#                 ]
# 
#             return stats
# 
#         except Exception as e:
#             logger.error(f"Error getting memory stats: {e}")
#             return {}
# 
#     def clear_memory(self) -> None:
#         """Clear all memory levels."""
#         try:
#             for level in range(len(self.memory)):
#                 self.memory[level].clear()
# 
#             # Reset counters
#             self.consolidation_counters = [0] * len(self.memory)
#             self.access_counts = [0] * len(self.memory)
#             self.retrieval_stats = {'hits': 0, 'misses': 0}
# 
#             logger.info(f"Cleared all memory levels")
# 
#         except Exception as e:
#             logger.error(f"Error clearing memory: {e}")
# 
#     def prune_by_age(self, max_age_seconds: float) -> int:
#         """
#         Remove memories older than the specified age.
# 
#         Args:
#             max_age_seconds: Maximum age in seconds
# 
#         Returns:
#             Number of memories pruned
#         """
#         try:
#             current_time = time.time()
#             pruned_count = 0
# 
#             for level in range(len(self.memory)):
#                 original_size = len(self.memory[level])
# 
#                 # Filter memories by age
#                 self.memory[level] = deque(
#                     entry for entry in self.memory[level]
#                     if current_time - entry['timestamp'] <= max_age_seconds
#                 )
# 
#                 # Count pruned entries
#                 pruned_count += original_size - len(self.memory[level])
# 
#             logger.info(f"Pruned {pruned_count} memories older than {max_age_seconds} seconds")
#             return pruned_count
# 
#         except Exception as e:
#             logger.error(f"Error pruning memory by age: {e}")
#             return 0
# 
# # =============================================================================
# # OntologicalField
# # =============================================================================
# class OntologicalField:
#     """
#     Ontological field with sophisticated resistance and adaptation mechanisms.
# 
#     Implements field dynamics that influence—and are influenced by—the agent's
#     quantum distinctions, representing the environment that the agent interacts with.
#     """
#     def __init__(self, field_size: int = 500,
#                  resistance_factor: float = 0.02,
#                  adaptation_rate: float = 0.01):
#         """
#         Initialize the ontological field.
# 
#         Args:
#             field_size: Size of the field array
#             resistance_factor: Base factor for field resistance
#             adaptation_rate: Rate of field adaptation to agent states
#         """
#         # Initialize field as uniform random array
#         self.field = np.random.uniform(0, 1, size=field_size)
# 
#         # Convert constants to dynamic instance variables
#         self.resistance_factor = resistance_factor
#         self.adaptation_rate = adaptation_rate
#         self.field_threshold = 0.1  # Default threshold for field updates
#         self.momentum_decay = 0.7  # From MOMENTUM_DECAY
# 
#         # Additional parameters for dynamic adaptation
#         self.min_adaptation_rate = 0.001
#         self.max_adaptation_rate = 0.05
#         self.min_resistance_factor = 0.005
#         self.max_resistance_factor = 0.1
#         self.constants_adaptation_rate = 0.01
# 
#         # Momentum for constant updates
#         self.adaptation_rate_momentum = 0.0
#         self.resistance_factor_momentum = 0.0
#         self.field_threshold_momentum = 0.0
# 
#         # Track historical data
#         self.field_history = deque(maxlen=1000)
#         self.resistance_history = deque(maxlen=1000)
#         self.adaptation_history = deque(maxlen=1000)
#         self.constants_history = deque(maxlen=100)
# 
#         # Dynamic parameters
#         self.field_momentum = np.zeros(field_size)
#         self.field_gradient = np.zeros(field_size)
#         self.stability_factor = 1.0
#         self.coherence_coupling = 0.0
# 
#         # Momentum tracking
#         self.resistance_momentum = 0.0
#         self.adaptation_momentum = 0.0
# 
#         # Statistics tracking
#         self.stats = {
#             'mean_resistance': [],
#             'field_stability': [],
#             'adaptation_events': 0,
#             'coherence_history': []
#         }
# 
#         # Record initial constants
#         self._record_constants_state("initialization")
# 
#         logger.info(f"Initialized OntologicalField with size {field_size}")
# 
#     def _record_constants_state(self, event_type: str) -> None:
#         """Record the current state of all dynamic constants for tracking changes over time."""
#         try:
#             constants_state = {
#                 'resistance_factor': self.resistance_factor,
#                 'adaptation_rate': self.adaptation_rate,
#                 'field_threshold': self.field_threshold,
#                 'momentum_decay': self.momentum_decay,
#                 'event_type': event_type,
#                 'timestamp': time.time()
#             }
#             self.constants_history.append(constants_state)
#         except Exception as e:
#             logger.error(f"Error recording constants state: {e}")
# 
#     def update_dynamic_constants(self,
#                               distinction_level: float,
#                               distinction_change: float,
#                               field_stability: float,
#                               quantum_coupling: float = 0.5) -> None:
#         """
#         Update dynamic constants based on system performance and behavior.
# 
#         Args:
#             distinction_level: Current distinction level
#             distinction_change: Magnitude of recent distinction change
#             field_stability: Current field stability
#             quantum_coupling: Quantum coupling strength (default 0.5)
#         """
#         try:
#             # Store previous values for tracking changes
#             prev_adaptation_rate = self.adaptation_rate
#             prev_resistance_factor = self.resistance_factor
# 
#             # 1. Update adaptation rate based on distinction change and field stability
#             # If distinction is changing rapidly, we may need faster adaptation
#             if distinction_change > 0.1 and field_stability > 0.5:
#                 # Calculate adaptation rate adjustment
#                 adaptation_adjustment = 0.002 * self.constants_adaptation_rate * distinction_change
# 
#                 # Update momentum
#                 self.adaptation_rate_momentum = update_momentum(
#                     self.adaptation_rate_momentum,
#                     adaptation_adjustment,
#                     decay=self.momentum_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.adaptation_rate += adaptation_adjustment + 0.1 * self.adaptation_rate_momentum
#                 self.adaptation_rate = np.clip(self.adaptation_rate, self.min_adaptation_rate, self.max_adaptation_rate)
# 
#                 logger.info(f"Increased adaptation_rate to {self.adaptation_rate:.4f} based on distinction change")
# 
#             # If system is stable with minimal changes, can reduce adaptation rate
#             elif distinction_change < 0.05 and field_stability > 0.8:
#                 # Calculate adaptation rate adjustment
#                 adaptation_adjustment = -0.001 * self.constants_adaptation_rate
# 
#                 # Update momentum
#                 self.adaptation_rate_momentum = update_momentum(
#                     self.adaptation_rate_momentum,
#                     adaptation_adjustment,
#                     decay=self.momentum_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.adaptation_rate += adaptation_adjustment + 0.1 * self.adaptation_rate_momentum
#                 self.adaptation_rate = np.clip(self.adaptation_rate, self.min_adaptation_rate, self.max_adaptation_rate)
# 
#                 logger.info(f"Decreased adaptation_rate to {self.adaptation_rate:.4f} based on stability")
# 
#             # 2. Update resistance factor based on field stability and quantum coupling
#             # If field is unstable, increase resistance to slow down changes
#             if field_stability < 0.4:
#                 # Calculate resistance factor adjustment
#                 resistance_adjustment = 0.002 * self.constants_adaptation_rate * (1.0 - field_stability)
# 
#                 # Update momentum
#                 self.resistance_factor_momentum = update_momentum(
#                     self.resistance_factor_momentum,
#                     resistance_adjustment,
#                     decay=self.momentum_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.resistance_factor += resistance_adjustment + 0.1 * self.resistance_factor_momentum
#                 self.resistance_factor = np.clip(self.resistance_factor, self.min_resistance_factor, self.max_resistance_factor)
# 
#                 logger.info(f"Increased resistance_factor to {self.resistance_factor:.4f} based on instability")
# 
#             # If quantum coupling is high and field is stable, can reduce resistance
#             elif quantum_coupling > 0.7 and field_stability > 0.7:
#                 # Calculate resistance factor adjustment
#                 resistance_adjustment = -0.001 * self.constants_adaptation_rate * quantum_coupling
# 
#                 # Update momentum
#                 self.resistance_factor_momentum = update_momentum(
#                     self.resistance_factor_momentum,
#                     resistance_adjustment,
#                     decay=self.momentum_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.resistance_factor += resistance_adjustment + 0.1 * self.resistance_factor_momentum
#                 self.resistance_factor = np.clip(self.resistance_factor, self.min_resistance_factor, self.max_resistance_factor)
# 
#                 logger.info(f"Decreased resistance_factor to {self.resistance_factor:.4f} based on quantum coupling")
# 
#             # 3. Update field threshold based on stability and distinction level
#             # If field is stable, can use a higher threshold to prevent unnecessary updates
#             if field_stability > 0.8:
#                 # Calculate field threshold adjustment
#                 threshold_adjustment = 0.005 * self.constants_adaptation_rate * field_stability
# 
#                 # Update momentum
#                 self.field_threshold_momentum = update_momentum(
#                     self.field_threshold_momentum,
#                     threshold_adjustment,
#                     decay=self.momentum_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.field_threshold += threshold_adjustment + 0.05 * self.field_threshold_momentum
#                 self.field_threshold = np.clip(self.field_threshold, 0.05, 0.2)
# 
#                 logger.info(f"Increased field_threshold to {self.field_threshold:.4f} based on stability")
# 
#             # If distinction is extreme, lower threshold to ensure field responds
#             elif abs(distinction_level - 0.5) > 0.3:
#                 # Calculate field threshold adjustment
#                 threshold_adjustment = -0.005 * self.constants_adaptation_rate
# 
#                 # Update momentum
#                 self.field_threshold_momentum = update_momentum(
#                     self.field_threshold_momentum,
#                     threshold_adjustment,
#                     decay=self.momentum_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.field_threshold += threshold_adjustment + 0.05 * self.field_threshold_momentum
#                 self.field_threshold = np.clip(self.field_threshold, 0.05, 0.2)
# 
#                 logger.info(f"Decreased field_threshold to {self.field_threshold:.4f} based on extreme distinction")
# 
#             # Record significant constant changes
#             if (abs(self.adaptation_rate - prev_adaptation_rate) > 0.002 or
#                 abs(self.resistance_factor - prev_resistance_factor) > 0.002):
#                 self._record_constants_state("significant_update")
# 
#         except Exception as e:
#             logger.error(f"Error updating field dynamic constants: {e}")
# 
#     def resistance(self, agent_distinction: float) -> float:
#         """
#         Calculate the field resistance based on the agent's distinction.
# 
#         Uses a combination of mean field value, gradient, momentum, and
#         coherence coupling to determine resistance.
# 
#         Args:
#             agent_distinction: The agent's current distinction level
# 
#         Returns:
#             Field resistance value (0.0 to 1.0)
#         """
#         try:
#             # Ensure agent_distinction is within valid range
#             agent_distinction = float(np.clip(agent_distinction, 0.0, 1.0))
# 
#             # Calculate base resistance from field-distinction difference
#             base_resistance = abs(np.mean(self.field) - agent_distinction)
# 
#             # Calculate smooth resistance using historical values
#             if len(self.resistance_history) >= 10:
#                 past_resistance_avg = np.mean(list(self.resistance_history)[-10:])
#             else:
#                 past_resistance_avg = base_resistance
# 
#             # Update resistance momentum to retain history of past influence
#             self.resistance_momentum = update_momentum(
#                 self.resistance_momentum,
#                 base_resistance,
#                 MOMENTUM_DECAY
#             )
# 
#             # Calculate final resistance with smoothing and momentum
#             resistance = (
#                 (0.6 * past_resistance_avg + 0.4 * base_resistance) *
#                 self.resistance_factor *
#                 (1.0 + 0.1 * self.resistance_momentum)
#             )
# 
#             # Store in history
#             self.resistance_history.append(resistance)
# 
#             # Ensure valid range
#             resistance = float(np.clip(resistance, 0, 1))
# 
#             logger.debug(f"Field resistance: {resistance:.4f} (base: {base_resistance:.4f})")
#             return resistance
# 
#         except Exception as e:
#             logger.error(f"Error calculating field resistance: {e}")
#             return self.resistance_factor  # Return default value on error
# 
#     def adapt_to_agent(self, agent_distinction: float, quantum_coupling: float = 1.0,
#                   field_threshold: Optional[float] = None, excess_stability: float = 0.0) -> None:
#         """
#         Adapt the field to the agent's state with stability-aware dynamics.
# 
#         Args:
#             agent_distinction: The agent's current distinction level
#             quantum_coupling: The quantum coupling strength
#             field_threshold: The threshold for field updates (uses instance value if None)
#             excess_stability: Amount of excess stability potential to consider
#         """
#         try:
#             # Validate inputs
#             agent_distinction = float(np.clip(agent_distinction, 0.0, 1.0))
#             quantum_coupling = float(np.clip(quantum_coupling, 0.0, 1.0))
# 
#             # Use instance field_threshold if none provided
#             if field_threshold is None:
#                 field_threshold = self.field_threshold
# 
#             # Track distinction stability - prevent adaptation if distinction hasn't meaningfully changed
#             if len(self.stats['coherence_history']) >= 5:
#                 recent_coherence = np.mean(self.stats['coherence_history'][-5:])
#             else:
#                 recent_coherence = 0.5  # Default mid-value
# 
#             # Calculate distinction change magnitude
#             distinction_change = abs(agent_distinction - recent_coherence)
# 
#             # Consider excess stability when determining whether to skip adaptation
#             skip_threshold = 0.05
#             if excess_stability > 0:
#                 # Lower the skip threshold when excess stability exists
#                 skip_threshold *= max(0.5, 1.0 - excess_stability)
# 
#             # Skip adaptation if distinction hasn't shifted enough
#             if distinction_change < skip_threshold:
#                 logger.debug("Skipping field adaptation: minimal distinction change")
#                 return
# 
#             # Calculate adaptation strength - using dynamic adaptation_rate
#             adaptation_strength = self.adaptation_rate * quantum_coupling
# 
#             # Enhance adaptation strength with excess stability
#             if excess_stability > 0:
#                 adaptation_boost = 1.0 + (excess_stability * 2.0)
#                 adaptation_strength *= adaptation_boost
#                 print(f"DEBUG: Boosting field adaptation by factor {adaptation_boost:.4f} due to excess stability")
# 
#             # Get stability trend
#             stability_trend = np.mean(self.stats['field_stability'][-10:]) if len(self.stats['field_stability']) > 10 else 1.0
#             stability_factor = 1.0 - np.clip(stability_trend, 0.5, 1.0)
# 
#             # Update adaptation momentum for smoother changes
#             self.adaptation_momentum = (
#                 self.momentum_decay * self.adaptation_momentum +
#                 (1 - self.momentum_decay) * adaptation_strength
#             )
# 
#             # Calculate target state and update field
#             target_state = agent_distinction
#             current_mean = np.mean(self.field)
# 
#             # Calculate field updates with momentum
#             field_update = (
#                 (target_state - self.field) * adaptation_strength * stability_factor +
#                 self.field_momentum * 0.1 +
#                 0.05 * self.adaptation_momentum
#             )
# 
#             # Modify field update threshold based on excess stability
#             effective_threshold = field_threshold
#             if excess_stability > 0:
#                 # Lower threshold when excess stability exists for more responsive field
#                 effective_threshold *= max(0.3, 1.0 - excess_stability)
#                 print(f"DEBUG: Lowering field update threshold from {field_threshold} to {effective_threshold} due to excess stability")
# 
#             # Apply updates only where they exceed threshold
#             update_mask = np.abs(field_update) > effective_threshold
#             self.field[update_mask] += field_update[update_mask]
# 
#             # Store previous field and calculate gradient
#             prev_field = self.field.copy()
#             self.field_gradient = self.field - prev_field
# 
#             # Enforce field bounds
#             self.field = np.clip(self.field, 0, 1)
# 
#             # Update stability factor
#             self.stability_factor = self._calculate_stability()
# 
#             # Record adaptation
#             self.adaptation_history.append({
#                 'target': target_state,
#                 'mean_update': float(np.mean(np.abs(field_update))),
#                 'stability': float(self.stability_factor),
#                 'adaptation_momentum': float(self.adaptation_momentum),
#                 'excess_stability': excess_stability,
#                 'timestamp': time.time()
#             })
# 
#             # Update statistics
#             self.stats['adaptation_events'] += 1
#             self.field_history.append(self.field.copy())
# 
#             # Update dynamic constants
#             self.update_dynamic_constants(
#                 distinction_level=agent_distinction,
#                 distinction_change=distinction_change,
#                 field_stability=self.stability_factor,
#                 quantum_coupling=quantum_coupling
#             )
# 
#             logger.debug(
#                 f"Field adapted: target={target_state:.4f}, "
#                 f"mean update={np.mean(np.abs(field_update)):.4f}, "
#                 f"stability={self.stability_factor:.4f}, "
#                 f"excess_stability={excess_stability:.4f}"
#             )
# 
#         except Exception as e:
#             logger.error(f"Error adapting field: {e}")
# 
#     def get_dynamic_constants(self) -> Dict[str, float]:
#         """
#         Return the current values of all dynamic constants.
# 
#         Returns:
#             Dictionary of dynamic constants and their current values
#         """
#         try:
#             return {
#                 'resistance_factor': self.resistance_factor,
#                 'adaptation_rate': self.adaptation_rate,
#                 'field_threshold': self.field_threshold,
#                 'momentum_decay': self.momentum_decay,
#                 'constants_adaptation_rate': self.constants_adaptation_rate
#             }
#         except Exception as e:
#             logger.error(f"Error getting dynamic constants: {e}")
#             return {}
# 
#     def get_constants_history(self) -> List[Dict]:
#         """
#         Get history of dynamic constant changes.
# 
#         Returns:
#             List of constant state snapshots with timestamps
#         """
#         try:
#             return list(self.constants_history)
#         except Exception as e:
#             logger.error(f"Error getting constants history: {e}")
#             return []
# 
#     def _calculate_stability(self) -> float:
#         """
#         Calculate field stability based on recent history and distinction momentum.
# 
#         Returns:
#             Stability factor (0.0 to 1.0)
#         """
#         try:
#             # Can't calculate stability without history
#             if len(self.field_history) < 2:
#                 return 1.0
# 
#             # Track recent field changes and distinction momentum
#             recent_changes = []
#             distinction_momentum = []
# 
#             # Calculate changes for the last 10 history points (or fewer if not available)
#             for i in range(1, min(10, len(self.field_history))):
#                 # Calculate mean absolute change between consecutive field states
#                 change = np.mean(np.abs(self.field_history[-i] - self.field_history[-i-1]))
#                 recent_changes.append(change)
# 
#                 # Track distinction trend changes (mean field value is proxy for distinction)
#                 distinction_change = abs(
#                     np.mean(self.field_history[-i]) -
#                     np.mean(self.field_history[-i-1])
#                 )
#                 distinction_momentum.append(distinction_change)
# 
#             # Handle empty lists (shouldn't happen given the check above)
#             if not recent_changes:
#                 return 1.0
# 
#             # Calculate stability metrics
#             mean_change = np.mean(recent_changes)
#             distinction_trend = np.mean(distinction_momentum) if distinction_momentum else 0.0
# 
#             # Calculate stability as inverse of change magnitudes
#             stability = 1.0 / (1.0 + mean_change + 0.5 * distinction_trend)
# 
#             # Update stability history
#             self.stats['field_stability'].append(float(stability))
# 
#             return float(np.clip(stability, 0.0, 1.0))
# 
#         except Exception as e:
#             logger.error(f"Error calculating stability: {e}")
#             return 1.0  # Return maximum stability on error
# 
#     def apply_quantum_influence(self, quantum_metrics: Dict[str, float]) -> None:
#         """
#         Apply quantum influence to the field dynamics.
# 
#         Args:
#             quantum_metrics: Dictionary of quantum state metrics
#         """
#         try:
#             # Validate input
#             if not isinstance(quantum_metrics, dict):
#                 logger.warning("Invalid quantum metrics provided")
#                 return
# 
#             # Extract key metrics with defaults
#             coherence = quantum_metrics.get('phase_coherence', 0.5)
#             entropy = quantum_metrics.get('normalized_entropy', 0.5)
# 
#             # Update coherence coupling
#             self.coherence_coupling = 0.8 * self.coherence_coupling + 0.2 * coherence
# 
#             # Calculate quantum factor - higher coherence and lower entropy means stronger influence
#             quantum_factor = coherence * (1.0 - entropy)
# 
#             # Create quantum-influenced field modulation
#             # This uses a deterministic approach based on the field's existing values
#             # rather than pure randomness
#             field_indices = np.arange(len(self.field))
#             field_modulation = quantum_factor * 0.1 * (
#                 np.sin(field_indices / len(self.field) * 2 * np.pi + self.coherence_coupling) - 0.5
#             )
# 
#             # Apply modulation to field
#             self.field += field_modulation
# 
#             # Update adaptation rate based on quantum influence
#             self.adaptation_rate *= (1.0 + 0.1 * (quantum_factor - 0.5))
#             self.adaptation_rate = np.clip(self.adaptation_rate, 0.001, 0.1)
# 
#             # Track coherence history
#             self.stats['coherence_history'].append(coherence)
# 
#             # Enforce field bounds
#             self.field = np.clip(self.field, 0, 1)
# 
#             logger.debug(
#                 f"Applied quantum influence: factor={quantum_factor:.4f}, "
#                 f"coherence={coherence:.4f}, entropy={entropy:.4f}"
#             )
# 
#         except Exception as e:
#             logger.error(f"Error applying quantum influence: {e}")
# 
#     def get_field_metrics(self) -> Dict[str, float]:
#         """
#         Get comprehensive metrics about the field state.
# 
#         Returns:
#             Dictionary containing field metrics
#         """
#         try:
#             # Calculate basic metrics
#             metrics = {
#                 'mean_field': float(np.mean(self.field)),
#                 'field_variance': float(np.var(self.field)),
#                 'stability': float(self.stability_factor),
#                 'coherence_coupling': float(self.coherence_coupling),
#                 'mean_momentum': float(np.mean(np.abs(self.field_momentum))),
#                 'mean_gradient': float(np.mean(np.abs(self.field_gradient))),
#                 'adaptation_rate': float(self.adaptation_rate),
#                 'resistance_momentum': float(self.resistance_momentum),
#                 'adaptation_momentum': float(self.adaptation_momentum),
#                 'field_size': len(self.field),
#                 'field_min': float(np.min(self.field)),
#                 'field_max': float(np.max(self.field))
#             }
# 
#             # Add resistance history metrics if available
#             if self.resistance_history:
#                 metrics['mean_resistance'] = float(np.mean(self.resistance_history))
#                 metrics['resistance_std'] = float(np.std(self.resistance_history))
# 
#             # Add adaptation metrics if available
#             if self.adaptation_history:
#                 # Get most recent adaptations
#                 recent_adaptations = list(self.adaptation_history)[-100:]
#                 metrics['recent_adaptation_strength'] = float(
#                     np.mean([a['mean_update'] for a in recent_adaptations])
#                 )
#                 metrics['mean_stability'] = float(
#                     np.mean([a['stability'] for a in recent_adaptations])
#                 )
#                 metrics['adaptation_count'] = len(self.adaptation_history)
# 
#             # Calculate field entropy as an additional complexity metric
#             # Use histogram to approximate probability distribution
#             hist, _ = np.histogram(self.field, bins=20, density=True)
#             hist = hist / np.sum(hist)  # Ensure normalization
#             entropy_val = -np.sum(hist * np.log2(hist + 1e-10))
#             metrics['field_entropy'] = float(entropy_val)
# 
#             return metrics
# 
#         except Exception as e:
#             logger.error(f"Error getting field metrics: {e}")
#             return {
#                 'mean_field': 0.5,
#                 'field_variance': 0.0,
#                 'stability': 1.0,
#                 'error': str(e)
#             }
# 
#     def get_condensed_field(self, sections: int = 10) -> List[float]:
#         """
#         Get a condensed representation of the field for visualization or analysis.
# 
#         Args:
#             sections: Number of sections to divide the field into
# 
#         Returns:
#             List of mean values for each section
#         """
#         try:
#             # Calculate section size
#             section_size = max(len(self.field) // sections, 1)
# 
#             # Create condensed representation
#             condensed = []
#             for i in range(0, len(self.field), section_size):
#                 section = self.field[i:i+section_size]
#                 condensed.append(float(np.mean(section)))
# 
#             return condensed
# 
#         except Exception as e:
#             logger.error(f"Error getting condensed field: {e}")
#             return [0.5] * sections  # Return default values
# 
#     def reset(self) -> None:
#         """Reset the field to initial random state."""
#         try:
#             # Re-initialize the field
#             self.field = np.random.uniform(0, 1, size=len(self.field))
# 
#             # Reset momentum and gradient
#             self.field_momentum = np.zeros_like(self.field)
#             self.field_gradient = np.zeros_like(self.field)
# 
#             # Reset other parameters
#             self.stability_factor = 1.0
#             self.coherence_coupling = 0.0
#             self.resistance_momentum = 0.0
#             self.adaptation_momentum = 0.0
#             self.adaptation_rate = 0.01
# 
#             # Clear histories but keep stats structure
#             self.field_history.clear()
#             self.resistance_history.clear()
#             self.adaptation_history.clear()
#             self.stats['coherence_history'] = []
#             self.stats['field_stability'] = []
# 
#             logger.info("Reset ontological field to initial state")
# 
#         except Exception as e:
#             logger.error(f"Error resetting field: {e}")
# 
#

"""# 7. Transformer Modules"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile transformer_modules.py
# """
# Transformer Modules for Émile-2 Simulation
# ------------------------------------------
# Implements neural network components for processing quantum-influenced data,
# including positional encoding, attention mechanisms, and transformer architectures.
# """
# import logging
# import math
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# from collections import deque
# from typing import Optional, Tuple, Dict, Union, List, Any
# import traceback
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.transformer_modules")
# 
# # Import from other modules
# from utilities import MOMENTUM_DECAY, MINIMUM_COHERENCE_FLOOR
# from data_classes import TransformerOutput
# 
# # ---------------------------
# # Positional Encoding Modules
# # ---------------------------
# class PositionalEncoding(nn.Module):
#     """
#     Standard positional encoding with quantum-aware modulation.
# 
#     Adds positional information to input embeddings and optionally
#     modulates them based on phase and coherence.
#     """
#     def __init__(self, d_model: int, max_len: int = 1000, dropout: float = 0.1):
#         super().__init__()
#         self.d_model = d_model
#         self.dropout = nn.Dropout(p=dropout)
# 
#         # Create standard positional encoding
#         position = torch.arange(max_len).unsqueeze(1)  # [max_len, 1]
#         div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0)/d_model))
#         pe = torch.zeros(max_len, 1, d_model)
#         pe[:, 0, 0::2] = torch.sin(position*div_term)
#         pe[:, 0, 1::2] = torch.cos(position*div_term)
#         self.register_buffer('pe', pe)
# 
#         # Quantum modulation parameters
#         self.phase_modulation = nn.Parameter(torch.zeros(1, 1, d_model))
#         self.coherence_scale = nn.Parameter(torch.ones(1, 1, d_model))
# 
#         logger.debug(f"Initialized PositionalEncoding with d_model={d_model}")
# 
#     def forward(self, x: torch.Tensor, phase: Optional[torch.Tensor]=None,
#                 coherence: Optional[torch.Tensor]=None) -> torch.Tensor:
#         """
#         Apply positional encoding with optional quantum modulation.
# 
#         Args:
#             x: Input tensor [batch, seq_len, d_model]
#             phase: Optional phase tensor for quantum modulation
#             coherence: Optional coherence tensor for quantum modulation
# 
#         Returns:
#             Positionally encoded tensor with same shape as input
#         """
#         try:
#             # Validate input
#             if x.dim() != 3:
#                 raise ValueError(f"Expected 3D input (batch, seq, features), got {x.dim()}D")
# 
#             # Get sequence length
#             batch, seq_len, d_model = x.size()
# 
#             # Apply standard positional encoding
#             x = x + self.pe[:seq_len]
# 
#             # Apply quantum modulation if phase is provided
#             if phase is not None:
#                 # Process phase tensor
#                 if isinstance(phase, torch.Tensor):
#                     # Convert to appropriate dimensions if needed
#                     if phase.dim() == 1:
#                         phase = phase.unsqueeze(-1).unsqueeze(0)  # [1, batch, 1]
#                     elif phase.dim() == 2:
#                         phase = phase.unsqueeze(0)  # [1, batch, seq]
# 
#                     # Create phase modulation factor
#                     phase_factor = torch.sin(phase)
# 
#                     # Create coherence scaling if provided
#                     if coherence is None:
#                         coherence = torch.ones_like(phase) * MINIMUM_COHERENCE_FLOOR
#                     else:
#                         # Ensure coherence has same dimensions as phase
#                         if coherence.dim() != phase.dim():
#                             coherence = coherence.view_as(phase)
# 
#                     # Apply coherence scaling
#                     coherence_scaling = torch.sigmoid(self.coherence_scale) * coherence
# 
#                     # Apply quantum modulation
#                     quantum_modulation = self.phase_modulation * phase_factor
#                     x = x * (1.0 + quantum_modulation * coherence_scaling)
#                 else:
#                     logger.warning("Phase provided is not a tensor, skipping quantum modulation")
# 
#             # Apply dropout
#             x = self.dropout(x)
# 
#             return x
# 
#         except Exception as e:
#             logger.error(f"Error in PositionalEncoding forward: {e}")
#             # Return original input on error as fallback
#             return x
# 
# class EnhancedPositionalEncoding(nn.Module):
#     """
#     Enhanced positional encoding with identity embedding and stability-modulated dropout.
# 
#     Adds positional information to input embeddings, identity embeddings,
#     and applies quantum-aware modulation with stability tracking.
#     """
#     def __init__(self, d_model: int, max_len: int = 1000, dropout: float = 0.1):
#         super().__init__()
#         self.d_model = d_model
#         self.dropout = nn.Dropout(p=dropout)
# 
#         # Create standard positional encoding
#         position = torch.arange(max_len).unsqueeze(1)
#         div_term = torch.exp(torch.arange(0, d_model, 2)*(-math.log(10000.0)/d_model))
#         pe = torch.zeros(max_len, 1, d_model)
#         pe[:, 0, 0::2] = torch.sin(position*div_term)
#         pe[:, 0, 1::2] = torch.cos(position*div_term)
#         self.register_buffer('pe', pe)
# 
#         # Enhanced parameters
#         self.phase_modulation = nn.Parameter(torch.zeros(1, 1, d_model))
#         self.coherence_scale = nn.Parameter(torch.ones(1, 1, d_model))
#         self.identity_embedding = nn.Parameter(torch.zeros(1, 1, d_model))
# 
#         # Stability tracking
#         self.stability_factor = 1.0
#         self.phase_momentum = 0.0
#         self.coherence_history = deque(maxlen=100)
# 
#         logger.debug(f"Initialized EnhancedPositionalEncoding with d_model={d_model}")
# 
#     def forward(self, x: torch.Tensor, phase: Optional[torch.Tensor]=None,
#                 coherence: Optional[torch.Tensor]=None) -> torch.Tensor:
#         """
#         Apply enhanced positional encoding with stability tracking.
# 
#         Args:
#             x: Input tensor [batch, seq_len, d_model]
#             phase: Optional phase tensor for quantum modulation
#             coherence: Optional coherence tensor for quantum modulation
# 
#         Returns:
#             Enhanced encoded tensor with same shape as input
#         """
#         try:
#             # Validate input
#             if x.dim() != 3:
#                 raise ValueError(f"Expected 3D input (batch, seq, features), got {x.dim()}D")
# 
#             # Get dimensions
#             batch_size, seq_len, d_model = x.shape
# 
#             # Apply standard positional encoding
#             encoded = x + self.pe[:seq_len]
# 
#             # Apply quantum modulation if phase is provided
#             if phase is not None:
#                 # Process phase tensor
#                 if isinstance(phase, torch.Tensor):
#                     # Convert to appropriate dimensions if needed
#                     if phase.dim() == 1:
#                         phase = phase.unsqueeze(-1).unsqueeze(0)  # [1, batch, 1]
#                     elif phase.dim() == 2:
#                         phase = phase.unsqueeze(0)  # [1, batch, seq]
# 
#                     # Create phase modulation factor
#                     phase_factor = torch.sin(phase)
# 
#                     # Update phase momentum for stability tracking
#                     current_phase = phase_factor.mean().item()
#                     self.phase_momentum = MOMENTUM_DECAY * self.phase_momentum + (1 - MOMENTUM_DECAY) * current_phase
# 
#                     # Create quantum modulation with momentum influence
#                     quantum_modulation = self.phase_modulation * phase_factor * (1.0 + 0.1 * self.phase_momentum)
# 
#                     # Add coherence scaling
#                     coherence_scaling = torch.sigmoid(self.coherence_scale)
#                     if coherence is None:
#                         coherence = torch.ones_like(phase) * MINIMUM_COHERENCE_FLOOR
#                     else:
#                         # Ensure coherence has same dimensions as phase
#                         if coherence.dim() != phase.dim():
#                             coherence = coherence.view_as(phase)
# 
#                     # Apply coherence to scaling
#                     coherence_scaling = coherence_scaling * coherence
# 
#                     # Apply quantum modulation
#                     encoded = encoded * (1.0 + quantum_modulation * coherence_scaling)
# 
#                     # Update stability factor
#                     self.stability_factor = 0.95*self.stability_factor + 0.05*(1.0 - abs(self.phase_momentum))
#                 else:
#                     logger.warning("Phase provided is not a tensor, skipping quantum modulation")
# 
#             # Apply identity embedding (skip connection)
#             identity_factor = torch.sigmoid(self.identity_embedding)
#             encoded = encoded + identity_factor * x
# 
#             # Apply stability-modulated dropout
#             effective_dropout = self.dropout.p * (2.0 - self.stability_factor)
#             effective_dropout = max(0.0, min(0.5, effective_dropout))  # Clamp to reasonable range
#             encoded = F.dropout(encoded, p=effective_dropout, training=self.training)
# 
#             return encoded
# 
#         except Exception as e:
#             logger.error(f"Error in EnhancedPositionalEncoding forward: {e}")
#             # Return original input on error as fallback
#             return x
# 
#     def get_stability_metrics(self) -> Dict[str, float]:
#         """Get stability metrics for monitoring."""
#         return {
#             'stability_factor': float(self.stability_factor),
#             'phase_momentum': float(self.phase_momentum),
#             'effective_dropout': float(self.dropout.p * (2.0 - self.stability_factor))
#         }
# 
# # ---------------------------
# # Enhanced Multi-Head Attention Module
# # ---------------------------
# class EnhancedMultiheadAttention(nn.Module):
#     """
#     Multi-head attention with quantum-aware modulation and stability adjustments.
# 
#     Implements attention mechanism with added quantum influence, shape handling,
#     and stability tracking.
#     """
#     def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):
#         super().__init__()
#         assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
# 
#         self.d_model = d_model
#         self.num_heads = num_heads
#         self.head_dim = d_model // num_heads
# 
#         # Projection layers
#         self.q_proj = nn.Linear(d_model, d_model)
#         self.k_proj = nn.Linear(d_model, d_model)
#         self.v_proj = nn.Linear(d_model, d_model)
#         self.o_proj = nn.Linear(d_model, d_model)
# 
#         # Quantum modulation parameters
#         self.phase_coupling = nn.Parameter(torch.zeros(1, num_heads, 1, 1))
#         self.coherence_scale = nn.Parameter(torch.ones(1, num_heads, 1, 1))
#         self.attention_scale = nn.Parameter(torch.ones(1))
#         self.stability_factor = nn.Parameter(torch.ones(1))
# 
#         # Dimension adaptation for handling different input sizes
#         self.dim_adapt_in = nn.Linear(d_model, d_model)
#         self.dim_adapt_out = nn.Linear(d_model, d_model)
# 
#         # Dropout and scaling
#         self.dropout = nn.Dropout(dropout)
#         self.scale = self.head_dim ** -0.5
# 
#         # Attention pattern tracking
#         self.attention_patterns = deque(maxlen=100)
#         self.stability_threshold = 0.1
#         self.dimension_mismatch_counter = 0
#         self.max_dimension_retries = 3
# 
#         logger.debug(f"Initialized EnhancedMultiheadAttention with d_model={d_model}, num_heads={num_heads}")
# 
#     def _validate_input_shapes(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
#         """
#         Validate and reshape input tensors to correct dimensions.
# 
#         Args:
#             q, k, v: Input tensors for query, key, value
# 
#         Returns:
#             Reshaped tensors with correct dimensions
#         """
#         try:
#             # Define shape fixing function
#             def fix_shape(x: torch.Tensor) -> torch.Tensor:
#                 # Handle extra dimensions by squeezing
#                 while x.dim() > 3:
#                     x = x.squeeze(1)
# 
#                 # Handle fewer dimensions by unsqueezing
#                 if x.dim() == 1:
#                     x = x.unsqueeze(0).unsqueeze(0)
#                 elif x.dim() == 2:
#                     x = x.unsqueeze(1)
# 
#                 return x
# 
#             # Fix shapes for all inputs
#             q = fix_shape(q)
#             k = fix_shape(k)
#             v = fix_shape(v)
# 
#             # Handle dimension mismatches with dimension adaptation layers
#             if q.size(-1) != self.d_model:
#                 logger.debug(f"Adapting query from {q.size(-1)} to {self.d_model} dimensions")
#                 q = self.dim_adapt_in(q)
# 
#             if k.size(-1) != self.d_model:
#                 logger.debug(f"Adapting key from {k.size(-1)} to {self.d_model} dimensions")
#                 k = self.dim_adapt_in(k)
# 
#             if v.size(-1) != self.d_model:
#                 logger.debug(f"Adapting value from {v.size(-1)} to {self.d_model} dimensions")
#                 v = self.dim_adapt_in(v)
# 
#             return q, k, v
# 
#         except Exception as e:
#             logger.error(f"Error in shape validation: {e}")
#             # Return original inputs on error
#             return q, k, v
# 
#     def _shape(self, x: torch.Tensor) -> torch.Tensor:
#         """
#         Reshape tensor for multi-head attention.
# 
#         Args:
#             x: Input tensor [batch, seq_len, d_model]
# 
#         Returns:
#             Reshaped tensor [batch, num_heads, seq_len, head_dim]
#         """
#         try:
#             # Extract dimensions
#             batch_size = x.size(0)
#             seq_len = x.size(1)
# 
#             # Adapt dimensions if needed
#             if x.size(-1) != self.d_model:
#                 x = self.dim_adapt_in(x)
# 
#             # Reshape: [batch, seq, d_model] -> [batch, seq, num_heads, head_dim]
#             x = x.view(batch_size, seq_len, self.num_heads, self.head_dim)
# 
#             # Transpose: [batch, seq, num_heads, head_dim] -> [batch, num_heads, seq, head_dim]
#             x = x.transpose(1, 2)
# 
#             return x
# 
#         except Exception as e:
#             logger.error(f"Error in tensor reshaping: {e}")
#             # Return safe default on error
#             batch_size = x.size(0) if x.dim() > 0 else 1
#             return torch.zeros((batch_size, self.num_heads, 1, self.head_dim), device=x.device)
# 
#     def _track_attention_pattern(self, attn_weights: torch.Tensor) -> None:
#         """
#         Track attention patterns for stability monitoring.
# 
#         Args:
#             attn_weights: Attention weight tensor
#         """
#         try:
#             with torch.no_grad():
#                 # Extract mean attention pattern across batch
#                 pattern = attn_weights.mean(dim=0).detach().cpu()
# 
#                 # Store in history
#                 self.attention_patterns.append(pattern)
# 
#                 # Check for pattern stability if we have enough history
#                 if len(self.attention_patterns) > 1:
#                     current = self.attention_patterns[-1]
#                     previous = self.attention_patterns[-2]
# 
#                     # Calculate pattern difference
#                     pattern_diff = torch.norm(current - previous)
# 
#                     # Update stability factor based on pattern difference
#                     if pattern_diff > self.stability_threshold:
#                         # Decrease stability on high differences
#                         self.stability_factor.data *= 0.95
#                     else:
#                         # Increase stability on low differences (with max cap)
#                         self.stability_factor.data = torch.min(
#                             self.stability_factor.data * 1.05,
#                             torch.tensor(1.0, device=self.stability_factor.device)
#                         )
# 
#         except Exception as e:
#             logger.error(f"Error tracking attention pattern: {e}")
# 
#     def forward(self, q: torch.Tensor, k: torch.Tensor, v: torch.Tensor,
#            phase: Optional[torch.Tensor]=None) -> Tuple[torch.Tensor, torch.Tensor]:
#         """
#         Forward pass for enhanced multi-head attention.
# 
#         Args:
#             q, k, v: Query, key, value tensors
#             phase: Optional phase tensor for quantum modulation
# 
#         Returns:
#             Tuple of (output tensor, attention weights)
#         """
#         try:
#             # Validate and fix input shapes
#             q, k, v = self._validate_input_shapes(q, k, v)
# 
#             # Get dimensions
#             batch_size = q.size(0)
#             seq_len = q.size(1)
# 
#             # Apply projection layers first (before reshaping)
#             q = self.q_proj(q)  # [batch, seq, d_model]
#             k = self.k_proj(k)  # [batch, seq, d_model]
#             v = self.v_proj(v)  # [batch, seq, d_model]
# 
#             # Reshape for multi-head attention
#             q = self._shape(q)  # [batch, num_heads, seq, head_dim]
#             k = self._shape(k)  # [batch, num_heads, seq, head_dim]
#             v = self._shape(v)  # [batch, num_heads, seq, head_dim]
# 
#             # Calculate attention scores: [batch, num_heads, seq, seq]
#             scores = torch.matmul(q, k.transpose(-2, -1)) * self.scale * self.attention_scale
# 
#             # Apply phase modulation if provided
#             if phase is not None:
#                 # Ensure phase is a tensor on the correct device
#                 if isinstance(phase, torch.Tensor):
#                     phase = phase.to(q.device)
# 
#                     # Reshape phase to match attention dimensions
#                     if phase.dim() == 1:
#                         phase = phase.unsqueeze(1).unsqueeze(1).unsqueeze(1)  # [batch, 1, 1, 1]
# 
#                     # Create phase factor
#                     phase_factor = torch.sin(phase)
# 
#                     # Apply phase coupling to attention scores
#                     scores = scores * (1.0 + self.phase_coupling * phase_factor)
#                 else:
#                     logger.warning("Phase is not a tensor, skipping modulation")
# 
#             # Apply softmax and dropout
#             attn_weights = F.softmax(scores, dim=-1)
#             attn_weights = self.dropout(attn_weights)
# 
#             # Track attention patterns for stability monitoring
#             self._track_attention_pattern(attn_weights)
# 
#             # Apply attention weights to values
#             output = torch.matmul(attn_weights, v)  # [batch, num_heads, seq, head_dim]
# 
#             # Reshape output: [batch, num_heads, seq, head_dim] -> [batch, seq, d_model]
#             output = output.transpose(1, 2).contiguous()
#             output = output.view(batch_size, seq_len, self.d_model)
# 
#             # Apply output projection with stability factor
#             output = self.o_proj(output) * self.stability_factor
# 
#             return output, attn_weights
# 
#         except Exception as e:
#             logger.error(f"Error in EnhancedMultiheadAttention forward: {e}")
# 
#             # Create safe default outputs on error
#             device = q.device if hasattr(q, 'device') else torch.device('cpu')
#             batch_size = q.size(0) if hasattr(q, 'size') else 1
#             seq_len = q.size(1) if q.dim() > 1 else 1
# 
#             default_output = torch.zeros((batch_size, seq_len, self.d_model), device=device)
#             default_weights = torch.zeros((batch_size, self.num_heads, seq_len, seq_len), device=device)
# 
#             return default_output, default_weights
# 
#     def get_stability_metrics(self) -> Dict[str, float]:
#         """Get stability metrics for monitoring."""
#         return {
#             'stability_factor': float(self.stability_factor.item()),
#             'attention_scale': float(self.attention_scale.item()),
#             'dimension_mismatches': self.dimension_mismatch_counter
#         }
# 
# # ---------------------------
# # Four-Dimensional Transformer Adapter
# # ---------------------------
# class FourDimTransformerAdapter(nn.Module):
#     """
#     Adapter for handling 4D inputs in a transformer.
# 
#     If the input has shape [batch, extra, seq, embed], this adapter supports two strategies:
#     - "merge": Merge the extra dimension with the sequence dimension.
#     - "separate": Process the extra dimension via a convolutional branch and then fuse.
#     """
#     def __init__(self, base_transformer: nn.Module, merge_strategy: str = "merge"):
#         super().__init__()
#         self.base_transformer = base_transformer
#         self.merge_strategy = merge_strategy
# 
#         # Validate strategy
#         if self.merge_strategy not in ["merge", "separate"]:
#             raise ValueError("merge_strategy must be either 'merge' or 'separate'")
# 
#         # Set up separate processing branch if needed
#         if self.merge_strategy == "separate":
#             # Find input dimension from base transformer
#             embed_dim = self._get_embed_dim_from_transformer(base_transformer)
# 
#             # Convolutional branch to process the extra dimension
#             self.extra_conv = nn.Conv2d(
#                 in_channels=1, out_channels=1, kernel_size=(3,1), padding=(1,0)
#             )
# 
#             # Fusion layer to combine processed features
#             self.fusion_layer = nn.Linear(embed_dim * 2, embed_dim)
# 
#         logger.info(f"Initialized FourDimTransformerAdapter with {merge_strategy} strategy")
# 
#     def _get_embed_dim_from_transformer(self, transformer: nn.Module) -> int:
#         """
#         Extract the embedding dimension from the base transformer.
#         """
#         embed_dim = None
# 
#         # Try various attribute paths that might contain the dimension
#         if hasattr(transformer, "input_layer") and hasattr(transformer.input_layer, "in_features"):
#             embed_dim = transformer.input_layer.in_features
#         elif hasattr(transformer, "d_model"):
#             embed_dim = transformer.d_model
#         elif hasattr(transformer, "embedding_dim"):
#             embed_dim = transformer.embedding_dim
#         elif hasattr(transformer, "emb_dim"):
#             embed_dim = transformer.emb_dim
# 
#         # If still not found, check first layer for Linear or Embedding
#         if embed_dim is None:
#             for module in transformer.modules():
#                 if isinstance(module, (nn.Linear, nn.Embedding)):
#                     if hasattr(module, "in_features"):
#                         embed_dim = module.in_features
#                         break
#                     elif hasattr(module, "embedding_dim"):
#                         embed_dim = module.embedding_dim
#                         break
# 
#         # If still not found, raise error
#         if embed_dim is None:
#             raise ValueError(
#                 "Could not determine embedding dimension from base transformer. "
#                 "Please specify embed_dim manually or use a transformer with "
#                 "detectable embedding dimensions."
#             )
# 
#         return embed_dim
# 
#     def forward(self, x: torch.Tensor, phase: Optional[torch.Tensor] = None) -> TransformerOutput:
#         """
#         Forward pass of the adapter.
# 
#         Handles emerging 4D inputs and adapts them according to the strategy.
# 
#         Args:
#             x: Input tensor, potentially 4D [batch, extra, seq, embed]
#             phase: Optional phase tensor for quantum modulation
# 
#         Returns:
#             TransformerOutput from base transformer
#         """
#         try:
#             # Ensure x is a tensor
#             if not isinstance(x, torch.Tensor):
#                 raise TypeError(
#                     f"Expected input to be a torch.Tensor, but got {type(x)} instead."
#                 )
# 
#             # Log input shape for debugging
#             original_shape = x.shape
#             dim_message = f"Input shape: {original_shape}"
# 
#             # Handle 4D input: [batch, extra, seq, embed]
#             if x.dim() == 4:
#                 batch, extra, seq, embed = x.size()
#                 dim_message += f" → 4D input detected"
# 
#                 if self.merge_strategy == "merge":
#                     # Merge extra with sequence dimension: [batch, extra*seq, embed]
#                     x = x.reshape(batch, extra * seq, embed)
#                     dim_message += f" → Merged to: {x.shape}"
# 
#                 elif self.merge_strategy == "separate":
#                     # Process extra dimension separately with convolutional branch
#                     # First reshape: [batch*extra, 1, seq, embed]
#                     x_reshaped = x.reshape(batch * extra, 1, seq, embed)
# 
#                     # Apply convolution
#                     x_conv = self.extra_conv(x_reshaped)  # [batch*extra, 1, seq, embed]
#                     x_conv = F.relu(x_conv)
# 
#                     # Restore original dimensions: [batch, extra, seq, embed]
#                     x_conv = x_conv.reshape(batch, extra, seq, embed)
# 
#                     # Extract primary slice: [batch, seq, embed]
#                     x_primary = x[:, 0, :, :]
# 
#                     # Average over extra dimension: [batch, seq, embed]
#                     x_extra = x_conv.mean(dim=1)
# 
#                     # Concatenate along embedding: [batch, seq, 2*embed]
#                     x_cat = torch.cat([x_primary, x_extra], dim=-1)
# 
#                     # Fuse with linear layer: [batch, seq, embed]
#                     x = self.fusion_layer(x_cat)
#                     dim_message += f" → Processed separately to: {x.shape}"
# 
#             # Log dimension info
#             logger.debug(dim_message)
# 
#             # Pass to base transformer
#             return self.base_transformer(x, phase)
# 
#         except Exception as e:
#             logger.error(f"Error in FourDimTransformerAdapter forward: {e}")
#             traceback.print_exc()
#             # Create an emergency default output
#             batch_size = x.size(0) if hasattr(x, 'size') else 1
#             device = x.device if hasattr(x, 'device') else torch.device('cpu')
# 
#             # Return minimal valid TransformerOutput
#             return TransformerOutput(
#                 prediction=torch.zeros((batch_size, 1), device=device),
#                 phase_prediction=torch.zeros((batch_size, 1), device=device),
#                 value_estimate=torch.zeros((batch_size, 1), device=device),
#                 attention_weights={},
#                 entropy=torch.tensor(0.0, device=device),
#                 coherence_estimate=torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
#             )
# 
# # ---------------------------
# # Transformer Layer and Recursive Transformer
# # ---------------------------
# class TransformerLayer(nn.Module):
#     """
#     A single transformer layer with multi-head attention and feed-forward network.
# 
#     Implements a standard transformer layer with residual connections,
#     layer normalization, and optional quantum modulation.
#     """
#     def __init__(self, d_model: int, nhead: int, dropout: float = 0.1):
#         super().__init__()
#         self.attention = EnhancedMultiheadAttention(d_model, nhead, dropout)
#         self.norm1 = nn.LayerNorm(d_model)
#         self.ff = nn.Sequential(
#             nn.Linear(d_model, d_model * 4),
#             nn.GELU(),
#             nn.Dropout(dropout),
#             nn.Linear(d_model * 4, d_model)
#         )
#         self.norm2 = nn.LayerNorm(d_model)
#         self.dropout = nn.Dropout(dropout)
# 
#         logger.debug(f"Initialized TransformerLayer with d_model={d_model}, nhead={nhead}")
# 
#     def forward(self, x: torch.Tensor, phase: Optional[torch.Tensor]=None) -> Tuple[torch.Tensor, torch.Tensor]:
#         """
#         Forward pass for transformer layer.
# 
#         Args:
#             x: Input tensor [batch, seq_len, d_model]
#             phase: Optional phase tensor for quantum modulation
# 
#         Returns:
#             Tuple of (output tensor, attention weights)
#         """
#         try:
#             # Apply attention with residual connection and normalization
#             attn_out, attn_weights = self.attention(x, x, x, phase)
#             x = self.norm1(x + self.dropout(attn_out))
# 
#             # Apply feed-forward with residual connection and normalization
#             ff_out = self.ff(x)
#             x = self.norm2(x + self.dropout(ff_out))
# 
#             return x, attn_weights
# 
#         except Exception as e:
#             logger.error(f"Error in TransformerLayer forward: {e}")
# 
#             # Create safe default outputs
#             batch_size, seq_len, d_model = x.size()
#             device = x.device
# 
#             default_output = torch.zeros_like(x)
#             default_weights = torch.zeros(
#                 batch_size, self.attention.num_heads, seq_len, seq_len, device=device
#             )
# 
#             return default_output, default_weights
# 
# class RecursiveDistinctionTransformer(nn.Module):
#     """
#     A transformer for processing and predicting distinction levels.
# 
#     Implements a transformer architecture with quantum awareness,
#     accepting 3D inputs and adapting to emergent dimensions.
#     """
#     def __init__(self,
#                  input_size: int = 20,
#                  d_model: int = 20,
#                  nhead: int = 4,
#                  num_layers: int = 2,
#                  output_size: int = 1,
#                  dropout: float = 0.1):
#         super().__init__()
# 
#         # Validate dimensions and parameters
#         self.input_size = input_size
#         self.d_model = d_model
# 
#         # Input projection layer
#         self.input_layer = nn.Linear(input_size, d_model)
# 
#         # Positional encoding - use enhanced version
#         self.positional_encoding = EnhancedPositionalEncoding(d_model, dropout=dropout)
# 
#         # Create transformer layers
#         self.layers = nn.ModuleList([
#             TransformerLayer(d_model, nhead, dropout) for _ in range(num_layers)
#         ])
# 
#         # Output projection layers
#         self.output_layer = nn.Linear(d_model, output_size)
#         self.phase_output = nn.Linear(d_model, 1)
#         self.value_output = nn.Linear(d_model, 1)
#         self.coherence_output = nn.Linear(d_model, 1)
# 
#         # Initialize weights
#         self._init_weights()
# 
#         logger.info(
#             f"Initialized RecursiveDistinctionTransformer with input_size={input_size}, "
#             f"d_model={d_model}, nhead={nhead}, num_layers={num_layers}"
#         )
# 
#     def _init_weights(self):
#         """Initialize transformer weights for stable training."""
#         # Xavier initialization for linear layers
#         for module in self.modules():
#             if isinstance(module, nn.Linear):
#                 nn.init.xavier_uniform_(module.weight, gain=0.01)
#                 if module.bias is not None:
#                     nn.init.zeros_(module.bias)
# 
#     def _validate_input(self, x: torch.Tensor) -> torch.Tensor:
#         """
#         Validate and adapt input tensor to correct shape.
# 
#         Args:
#             x: Input tensor of arbitrary shape
# 
#         Returns:
#             Properly shaped tensor [batch, seq, input_size]
#         """
#         device = x.device if hasattr(x, 'device') else torch.device('cpu')
# 
#         # Handle emergent 4D+ inputs
#         if x.dim() > 3:
#             batch_size = x.size(0)
#             embed_dim = x.size(-1)
#             new_seq_len = int(np.prod(x.shape[1:-1]))
# 
#             logger.debug(
#                 f"Emergent shape detected: {x.shape}. "
#                 f"Reshaping to [{batch_size}, {new_seq_len}, {embed_dim}]"
#             )
# 
#             # Merge all dimensions between batch and embedding
#             x = x.view(batch_size, new_seq_len, embed_dim)
# 
#         # Handle 1D tensor (single vector)
#         elif x.dim() == 1:
#             x = x.unsqueeze(0).unsqueeze(0)
# 
#         # Handle 2D tensor (batch of vectors)
#         elif x.dim() == 2:
#             x = x.unsqueeze(1)
# 
#         # Check final shape
#         batch_size, seq_len, input_size = x.size()
# 
#         # Handle input size mismatch
#         if input_size != self.input_size:
#             logger.warning(
#                 f"Input size {input_size} does not match expected {self.input_size}. "
#                 f"Reshaping tensor."
#             )
# 
#             if input_size > self.input_size:
#                 # Trim if larger
#                 x = x[:, :, :self.input_size]
#             else:
#                 # Pad if smaller
#                 padding = torch.zeros(
#                     batch_size, seq_len, self.input_size - input_size,
#                     device=device
#                 )
#                 x = torch.cat([x, padding], dim=2)
# 
#         return x
# 
#     def forward(self, x: torch.Tensor, phase: Optional[torch.Tensor]=None) -> TransformerOutput:
#         """
#         Forward pass of the recursive distinction transformer.
# 
#         Args:
#             x: Input tensor, may be variably shaped
#             phase: Optional phase tensor for quantum modulation
# 
#         Returns:
#             TransformerOutput containing predictions and attention
#         """
#         try:
#             # Set device
#             device = x.device if hasattr(x, 'device') else torch.device('cpu')
# 
#             # Validate and reshape input
#             x = self._validate_input(x)
#             batch_size, seq_len, input_size = x.size()
# 
#             # Apply input projection
#             x = self.input_layer(x)
# 
#             # Apply positional encoding with phase modulation
#             x = self.positional_encoding(x, phase)
# 
#             # Track attention weights and layer outputs
#             attention_weights = {}
#             layer_outputs = []
# 
#             # Process through transformer layers
#             for i, layer in enumerate(self.layers):
#                 try:
#                     # Apply transformer layer
#                     layer_output, layer_attn = layer(x, phase)
# 
#                     # Store results
#                     x = layer_output
#                     attention_weights[f'layer_{i}'] = layer_attn
#                     layer_outputs.append(layer_output)
# 
#                 except Exception as layer_error:
#                     logger.error(f"Error in layer {i}: {layer_error}")
#                     # Continue with current x if a layer fails
#                     attention_weights[f'layer_{i}'] = torch.zeros(
#                         batch_size, self.layers[0].attention.num_heads,
#                         seq_len, seq_len, device=device
#                     )
# 
#             try:
#                 # Generate outputs
#                 # Main prediction - applies sigmoid for 0-1 range
#                 prediction = torch.sigmoid(self.output_layer(x))
# 
#                 # Phase prediction
#                 phase_pred = self.phase_output(x)
# 
#                 # Value estimate
#                 value_est = self.value_output(x)
# 
#                 # Calculate coherence estimates from each layer
#                 coherence_estimates = []
#                 for output in layer_outputs:
#                     coherence_est = torch.sigmoid(self.coherence_output(output))
#                     coherence_estimates.append(coherence_est)
# 
#                 # Combine coherence estimates with weighted average
#                 if coherence_estimates:
#                     # Calculate softmax weights based on mean coherence values
#                     coherence_weights = torch.softmax(
#                         torch.stack([est.mean() for est in coherence_estimates]),
#                         dim=0
#                     )
# 
#                     # Apply weights to coherence estimates
#                     coherence_est = (
#                         torch.stack(coherence_estimates) * coherence_weights.view(-1, 1, 1)
#                     ).sum(0)
#                 else:
#                     # Default coherence if no estimates available
#                     coherence_est = torch.tensor(
#                         MINIMUM_COHERENCE_FLOOR,
#                         device=device
#                     ).expand_as(prediction)
# 
#                 # Calculate attention entropy
#                 entropies = []
#                 for weights in attention_weights.values():
#                     # Add small epsilon to prevent log(0)
#                     entropy = -(weights * torch.log(weights + 1e-10)).sum(dim=-1).mean()
#                     entropies.append(entropy)
# 
#                 # Average entropy across layers
#                 attn_entropy = torch.stack(entropies).mean() if entropies else torch.tensor(0.0, device=device)
# 
#                 # Construct the final output
#                 return TransformerOutput(
#                     prediction=prediction.squeeze(-1),
#                     phase_prediction=phase_pred.squeeze(-1),
#                     value_estimate=value_est.squeeze(-1),
#                     attention_weights=attention_weights,
#                     entropy=attn_entropy,
#                     coherence_estimate=coherence_est
#                 )
# 
#             except Exception as output_error:
#                 logger.error(f"Error generating transformer outputs: {output_error}")
#                 # Return default output on error
#                 return TransformerOutput(
#                     prediction=torch.zeros((batch_size, seq_len), device=device),
#                     phase_prediction=torch.zeros((batch_size, seq_len), device=device),
#                     value_estimate=torch.zeros((batch_size, seq_len), device=device),
#                     attention_weights={},
#                     entropy=torch.tensor(0.0, device=device),
#                     coherence_estimate=torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
#                 )
# 
#         except Exception as e:
#             logger.error(f"Error in RecursiveDistinctionTransformer forward: {e}")
# 
#             # Determine batch size for default output
#             batch_size = x.size(0) if hasattr(x, 'size') else 1
# 
#             # Return default output with proper device
#             device = x.device if hasattr(x, 'device') else torch.device('cpu')
# 
#             return TransformerOutput(
#                 prediction=torch.zeros((batch_size, 1), device=device),
#                 phase_prediction=torch.zeros((batch_size, 1), device=device),
#                 value_estimate=torch.zeros((batch_size, 1), device=device),
#                 attention_weights={},
#                 entropy=torch.tensor(0.0, device=device),
#                 coherence_estimate=torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
#             )
# 
#     def get_attention_patterns(self) -> Dict[str, torch.Tensor]:
#         """Get attention patterns from each layer for visualization."""
#         patterns = {}
# 
#         for i, layer in enumerate(self.layers):
#             if hasattr(layer.attention, 'attention_patterns') and layer.attention.attention_patterns:
#                 # Get most recent pattern
#                 pattern = layer.attention.attention_patterns[-1]
#                 patterns[f'layer_{i}'] = pattern
# 
#         return patterns
# 
#

"""# 8. Surplus Dynamics"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile surplusdynamics.py
# """Surplus and Distinction Dynamics"""
# """
# Manages surplus stabilization, adaptation, and recursive feedback in the quantum system.
# Regulates cognitive and ontological surplus levels based on phase coherence and entropy.
# """
# import numpy as np
# import time
# import traceback
# from collections import deque, defaultdict
# from typing import Dict, Tuple, Any, Optional, Union, List
# import logging_setup
# from logging_setup import setup_logging
# from data_classes import SurplusState
# 
# # Import only necessary constants and functions, not the ones we're making dynamic
# from utilities import (
#     MAX_SURPLUS,
#     MOMENTUM_DECAY,
#     ensure_real,
#     MINIMUM_COHERENCE_FLOOR,
#     HIDDEN_DIM,
#     update_momentum,
#     compute_phase_coherence,
# )
# from analysis import CausalityAnalysis
# 
# # ---------------------------
# # EnhancedSurplusDynamics
# # ---------------------------
# class EnhancedSurplusDynamics:
#     """
#     Enhanced surplus dynamics with improved quantum coupling and state management.
#     Implements advanced surplus accumulation, adaptation, and expulsion mechanisms.
#     """
#     def __init__(self):
#         # Initialize dynamic constants - moved from utilities.py to instance variables
#         # These constants can now be adjusted dynamically during runtime
#         self.surplus_threshold = 1.5  # Default from SURPLUS_THRESHOLD
#         self.expulsion_recovery_rate = 0.02  # Default from EXPULSION_RECOVERY_RATE
#         self.surplus_adjustment_rate = 0.05  # Default from SURPLUS_ADJUSTMENT_RATE
#         self.phase_scaling_factor = 0.3  # Default from PHASE_SCALING_FACTOR
#         self.surplus_recycle_fraction = 0.7  # Default from SURPLUS_RECYCLE_FRACTION
#         self.core_distinction_update_rate = 0.01  # Default from CORE_DISTINCTION_UPDATE_RATE
#         self.distinction_anchor_weight = 0.2  # Default from DISTINCTION_ANCHOR_WEIGHT
#         self.target_distinction = 0.7  # Default from TARGET_DISTINCTION
#         self.collapse_dissipation_threshold = 0.35  # Default from COLLAPSE_DISSIPATION_THRESHOLD
#         self.collapse_dissipation_rate = 0.02  # Default from COLLAPSE_DISSIPATION_RATE
#         self.instability_grace_period = 3  # Default from INSTABILITY_GRACE_PERIOD
# 
#         # Learning rates for dynamic constant adaptation
#         self.constant_adaptation_rate = 0.01  # Base learning rate for updating constants
#         self.threshold_momentum = 0.0  # Momentum for surplus_threshold updates
#         self.distinction_momentum = 0.0  # Momentum for target_distinction updates
#         self.recovery_momentum = 0.0  # Momentum for expulsion_recovery_rate updates
# 
#         # History tracking for dynamic constant adaptation
#         self.constants_history = deque(maxlen=1000)  # Track changes to constants
#         self.adaptation_feedback = deque(maxlen=100)  # Track feedback from adaptation
# 
#         # Initialize counters and tracking first
#         self.stability_threshold = 0.3
#         self.initialization_attempts = 0
#         self.max_initialization_attempts = 3
#         self.steps_since_expulsion = 0
#         self.expulsion_cooldown = 50
#         self.stability_decay = 0.99
#         self.coupling_momentum = 0.0
#         self.expulsion_threshold = 0.95  # Adjust as needed
#         self.instability_counter = 0  # Track system instability
#         self.min_steps_before_recovery = 10  # Minimum steps before resetting
#         self.steps_taken = 0  # Track steps taken
#         self.recovery_state = None
#         self.minimum_stability = 0.1
# 
#         # Initialize surplus state first! (BEFORE calling any methods using it)
#         self.surplus_state = self._initialize_surplus_state()
# 
#         # Ensure surplus_state is valid before proceeding
#         if self.surplus_state is None or not isinstance(self.surplus_state, SurplusState):
#             raise ValueError("❌ Critical Error: SurplusState initialization failed completely.")
# 
#         # Initialize excess stability tracking
#         self.excess_stability_potential = 0.0
# 
#         # Initialize history tracking
#         self.expulsion_history = deque(maxlen=100)
#         self.accumulation_history = deque(maxlen=1000)
#         self.stability_history = deque(maxlen=1000)
#         self.initialization_history = deque(maxlen=100)
# 
#         # Initialize surplus tracking attributes
#         self.basal_surplus = self.surplus_state.values['basal']
#         self.cognitive_surplus = self.surplus_state.values['cognitive']
#         self.predictive_surplus = self.surplus_state.values['predictive']
#         self.ontological_surplus = self.surplus_state.values['ontological']
#         self.stability = self.surplus_state.stability
#         self.quantum_coupling = self.surplus_state.quantum_coupling
#         self.accumulation_momentum = {k: 0.0 for k in self.surplus_state.values.keys()}
#         self.stability_momentum = 0.0
# 
#         # Prevent instant expulsion; only trigger if surplus exceeds a defined threshold
#         if self.surplus_state.total_surplus() > self.surplus_threshold:
#             self.gradual_surplus_expulsion()
# 
#         # Emergence tracking
#         self.emergence_patterns = deque(maxlen=1000)
#         self.pattern_momentum = 0.0
#         self.emergence_threshold = 0.3
#         self.emergence_counter = 0
#         self.emergence_history = []
#         self.novelty_score = 0.0
#         self.complexity_score = 0.0
#         self.emergence_sensitivity = 0.5
# 
#         # Record initial constants
#         self._record_constants_state("initialization")
# 
#     def _record_constants_state(self, event_type: str) -> None:
#         """Record the current state of all dynamic constants for tracking changes over time."""
#         try:
#             constants_state = {
#                 'surplus_threshold': self.surplus_threshold,
#                 'expulsion_recovery_rate': self.expulsion_recovery_rate,
#                 'surplus_adjustment_rate': self.surplus_adjustment_rate,
#                 'phase_scaling_factor': self.phase_scaling_factor,
#                 'surplus_recycle_fraction': self.surplus_recycle_fraction,
#                 'core_distinction_update_rate': self.core_distinction_update_rate,
#                 'distinction_anchor_weight': self.distinction_anchor_weight,
#                 'target_distinction': self.target_distinction,
#                 'collapse_dissipation_threshold': self.collapse_dissipation_threshold,
#                 'collapse_dissipation_rate': self.collapse_dissipation_rate,
#                 'instability_grace_period': self.instability_grace_period,
#                 'event_type': event_type,
#                 'timestamp': time.time()
#             }
#             self.constants_history.append(constants_state)
#         except Exception as e:
#             print(f"Error recording constants state: {e}")
# 
#     def update_dynamic_constants(self,
#                                phase_coherence: float,
#                                distinction_level: float,
#                                stability: float,
#                                emergence_detected: bool = False) -> None:
#         """
#         Update dynamic constants based on system state and performance feedback.
#         This implements a rule-based and momentum-based approach to constant adaptation.
# 
#         Args:
#             phase_coherence: Current phase coherence
#             distinction_level: Current distinction level
#             stability: Current system stability
#             emergence_detected: Whether emergence was detected in the current step
#         """
#         try:
#             # Ensure inputs are valid floats
#             phase_coherence = ensure_real(phase_coherence, 0.5)
#             distinction_level = ensure_real(distinction_level, 0.5)
#             stability = ensure_real(stability, 0.5)
# 
#             # Store previous values to track changes
#             prev_threshold = self.surplus_threshold
#             prev_distinction = self.target_distinction
# 
#             # 1. Update surplus_threshold based on average surplus levels
#             avg_surplus = self.surplus_state.total_surplus() / len(self.surplus_state.values)
# 
#             # If average surplus is consistently high, increase the threshold
#             if avg_surplus > self.surplus_threshold * 1.2 and stability > 0.6:
#                 # Calculate threshold adjustment
#                 threshold_adjustment = 0.05 * self.constant_adaptation_rate * (avg_surplus / self.surplus_threshold - 1.0)
# 
#                 # Update momentum
#                 self.threshold_momentum = update_momentum(
#                     self.threshold_momentum,
#                     threshold_adjustment,
#                     decay=MOMENTUM_DECAY
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.surplus_threshold += threshold_adjustment + 0.1 * self.threshold_momentum
#                 self.surplus_threshold = min(self.surplus_threshold, MAX_SURPLUS * 0.5)  # Cap at 50% of MAX_SURPLUS
# 
#                 print(f"Increased surplus_threshold to {self.surplus_threshold:.4f} based on high surplus")
# 
#             # If system is unstable and surplus is low, decrease threshold
#             elif self.instability_counter > 1 and avg_surplus < self.surplus_threshold * 0.8:
#                 # Calculate threshold adjustment
#                 threshold_adjustment = -0.05 * self.constant_adaptation_rate * (1.0 - avg_surplus / self.surplus_threshold)
# 
#                 # Update momentum
#                 self.threshold_momentum = update_momentum(
#                     self.threshold_momentum,
#                     threshold_adjustment,
#                     decay=MOMENTUM_DECAY
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.surplus_threshold += threshold_adjustment + 0.1 * self.threshold_momentum
#                 self.surplus_threshold = max(self.surplus_threshold, 0.5)  # Ensure threshold doesn't go too low
# 
#                 print(f"Decreased surplus_threshold to {self.surplus_threshold:.4f} based on instability")
# 
#             # 2. Update target_distinction based on current distinction and stability
#             # If the system is stable, move target distinction toward current distinction
#             distinction_delta = distinction_level - self.target_distinction
# 
#             # Only update if there's a significant difference and system is stable
#             if abs(distinction_delta) > 0.1 and stability > 0.7:
#                 # Calculate distinction adjustment
#                 distinction_adjustment = 0.02 * self.constant_adaptation_rate * distinction_delta
# 
#                 # Update momentum
#                 self.distinction_momentum = update_momentum(
#                     self.distinction_momentum,
#                     distinction_adjustment,
#                     decay=MOMENTUM_DECAY
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.target_distinction += distinction_adjustment + 0.1 * self.distinction_momentum
#                 self.target_distinction = np.clip(self.target_distinction, 0.3, 0.9)  # Keep in reasonable range
# 
#                 print(f"Updated target_distinction to {self.target_distinction:.4f} based on current distinction")
# 
#             # 3. Update expulsion_recovery_rate based on system performance
#             if emergence_detected:
#                 # Increase recovery rate when emergence is detected
#                 self.expulsion_recovery_rate = min(self.expulsion_recovery_rate * 1.1, 0.05)
#                 print(f"Increased expulsion_recovery_rate to {self.expulsion_recovery_rate:.4f} based on emergence")
# 
#             # Adjust instability_grace_period based on system stability
#             if self.instability_counter > self.instability_grace_period - 1:
#                 # System is approaching instability threshold, increase grace period
#                 self.instability_grace_period = min(self.instability_grace_period + 1, 10)
#                 print(f"Increased instability_grace_period to {self.instability_grace_period}")
#             elif stability > 0.8 and self.instability_counter == 0 and self.instability_grace_period > 3:
#                 # System is very stable, gradually decrease grace period
#                 self.instability_grace_period = max(self.instability_grace_period - 1, 3)
#                 print(f"Decreased instability_grace_period to {self.instability_grace_period}")
# 
#             # Record significant constant changes
#             if (abs(self.surplus_threshold - prev_threshold) > 0.05 or
#                 abs(self.target_distinction - prev_distinction) > 0.05):
#                 self._record_constants_state("significant_update")
# 
#             # Record adaptation feedback
#             self.adaptation_feedback.append({
#                 'stability': stability,
#                 'distinction_level': distinction_level,
#                 'surplus': avg_surplus,
#                 'emergence_detected': emergence_detected,
#                 'constants_adjusted': {
#                     'surplus_threshold': self.surplus_threshold != prev_threshold,
#                     'target_distinction': self.target_distinction != prev_distinction
#                 },
#                 'timestamp': time.time()
#             })
# 
#         except Exception as e:
#             print(f"Error updating dynamic constants: {e}")
#             traceback.print_exc()
# 
#     def track_emergence(self, current_state: Dict[str, float]):
#         """Track and analyze emergent patterns while maintaining stability."""
#         try:
#             # -- FIX: Ensure novelty has a default value, even if emergence_patterns is empty
#             novelty = 0.0
# 
#             # Calculate novelty compared to recent history
#             if self.emergence_patterns:
#                 recent_states = list(self.emergence_patterns)[-10:]
#                 novelty = np.mean([
#                     self._compute_state_difference(current_state, past_state)
#                     for past_state in recent_states
#                 ])
#                 self.novelty_score = novelty
# 
#             # Calculate complexity
#             complexity = self._calculate_complexity(current_state)
#             self.complexity_score = complexity
# 
#             # Track the pattern
#             self.emergence_patterns.append(current_state)
# 
#             # Factor in excess stability potential to emergence detection
#             emergence_threshold_modifier = 1.0
#             # Initialize excess_stability_potential if not exists
#             if not hasattr(self, 'excess_stability_potential'):
#                 self.excess_stability_potential = 0.0
# 
#             if self.excess_stability_potential > 0:
#                 # Decrease threshold (making emergence more likely) when excess stability exists
#                 emergence_threshold_modifier = max(0.7, 1.0 - (self.excess_stability_potential * 0.5))
#                 print(f"DEBUG: Excess stability {self.excess_stability_potential:.4f} modifying emergence threshold by factor {emergence_threshold_modifier:.4f}")
# 
#             # Apply threshold modifier
#             effective_threshold = self.emergence_threshold * emergence_threshold_modifier
# 
#             # Update emergence counter if threshold exceeded
#             if novelty > effective_threshold and complexity > effective_threshold:
#                 # Calculate additional increments based on excess stability
#                 extra_increment = 0
#                 if self.excess_stability_potential > 0:
#                     extra_increment = int(self.excess_stability_potential * 3)
# 
#                 # Increment counter with potential bonus
#                 base_increment = 1
#                 total_increment = base_increment + extra_increment
# 
#                 # Apply increment
#                 self.emergence_counter += total_increment
# 
#                 if extra_increment > 0:
#                     print(f"DEBUG: Emergence counter increased by {total_increment} (including {extra_increment} from excess stability)")
# 
#                 self.emergence_history.append({
#                     'timestamp': time.time(),
#                     'novelty': novelty,
#                     'complexity': complexity,
#                     'state': current_state.copy(),
#                     'excess_stability': self.excess_stability_potential,
#                     'emergence_bonus': extra_increment
#                 })
# 
#                 # Gradually increase sensitivity to emergence
#                 self.emergence_sensitivity = min(1.0, self.emergence_sensitivity * 1.05)
# 
#                 # Update dynamic constants when emergence is detected
#                 self.update_dynamic_constants(
#                     phase_coherence=0.5,  # Default value if not available
#                     distinction_level=0.5,  # Default value if not available
#                     stability=self.surplus_state.stability,
#                     emergence_detected=True
#                 )
# 
#             else:
#                 # Gradually decrease sensitivity if no emergence detected
#                 self.emergence_sensitivity = max(0.1, self.emergence_sensitivity * 0.95)
# 
#         except Exception as e:
#             print(f"Error tracking emergence: {e}")
# 
# 
#     def _calculate_complexity(self, state: Dict[str, float]) -> float:
#         """Calculate complexity of current state with dynamic scaling."""
#         try:
#             # Calculate interaction terms between different surplus types
#             values = np.array(list(state.values()))
#             interactions = np.outer(values, values)
# 
#             # Calculate entropy of interactions
#             flat_interactions = interactions.flatten()
#             normalized = flat_interactions / (np.sum(flat_interactions) + 1e-10)
#             entropy = -np.sum(normalized * np.log2(normalized + 1e-10))
# 
#             # Calculate rate of change if history exists
#             if self.emergence_patterns:
#                 prev_state = self.emergence_patterns[-1]
#                 prev_values = np.array(list(prev_state.values()))
#                 rate_of_change = np.mean(np.abs(values - prev_values))
#             else:
#                 rate_of_change = 0.0
# 
#             # Combine metrics with dynamic weighting
#             complexity = (entropy * 0.6 + rate_of_change * 0.4)
#             return float(np.clip(complexity, 0, 1))
# 
#         except Exception as e:
#             print(f"Error calculating complexity: {e}")
#             return 0.0
# 
#     def _compute_state_difference(self, state1: Dict[str, float], state2: Dict[str, float]) -> float:
#         """Compute difference between two states with dynamic weighting."""
#         try:
#             keys = set(state1.keys()) & set(state2.keys())
#             if not keys:
#                 return 1.0  # Maximum difference if no common keys
# 
#             differences = []
#             for key in keys:
#                 val1 = float(state1[key])
#                 val2 = float(state2[key])
#                 # Calculate normalized difference
#                 max_val = max(abs(val1), abs(val2), 1.0)  # Avoid division by zero
#                 diff = abs(val1 - val2) / max_val
#                 differences.append(diff)
# 
#             return float(np.mean(differences))
# 
#         except Exception as e:
#             print(f"Error computing state difference: {e}")
#             return 0.0
# 
#     def _initialize_surplus_state(self) -> SurplusState:
#         """Ensure surplus state is always correctly initialized."""
#         try:
#             print("🔄 Initializing new surplus state...")
# 
#             # Create new surplus state with different accumulation rates
#             new_state = SurplusState()
# 
#             # Set different accumulation rates for different surplus types
#             new_state.accumulation_rate = {
#                 'basal': 0.008,       # Slower rate
#                 'cognitive': 0.012,   # Standard rate
#                 'predictive': 0.015,  # Faster rate
#                 'ontological': 0.010  # Medium rate
#             }
# 
#             # Validate new state
#             if not new_state.validate():
#                 print("⚠️ Initial surplus state validation failed, retrying...")
#                 new_state = SurplusState()  # Attempt a second initialization
# 
#                 if not new_state.validate():
#                     print("❌ Second validation attempt failed. Reverting to default.")
#                     new_state = SurplusState()  # Assign a default, even if imperfect
# 
#             return new_state
# 
#         except Exception as e:
#             print(f"❌ Error initializing surplus state: {e}")
#             traceback.print_exc()
#             return SurplusState()  # Ensure a valid fallback state
# 
#     def validate_current_state(self) -> bool:
#         """Validate surplus state and reset if needed."""
#         try:
#             if not isinstance(self.surplus_state, SurplusState):
#                 print("⚠️ Invalid surplus state type detected, reinitializing...")
#                 self.surplus_state = self._initialize_surplus_state()
# 
#             elif not self.surplus_state.validate():
#                 print("⚠️ Surplus state validation failed. Resetting...")
#                 self.surplus_state = self._initialize_surplus_state()
# 
#             return self.surplus_state.validate()
# 
#         except Exception as e:
#             print(f"❌ Error validating surplus state: {e}")
#             return False
# 
#     def check_system_stability(self):
#         """Monitors stability, avoiding immediate recovery unless multiple instability conditions persist."""
#         if not hasattr(self, 'min_steps_before_recovery'):
#             self.min_steps_before_recovery = 10  # Minimum steps before resetting
#             self.steps_taken = 0  # Track steps
# 
#         self.steps_taken += 1  # Increment step count
# 
#         if self.surplus_state.stability < self.stability_threshold:
#             self.instability_counter += 1  # Track consecutive instability events
# 
#             # Prevent premature recovery if we haven't hit the minimum step count
#             if self.steps_taken < self.min_steps_before_recovery:
#                 return  # Allow system to evolve before triggering recovery
# 
#             if self.instability_counter >= self.instability_grace_period:
#                 print("⚠️ System instability detected! Entering recovery mode...")
#                 self.enter_recovery_mode()
#                 self.instability_counter = 0  # Reset counter
#                 self.steps_taken = 0  # Reset step count
#         else:
#             self.instability_counter = max(0, self.instability_counter - 1)  # Reduce counter if stable
# 
#     def enter_recovery_mode(self):
#         """Enter recovery mode to stabilize the system."""
#         try:
#             self.recovery_state = {
#                 'steps_remaining': 30,
#                 'initial_stability': self.surplus_state.stability
#             }
# 
#             # Reduce surplus values to prevent cascading instability
#             for key in self.surplus_state.values:
#                 self.surplus_state.values[key] *= 0.8
# 
#             # Reset momentum values
#             self.accumulation_momentum = {k: 0.0 for k in self.surplus_state.values.keys()}
#             self.stability_momentum = 0.0
# 
#             # Set a moderate stability to allow recovery
#             self.surplus_state.stability = max(self.surplus_state.stability, 0.3)
# 
#             print("⚠️ Entering recovery mode for 30 steps")
# 
#         except Exception as e:
#             print(f"❌ Error entering recovery mode: {e}")
# 
#     def reset_surplus_state(self) -> bool:
#         """Reset surplus state to initial values."""
#         self.surplus_state = self._initialize_surplus_state()
#         return self.surplus_state is not None  # Ensures we always return a valid state
# 
#     def gradual_surplus_expulsion(self):
#         """Reduces surplus values gradually instead of instant expulsion."""
#         if not hasattr(self, 'surplus_state') or self.surplus_state is None:
#             print("⚠️ Warning: Cannot perform surplus expulsion—surplus_state not initialized.")
#             return
# 
#         DECAY_FACTOR = 0.1  # Adjust based on tuning
# 
#         try:
#             for key in self.surplus_state.values:
#                 self.surplus_state.values[key] *= (1 - DECAY_FACTOR)  # Slow decrease
#             print("⚠️ Gradual surplus expulsion applied.")
#         except Exception as e:
#             print(f"❌ Error in gradual_surplus_expulsion: {e}")
# 
#     def _create_initial_surplus_state(self) -> SurplusState:
#         """Create a properly initialized surplus state with retry mechanism."""
#         self.initialization_attempts = 0  # Reset counter
# 
#         while self.initialization_attempts < self.max_initialization_attempts:
#             try:
#                 print(f"Attempting surplus state initialization (attempt {self.initialization_attempts + 1})")
#                 new_state = SurplusState()
# 
#                 if new_state.validate():
#                     # Record successful initialization
#                     self.initialization_history.append({
#                         'timestamp': time.time(),
#                         'attempt': self.initialization_attempts,
#                         'success': True
#                     })
#                     print("✅ Surplus state initialized successfully")
#                     return new_state
#                 else:
#                     self.initialization_attempts += 1
#                     print(f"❌ Surplus state validation failed, attempt {self.initialization_attempts}")
# 
#                     # Record failed attempt
#                     self.initialization_history.append({
#                         'timestamp': time.time(),
#                         'attempt': self.initialization_attempts,
#                         'success': False
#                     })
# 
#             except Exception as e:
#                 self.initialization_attempts += 1
#                 print(f"❌ Error creating surplus state: {e}")
# 
#                 # Record error
#                 self.initialization_history.append({
#                     'timestamp': time.time(),
#                     'attempt': self.initialization_attempts,
#                     'success': False,
#                     'error': str(e)
#                 })
# 
#         print("⚠️ Maximum initialization attempts reached, using default state")
#         default_state = SurplusState()
# 
#         # Ensure default state is properly initialized
#         default_state.__post_init__()
# 
#         return default_state
# 
#     def reset_state(self) -> bool:
#         """Reset surplus state to initial values."""
#         try:
#             self.initialization_attempts = 0
#             self.surplus_state = self._create_initial_surplus_state()
#             self.accumulation_momentum = {k: 0.0 for k in self.surplus_state.values.keys()}
#             self.coupling_momentum = 0.0
#             self.steps_since_expulsion = 0
#             return self.surplus_state.validate()
#         except Exception as e:
#             print(f"Error resetting surplus state: {e}")
#             return False
# 
#     def check_expulsion_needed(self, distinction_level: float) -> bool:
#         """Check if surplus expulsion is needed based on current state"""
#         try:
#             if self.steps_since_expulsion < self.expulsion_cooldown:
#                 self.steps_since_expulsion += 1
#                 return False
# 
#             total_surplus = self.surplus_state.total_surplus()
#             stability = self.surplus_state.stability
# 
#             # Check expulsion conditions using dynamic surplus_threshold
#             conditions = [
#                 total_surplus > self.surplus_threshold,  # Using dynamic threshold instead of constant
#                 stability < self.stability_threshold,
#                 abs(distinction_level - 0.5) > 0.3  # High distinction deviation
#             ]
# 
#             return any(conditions)
# 
#         except Exception as e:
#             print(f"Error checking expulsion need: {e}")
#             return False
# 
#     def get_dynamic_constants(self) -> Dict[str, float]:
#         """
#         Return the current values of all dynamic constants.
# 
#         Returns:
#             Dictionary of dynamic constants and their current values
#         """
#         try:
#             return {
#                 'surplus_threshold': self.surplus_threshold,
#                 'expulsion_recovery_rate': self.expulsion_recovery_rate,
#                 'surplus_adjustment_rate': self.surplus_adjustment_rate,
#                 'phase_scaling_factor': self.phase_scaling_factor,
#                 'surplus_recycle_fraction': self.surplus_recycle_fraction,
#                 'core_distinction_update_rate': self.core_distinction_update_rate,
#                 'distinction_anchor_weight': self.distinction_anchor_weight,
#                 'target_distinction': self.target_distinction,
#                 'collapse_dissipation_threshold': self.collapse_dissipation_threshold,
#                 'collapse_dissipation_rate': self.collapse_dissipation_rate,
#                 'instability_grace_period': self.instability_grace_period,
#                 'constant_adaptation_rate': self.constant_adaptation_rate
#             }
#         except Exception as e:
#             print(f"Error getting dynamic constants: {e}")
#             return {}
# 
#     def get_constants_history(self) -> List[Dict]:
#         """
#         Get history of dynamic constant changes.
# 
#         Returns:
#             List of constant state snapshots with timestamps
#         """
#         try:
#             return list(self.constants_history)
#         except Exception as e:
#             print(f"Error getting constants history: {e}")
#             return []
# 
#     def update_surplus(self, phase_coherence: float, normalized_entropy: float):
#         """Update surplus values with enhanced validation and error handling."""
#         try:
#             # Validate current state before update
#             if not self.validate_current_state():
#                 print("⚠️ State validation failed, resetting surplus state")
#                 self.reset_state()
#                 return
# 
#             # Track emergence with properly referenced self state
#             if hasattr(self, 'track_emergence') and hasattr(self, 'surplus_state'):
#                 self.track_emergence(self.surplus_state.values)
# 
#             # Ensure inputs are proper floats
#             phase_coherence = float(phase_coherence)
#             normalized_entropy = float(normalized_entropy)
# 
#             # Compute base adjustment factors
#             coherence_factor = phase_coherence
#             entropy_factor = 1.0 - normalized_entropy
# 
#             # Store previous total surplus for stability calculation
#             previous_total_surplus = self.surplus_state.total_surplus()
# 
#             # Update each surplus type with different factors for each category
#             for key in self.surplus_state.values:
#                 try:
#                     # Get base rate with safety check
#                     base_rate = float(self.surplus_state.accumulation_rate.get(key, 0.01))
# 
#                     # Apply different adjustment calculations for each surplus type
#                     if key == 'basal':
#                         # Basal responds more to entropy
#                         adjustment = base_rate * (0.3 * coherence_factor - 0.7 * entropy_factor)
#                     elif key == 'cognitive':
#                         # Cognitive responds more to coherence
#                         adjustment = base_rate * (0.7 * coherence_factor - 0.3 * entropy_factor)
#                     elif key == 'predictive':
#                         # Predictive has its own unique pattern
#                         adjustment = base_rate * (coherence_factor * entropy_factor)
#                     elif key == 'ontological':
#                         # Ontological responds to the balance between coherence and entropy
#                         adjustment = base_rate * (coherence_factor - entropy_factor) * self.emergence_sensitivity
#                     else:
#                         # Default calculation for any other keys
#                         adjustment = base_rate * (coherence_factor - entropy_factor)
# 
#                     # Add randomness factor to create more variability (small random fluctuations)
#                     adjustment *= (1.0 + 0.1 * (np.random.random() - 0.5))
# 
#                     # Initialize momentum if needed
#                     if not hasattr(self, 'accumulation_momentum'):
#                         self.accumulation_momentum = {}
# 
#                     # Update momentum
#                     if key not in self.accumulation_momentum:
#                         self.accumulation_momentum[key] = 0.0
# 
#                     self.accumulation_momentum[key] = (
#                         MOMENTUM_DECAY * self.accumulation_momentum[key] +
#                         (1 - MOMENTUM_DECAY) * adjustment
#                     )
# 
#                     # Adjust surplus value with momentum
#                     current_value = float(self.surplus_state.values[key])
#                     new_value = current_value * (
#                         1.0 + adjustment + 0.1 * self.accumulation_momentum[key]
#                     )
# 
#                     # Ensure value stays within bounds
#                     self.surplus_state.values[key] = np.clip(new_value, 0.1, MAX_SURPLUS)
# 
#                 except Exception as e:
#                     print(f"❌ Error updating surplus for {key}: {e}")
#                     self.surplus_state.values[key] = 1.0  # Reset to safe default
# 
#             # Calculate current total surplus
#             current_total_surplus = self.surplus_state.total_surplus()
# 
#             # Calculate surplus change for stability update
#             surplus_change = abs(current_total_surplus - previous_total_surplus)
# 
#             # Update stability based on surplus change
#             stability_decrease = surplus_change * 0.05  # Adjust factor as needed
#             stability_increase = 0.02  # Small increase if surplus is stable
# 
#             # Calculate raw stability before clipping
#             raw_stability = self.surplus_state.stability - stability_decrease + stability_increase
# 
#             # Initialize excess stability potential attribute if it doesn't exist
#             if not hasattr(self, 'excess_stability_potential'):
#                 self.excess_stability_potential = 0.0
# 
#             # Store excess stability potential
#             self.excess_stability_potential = max(0.0, raw_stability - 1.0)
# 
#             # Apply clipping after storing the excess
#             new_stability = np.clip(raw_stability, 0.1, 1.0)
# 
#             print(f"DEBUG: Before stability update - current stability: {self.surplus_state.stability}")
#             print(f"DEBUG: surplus change: {surplus_change}, raw stability: {raw_stability}, excess: {self.excess_stability_potential}")
#             self.surplus_state.stability = new_stability
#             print(f"DEBUG: After stability update - new stability: {self.surplus_state.stability}")
# 
#             # Apply excess stability effects to surplus accumulation rates
#             if hasattr(self, 'excess_stability_potential') and self.excess_stability_potential > 0:
#                 # Scale accumulation rates based on excess stability
#                 stability_multiplier = 1.0 + (self.excess_stability_potential * 0.5)
#                 for key in self.surplus_state.accumulation_rate:
#                     original_rate = self.surplus_state.accumulation_rate[key]
#                     self.surplus_state.accumulation_rate[key] = original_rate * stability_multiplier
#                     print(f"DEBUG: Increased {key} accumulation rate from {original_rate:.6f} to {self.surplus_state.accumulation_rate[key]:.6f}")
# 
#             # Track history
#             self.accumulation_history.append({
#                 'timestamp': time.time(),
#                 'values': self.surplus_state.values.copy(),
#                 'stability': self.surplus_state.stability,
#                 'momentum': self.accumulation_momentum.copy(),
#                 'excess_stability': getattr(self, 'excess_stability_potential', 0.0)
#             })
# 
#             # Update dynamic constants based on current system state
#             self.update_dynamic_constants(
#                 phase_coherence=phase_coherence,
#                 distinction_level=0.5,  # Default if not available
#                 stability=self.surplus_state.stability
#             )
# 
#         except Exception as e:
#             print(f"❌ Error in surplus update: {e}")
#             self.reset_state()
# 
#     def get_emergence_metrics(self) -> Dict[str, float]:
#         """Get metrics about emergence patterns."""
#         return {
#             'novelty_score': self.novelty_score,
#             'complexity_score': self.complexity_score,
#             'emergence_sensitivity': self.emergence_sensitivity,
#             'emergence_count': self.emergence_counter,
#             'recent_emergence_rate': len(self.emergence_history) / max(1, len(self.emergence_patterns))
#         }
# 
#     def validate_state(self) -> bool:
#         """Validate current surplus state."""
#         try:
#             if not isinstance(self.surplus_state, SurplusState):
#                 print("Invalid surplus state type")
#                 return False
# 
#             if not isinstance(self.surplus_state.values, dict):
#                 print("Invalid surplus values type")
#                 return False
# 
#             required_keys = {'basal', 'cognitive', 'predictive', 'ontological'}
#             if not all(key in self.surplus_state.values for key in required_keys):
#                 print("Missing required surplus keys")
#                 return False
# 
#             if not all(isinstance(v, (int, float)) for v in self.surplus_state.values.values()):
#                 print("Invalid surplus value types")
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error validating surplus state: {e}")
#             return False
# 
#     def adjust_surplus(self, distinction_level: float,
#                    quantum_metrics: Dict[str, float],
#                    field_resistance: float) -> None:
#         """
#         Adjust surplus values based on current distinction and quantum metrics.
#         """
#         try:
#             # Compute a distinction factor to modulate adjustments.
#             distinction_factor = 1.0 - abs(distinction_level - 0.5)
#             base_rate = SURPLUS_ADJUSTMENT_RATE * distinction_factor
#             self._update_coupling_and_stability(quantum_metrics)
# 
#             for key in self.surplus_state.accumulation_rate:
#                 # Differentiate adjustment calculations by surplus type
#                 if key == 'basal':
#                     adjustment = base_rate * self.surplus_state.accumulation_rate[key] * 0.9
#                     adjustment *= (1.0 + 0.4 * quantum_metrics.get('phase_coherence', 0.5))
#                 elif key == 'cognitive':
#                     adjustment = base_rate * self.surplus_state.accumulation_rate[key] * 1.1
#                     adjustment *= (1.0 + 0.7 * (1.0 - quantum_metrics.get('normalized_entropy', 0.5)))
#                 elif key == 'predictive':
#                     adjustment = base_rate * self.surplus_state.accumulation_rate[key] * 1.0
#                     adjustment *= (0.8 + 0.4 * (1.0 - field_resistance))
#                 elif key == 'ontological':
#                     # Fixed the syntax error here - removed line continuation
#                     adjustment = base_rate * self.surplus_state.accumulation_rate[key] * 1.2
#                     adjustment *= (1.0 + 0.3 * quantum_metrics.get('phase_coherence', 0.5))
#                     adjustment *= (1.0 - 0.3 * quantum_metrics.get('normalized_entropy', 0.5))
# 
#                 # Add small random factor for natural variation
#                 adjustment *= (1.0 + 0.05 * (np.random.random() - 0.5))
# 
#                 self.accumulation_momentum[key] = update_momentum(self.accumulation_momentum[key], adjustment)
#                 self.surplus_state.values[key] *= (1.0 + adjustment + 0.1 * self.accumulation_momentum[key])
#                 self.surplus_state.values[key] = np.clip(self.surplus_state.values[key], 0.1, MAX_SURPLUS)
# 
#             # Append a snapshot of the current accumulation history.
#             self.accumulation_history.append({
#                 'values': self.surplus_state.values.copy(),
#                 'rates': self.surplus_state.accumulation_rate.copy(),
#                 'stability': self.surplus_state.stability,
#                 'quantum_coupling': self.surplus_state.quantum_coupling
#             })
#         except Exception as e:
#             print(f"Error adjusting surplus: {e}")
# 
#     def _update_coupling_and_stability(self, quantum_metrics: Dict[str, float]) -> None:
#         """
#         Update quantum coupling and surplus stability based on quantum metrics.
#         """
#         try:
#             target_coupling = quantum_metrics.get('phase_coherence', 0.5) * (1.0 - quantum_metrics.get('normalized_entropy', 0.5))
#             self.coupling_momentum = update_momentum(self.coupling_momentum, target_coupling - self.surplus_state.quantum_coupling)
#             self.surplus_state.quantum_coupling = np.clip(
#                 self.surplus_state.quantum_coupling + 0.1 * self.coupling_momentum, 0.1, 1.0
#             )
#             self.surplus_state.stability = np.clip(
#                 self.stability_decay * self.surplus_state.stability + 0.1 * quantum_metrics.get('phase_coherence', 0.5),
#                 0.1, 1.0
#             )
#             self.stability_history.append(self.surplus_state.stability)
#         except Exception as e:
#             print(f"Error updating coupling and stability: {e}")
# 
#     def get_surplus_state(self) -> SurplusState:
#         """Return the current surplus state."""
#         return self.surplus_state
# 
#     def perform_expulsion(self, quantum_state: Any) -> Tuple[Dict[str, float], float]:
#         """Perform surplus expulsion with quantum feedback and proper type handling"""
#         try:
#             # Validate and store current surplus state
#             if not isinstance(self.surplus_state, SurplusState):
#                 print("Warning: Invalid surplus state type, creating new instance")
#                 self.surplus_state = SurplusState()
# 
#             # Create a copy of current values before expulsion
#             expelled = self.surplus_state.values.copy()
# 
#             # Calculate total surplus with proper type handling
#             total_expelled = float(sum(self.surplus_state.values.values()))
# 
#             # Reset surplus to baseline with proper type handling
#             for key in self.surplus_state.values:
#                 self.surplus_state.values[key] = 1.0
# 
#             # Apply quantum corrections
#             try:
#                 quantum_state.apply_gate('x', [0])
#                 phase_shift = float(np.pi * (1.0 - self.surplus_state.stability))
#                 quantum_state.apply_phase_shift(phase_shift)
#             except Exception as qe:
#                 print(f"Error applying quantum corrections: {qe}")
# 
#             # Update expulsion tracking
#             self.surplus_state.last_expulsion = float(total_expelled)
#             self.steps_since_expulsion = 0
# 
#             # Track expulsion event
#             self.expulsion_history.append({
#                 'magnitude': float(total_expelled),
#                 'stability': float(self.surplus_state.stability),
#                 'timestamp': time.time()
#             })
# 
#             # Set up recovery state with proper type handling
#             self.recovery_state = {
#                 'steps_remaining': int(total_expelled * 10),
#                 'initial_magnitude': float(total_expelled)
#             }
# 
#             return expelled, float(total_expelled)
# 
#         except Exception as e:
#             print(f"Error performing expulsion: {e}")
#             default_values = {'basal': 1.0, 'cognitive': 1.0, 'predictive': 1.0, 'ontological': 1.0}
#             return default_values, 0.0
# 
#     def process_recovery(self, quantum_state: Any, distinction_level: float) -> None:
#         """Process recovery after surplus expulsion"""
#         if self.recovery_state is None:
#             return
# 
#         try:
#             self.recovery_state['steps_remaining'] -= 1
#             if self.recovery_state['steps_remaining'] <= 0:
#                 self.recovery_state = None
#                 print("Recovery complete")
#                 return
# 
#             # Calculate recovery rate based on distinction
#             recovery_rate = self.expulsion_recovery_rate * (1.0 + distinction_level)
# 
#             # Apply recovery to surplus values
#             for key in self.surplus_state.values:
#                 self.surplus_state.values[key] += recovery_rate
# 
#             # Apply quantum corrections during early recovery
#             phase_shift = np.pi * (0.8 - 0.5 * self.surplus_state.stability)
#             quantum_state.apply_phase_shift(phase_shift)
# 
#         except Exception as e:
#             print(f"Error processing recovery: {e}")
#             self.recovery_state = None
# 
#     def get_surplus_metrics(self) -> Dict[str, float]:
#         """Get comprehensive surplus metrics"""
#         try:
#             if not hasattr(self, 'surplus_state') or not isinstance(self.surplus_state, SurplusState):
#                 return {'error': 'Invalid surplus state'}
# 
#             metrics = {
#                 'total_surplus': self.surplus_state.total_surplus(),
#                 'stability': self.surplus_state.stability,
#                 'stability_momentum': self.stability_momentum,
#                 'coupling_momentum': self.coupling_momentum,
#                 'steps_since_expulsion': self.steps_since_expulsion,
#                 'in_recovery': self.recovery_state is not None,
#                 'excess_stability': getattr(self, 'excess_stability_potential', 0.0)  # Add this line
#             }
# 
#             # Add per-type metrics
#             for key in self.surplus_state.values:
#                 metrics[f'{key}_surplus'] = self.surplus_state.values[key]
#                 metrics[f'{key}_momentum'] = self.accumulation_momentum.get(key, 0.0)
#                 metrics[f'{key}_rate'] = self.surplus_state.accumulation_rate.get(key, 0.01)
# 
#             # Add recovery metrics if active
#             if self.recovery_state is not None:
#                 metrics['recovery_progress'] = 1.0 - (
#                     self.recovery_state['steps_remaining'] /
#                     (self.recovery_state['initial_magnitude'] * 10)
#                 )
# 
#             return metrics
# 
#         except Exception as e:
#             print(f"Error getting surplus metrics: {e}")
#             return {
#                 'total_surplus': 0.0,
#                 'stability': self.minimum_stability,
#                 'error': str(e)
#             }
# 
# # ---------------------------
# # EnhancedDistinctionDynamics
# # ---------------------------
# class EnhancedDistinctionDynamics:
#     """
#     Enhanced distinction dynamics with momentum-based updates and core distinction anchoring.
#     """
#     def __init__(self, memory_size: int = 100, phase_history_maxlen: int = 100) -> None:
#         self.cognitive_metrics = {}
#         self.distinction_history = deque(maxlen=memory_size)
#         self.phase_history = deque(maxlen=phase_history_maxlen)
#         self.adjustment_momentum = 0.0
#         self.distinction_level = 0.5
#         self.stability_factor = 1.0
#         self.quantum_influence = 0.0
#         self.core_distinction = 1.0  # Core anchoring mechanism
# 
#         # Convert hard-coded constants to dynamic instance variables
#         self.core_distinction_update_rate = 0.01  # Default from CORE_DISTINCTION_UPDATE_RATE
#         self.distinction_threshold = 0.7  # Default from TARGET_DISTINCTION
#         self.stability_decay = 0.7  # Default from MOMENTUM_DECAY
#         self.distinction_anchor_weight = 0.2  # Default from DISTINCTION_ANCHOR_WEIGHT
# 
#         # Additional adaptive parameters for dynamic operation
#         self.dynamic_anchor_weight = 0.2  # Initial value, can be adjusted
#         self.anchor_weight_momentum = 0.0
#         self.update_rate_momentum = 0.0
#         self.constants_adaptation_rate = 0.01  # Base rate for constant adjustment
# 
#         self.coherence_momentum = 0.0
#         self.distinction_momentum = 0.0
#         self.adaptation_history = deque(maxlen=1000)
#         self.stability_threshold = 0.1
#         self.recovery_mode = False
#         self.recovery_steps = 0
#         self.minimum_distinction = 0.1
#         self.learning_rate = 0.005  # Added learning rate attribute
# 
#         # History tracking for dynamic constants
#         self.constants_history = deque(maxlen=100)
#         self._record_constants_state("initialization")
# 
#     def _record_constants_state(self, event_type: str) -> None:
#         """Record the current state of all dynamic constants for tracking changes over time."""
#         try:
#             constants_state = {
#                 'core_distinction_update_rate': self.core_distinction_update_rate,
#                 'distinction_threshold': self.distinction_threshold,
#                 'stability_decay': self.stability_decay,
#                 'distinction_anchor_weight': self.distinction_anchor_weight,
#                 'dynamic_anchor_weight': self.dynamic_anchor_weight,
#                 'event_type': event_type,
#                 'timestamp': time.time()
#             }
#             self.constants_history.append(constants_state)
#         except Exception as e:
#             print(f"Error recording constants state: {e}")
# 
#     def update_dynamic_constants(self,
#                                distinction_level: float,
#                                stability_factor: float,
#                                distinction_variance: float,
#                                coherence: float) -> None:
#         """
#         Update dynamic constants based on system behavior and performance.
# 
#         Args:
#             distinction_level: Current distinction level
#             stability_factor: Current stability factor
#             distinction_variance: Variance in distinction level
#             coherence: Current coherence level
#         """
#         try:
#             # Store previous values to track changes
#             prev_anchor_weight = self.distinction_anchor_weight
#             prev_update_rate = self.core_distinction_update_rate
# 
#             # 1. Adjust anchor weight based on distinction variance and stability
#             if distinction_variance > 0.1:
#                 # High variance - increase anchor weight to stabilize
#                 anchor_adjustment = 0.02 * self.constants_adaptation_rate * distinction_variance
# 
#                 # Update momentum
#                 self.anchor_weight_momentum = update_momentum(
#                     self.anchor_weight_momentum,
#                     anchor_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.distinction_anchor_weight += anchor_adjustment + 0.1 * self.anchor_weight_momentum
#                 self.distinction_anchor_weight = np.clip(self.distinction_anchor_weight, 0.1, 0.5)
# 
#                 print(f"Increased distinction_anchor_weight to {self.distinction_anchor_weight:.4f} based on high variance")
# 
#             elif stability_factor > 0.8 and distinction_variance < 0.05:
#                 # System is stable with low variance - can reduce anchor weight
#                 anchor_adjustment = -0.01 * self.constants_adaptation_rate
# 
#                 # Update momentum
#                 self.anchor_weight_momentum = update_momentum(
#                     self.anchor_weight_momentum,
#                     anchor_adjustment,
#                     decay=self.stability_decay
#                 )
#                 # Apply adjustment with momentum
#                 self.distinction_anchor_weight += anchor_adjustment + 0.1 * self.anchor_weight_momentum
#                 self.distinction_anchor_weight = np.clip(self.distinction_anchor_weight, 0.1, 0.5)
# 
#                 print(f"Decreased distinction_anchor_weight to {self.distinction_anchor_weight:.4f} based on stability")
# 
#             # 2. Adjust update rate based on coherence and stability
#             if coherence > 0.7 and stability_factor > 0.7:
#                 # High coherence and stability - can use faster update rate
#                 update_rate_adjustment = 0.005 * self.constants_adaptation_rate * coherence
# 
#                 # Update momentum
#                 self.update_rate_momentum = update_momentum(
#                     self.update_rate_momentum,
#                     update_rate_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.core_distinction_update_rate += update_rate_adjustment + 0.05 * self.update_rate_momentum
#                 self.core_distinction_update_rate = np.clip(self.core_distinction_update_rate, 0.005, 0.05)
# 
#                 print(f"Increased core_distinction_update_rate to {self.core_distinction_update_rate:.4f} based on coherence")
# 
#             elif stability_factor < 0.4 or coherence < 0.3:
#                 # Low stability or coherence - use slower update rate
#                 update_rate_adjustment = -0.005 * self.constants_adaptation_rate
# 
#                 # Update momentum
#                 self.update_rate_momentum = update_momentum(
#                     self.update_rate_momentum,
#                     update_rate_adjustment,
#                     decay=self.stability_decay
#                 )
# 
#                 # Apply adjustment with momentum
#                 self.core_distinction_update_rate += update_rate_adjustment + 0.05 * self.update_rate_momentum
#                 self.core_distinction_update_rate = np.clip(self.core_distinction_update_rate, 0.005, 0.05)
# 
#                 print(f"Decreased core_distinction_update_rate to {self.core_distinction_update_rate:.4f} based on low stability")
# 
#             # 3. Adjust distinction threshold based on actual distinction level
#             distinction_delta = distinction_level - self.distinction_threshold
#             if abs(distinction_delta) > 0.2 and stability_factor > 0.6:
#                 # Move threshold toward actual distinction if stable
#                 threshold_adjustment = 0.02 * self.constants_adaptation_rate * distinction_delta
#                 self.distinction_threshold += threshold_adjustment
#                 self.distinction_threshold = np.clip(self.distinction_threshold, 0.3, 0.9)
# 
#                 print(f"Adjusted distinction_threshold to {self.distinction_threshold:.4f} based on actual distinction")
# 
#             # Record significant constant changes
#             if (abs(self.distinction_anchor_weight - prev_anchor_weight) > 0.02 or
#                 abs(self.core_distinction_update_rate - prev_update_rate) > 0.005):
#                 self._record_constants_state("significant_update")
# 
#         except Exception as e:
#             print(f"Error updating distinction dynamic constants: {e}")
#             traceback.print_exc()
# 
# 
#     def compute_distinction(self, quantum_metrics: Dict[str, float],
#                        field_resistance: float,
#                        surplus_state: SurplusState,
#                        excess_stability: float = 0.0) -> float:
#        """
#        Compute distinction with enhanced stability through momentum and core anchoring.
# 
#        Args:
#            quantum_metrics: Dictionary of quantum system metrics
#            field_resistance: Field resistance value
#            surplus_state: Surplus state object
#            excess_stability: Amount of excess stability potential
# 
#        Returns:
#            Distinction level value in range [0.1, 1.0]
#        """
#        try:
#            # Validate input metrics
#            if not isinstance(quantum_metrics, dict):
#                raise ValueError("Invalid metrics input")
# 
#            # Define default metrics with safe values
#            default_metrics = {
#                'normalized_entropy': 0.5,
#                'phase_distinction': 0.5,
#                'coherence_distinction': 0.5,
#                'quantum_surplus_coupling': 0.5,
#                'stability': 1.0,
#                'quantum_coupling': 1.0
#            }
# 
#            # Update metrics with defaults for missing values
#            metrics = {}
#            for key, default_value in default_metrics.items():
#                metrics[key] = quantum_metrics.get(key, default_value)
# 
#            entropy_component = 1.0 - metrics['normalized_entropy']
#            phase_component = metrics['phase_distinction']
#            coherence_component = max(metrics['coherence_distinction'], MINIMUM_COHERENCE_FLOOR)
# 
#            base_distinction = (
#                0.3 * phase_component +
#                0.3 * entropy_component +
#                0.2 * coherence_component +
#                0.2 * (1.0 - field_resistance)
#            )
# 
#            # Ensure surplus_state is valid
#            if not isinstance(surplus_state, SurplusState):
#                print("Warning: Invalid surplus state, creating new instance")
#                surplus_state = SurplusState()
# 
#            # Get quantum coupling
#            coupling = metrics['quantum_surplus_coupling']
# 
#            # Scale distinction
#            scaled_distinction = base_distinction * (0.5 + 0.5 * coupling)
# 
#            # Update momentum
#            delta = scaled_distinction - self.distinction_level
#            self.adjustment_momentum = update_momentum(self.adjustment_momentum, delta)
# 
#            # Calculate momentum contribution
#            momentum_contribution = self.adjustment_momentum * (1.0 - 0.8 * abs(scaled_distinction - 0.5))
# 
#            # Modify distinction calculation with excess stability
#            if excess_stability > 0:
#                # Amplify distinction changes when excess stability is present
#                scaled_distinction *= (1.0 + excess_stability * 0.5)
# 
#                # Reduce anchor weight to allow more variability
#                effective_anchor_weight = self.distinction_anchor_weight * (1.0 - min(0.4, excess_stability))
# 
#                # Update core distinction
#                if scaled_distinction > self.core_distinction:
#                    self.core_distinction = 0.95 * self.core_distinction + 0.05 * scaled_distinction
#                else:
#                    self.core_distinction = (
#                        (1 - self.core_distinction_update_rate) * self.core_distinction +
#                        self.core_distinction_update_rate * scaled_distinction
#                    )
# 
#                # Compute final distinction
#                final_distinction = (
#                    (1 - effective_anchor_weight) * (scaled_distinction + momentum_contribution) +
#                    effective_anchor_weight * self.core_distinction
#                )
#            else:
#                # Update core distinction
#                if scaled_distinction > self.core_distinction:
#                    self.core_distinction = 0.95 * self.core_distinction + 0.05 * scaled_distinction
#                else:
#                    self.core_distinction = (
#                        (1 - self.core_distinction_update_rate) * self.core_distinction +
#                        self.core_distinction_update_rate * scaled_distinction
#                    )
# 
#                # Apply momentum with stability
#                momentum_contribution = self.adjustment_momentum * (1.0 - 0.8 * abs(scaled_distinction - 0.5))
# 
#                # Compute final distinction
#                final_distinction = (
#                    (1 - self.distinction_anchor_weight) * (scaled_distinction + momentum_contribution) +
#                    self.distinction_anchor_weight * self.core_distinction
#                )
# 
#            # Apply stability-based adjustment
#            if self.stability_factor < 0.5:
#                final_distinction = 0.7 * final_distinction + 0.3 * self.core_distinction
# 
#            # Store history
#            self.distinction_history.append(final_distinction)
# 
#            # Calculate distinction variance for dynamic constant adjustment
#            distinction_variance = np.var(list(self.distinction_history)[-10:]) if len(self.distinction_history) >= 10 else 0.0
# 
#            # Update dynamic constants based on performance
#            self.update_dynamic_constants(
#                distinction_level=final_distinction,
#                stability_factor=self.stability_factor,
#                distinction_variance=distinction_variance,
#                coherence=metrics['coherence_distinction']
#            )
# 
#            return float(np.clip(final_distinction, self.minimum_distinction, 1.0))
# 
#        except Exception as e:
#            print(f"Error computing distinction: {e}")
#            return self.distinction_history[-1] if self.distinction_history else 0.5
# 
# 
#     def update_distinction_from_phase(self, estimated_phase: float) -> float:
#         """
#         Update the distinction level based on a new phase estimate.
#         This uses an exponential moving average to update momentum and blends the current
#         distinction with the historical average.
#         """
#         try:
#             avg_phase = estimated_phase if not self.phase_history else np.mean(self.phase_history)
#             phase_diff = abs(estimated_phase - avg_phase)
#             self.distinction_momentum = update_momentum(self.distinction_momentum, phase_diff)
#             new_distinction = ((1 - DISTINCTION_ANCHOR_WEIGHT) *
#                                (self.distinction_level +
#                                 self.distinction_momentum +
#                                 0.1 * (self.core_distinction - self.distinction_level)) +
#                                DISTINCTION_ANCHOR_WEIGHT * avg_phase)
#             self.distinction_level = float(np.clip(new_distinction, self.minimum_distinction, 1.0))
#             self.phase_history.append(estimated_phase)
#             self._update_stability(new_distinction)
#             return self.distinction_level
#         except Exception as e:
#             print(f"Error updating distinction from phase: {e}")
#             return self.distinction_level
# 
#     def _update_stability(self, new_distinction: float) -> None:
#         """
#         Update stability metrics based on recent changes in distinction.
#         """
#         try:
#             if len(self.distinction_history) > 1:
#                 recent_variance = np.var(list(self.distinction_history)[-10:])
#                 self.stability_factor = 1.0 / (1.0 + recent_variance)
#                 self.adaptation_history.append({
#                     'distinction': new_distinction,
#                     'stability': self.stability_factor,
#                     'momentum': self.distinction_momentum
#                 })
#                 if self.stability_factor < self.stability_threshold and not self.recovery_mode:
#                     if len(self.distinction_history) > 10 and np.var(list(self.distinction_history)[-10:]) > 0.01:
#                         self.enter_recovery_mode()
# 
#         except Exception as e:
#             print(f"Error updating stability: {e}")
# 
#     def enter_recovery_mode(self) -> None:
#         """Enter recovery mode to reduce rapid fluctuations in distinction."""
#         try:
#             self.recovery_mode = True
#             self.recovery_steps = 50
#             self.distinction_momentum = 0.0
#             print("Distinction dynamics entering recovery mode")
#         except Exception as e:
#             print(f"Error entering recovery mode: {e}")
# 
#     def get_distinction_metrics(self) -> Dict[str, float]:
#        """
#        Return a dictionary of distinction metrics including current distinction,
#        core distinction, stability, momentum values, and quantum influence.
#        """
#        try:
#            metrics = {
#                'current_distinction': self.distinction_level,
#                'core_distinction': self.core_distinction,
#                'stability_factor': self.stability_factor,
#                'distinction_momentum': self.distinction_momentum,
#                'adjustment_momentum': self.adjustment_momentum,
#                'quantum_influence': self.quantum_influence,
#                'recovery_mode': self.recovery_mode,
#                'recovery_steps': self.recovery_steps if self.recovery_mode else 0,
#                # Add dynamic constants to metrics
#                'distinction_anchor_weight': self.distinction_anchor_weight,
#                'core_distinction_update_rate': self.core_distinction_update_rate,
#                'distinction_threshold': self.distinction_threshold,
#                'anchor_weight_momentum': self.anchor_weight_momentum,
#                'update_rate_momentum': self.update_rate_momentum
#            }
#            if self.distinction_history:
#                metrics.update({
#                    'mean_distinction': float(np.mean(self.distinction_history)),
#                    'distinction_variance': float(np.var(self.distinction_history))
#                })
#            return metrics
#        except Exception as e:
#            print(f"Error getting distinction metrics: {e}")
#            return {
#                'current_distinction': self.distinction_level,
#                'core_distinction': self.core_distinction,
#                'stability_factor': self.stability_factor
#            }
# 
#     def get_constants_history(self) -> List[Dict]:
#        """
#        Get history of dynamic constant changes.
# 
#        Returns:
#            List of constant state snapshots with timestamps
#        """
#        try:
#            return list(self.constants_history)
#        except Exception as e:
#            print(f"Error getting constants history: {e}")
#            return []
# 
#     def track_history(self):
#         """Append the current distinction level to history."""
#         self.distinction_history.append(self.distinction_level)
# 
#

"""# 9. Emergent Potential (Field Interaction)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile emergent_potential.py
# """
# Emergent Potential Field for Émile-3K Simulation
# ------------------------------------------------
# Tracks and manages excess stability potentials across system components to
# create emergent properties and phase transitions.
# """
# import time
# import numpy as np
# import random
# from collections import deque
# from typing import Dict, List, Tuple, Any, Optional
# import traceback
# 
# class EmergentPotentialField:
#     """
#     Manages excess stability potentials across system components to create
#     emergent properties and phase transitions.
#     """
# 
#     def __init__(self, base_threshold: float = 0.5, history_size: int = 100):
#         """
#         Initialize the emergent potential field with enhanced adaptivity.
# 
#         Args:
#             base_threshold: Baseline threshold for triggering emergence
#             history_size: Size of history tracking
#         """
#         self.component_potentials = {}  # Maps components to their excess potentials
#         self.component_weights = {}     # Importance weights for different components
#         self.total_potential = 0.0      # Accumulated potential across all components
#         self.base_threshold = base_threshold  # Threshold baseline - never changes
#         self.threshold = base_threshold  # Current adaptive threshold
# 
#         # Advanced threshold adaptation parameters
#         self.min_threshold = 0.1        # Lower bound for threshold
#         self.max_threshold = 2.0        # Upper bound for threshold
#         self.threshold_adaptation_rate = 0.05  # How quickly threshold adapts
#         self.system_stability = 1.0     # Overall system stability measure
# 
#         # Enhanced emergence probability calculation
#         self.min_probability = 0.001    # Minimum emergence probability
#         self.probability_modifiers = {}  # Environmental factors affecting probability
# 
#         # Learning parameters
#         self.learning_rate = 0.01       # Rate for learning from emergence events
#         self.learning_enabled = True    # Whether to enable learning
# 
#         # History tracking
#         self.potential_history = deque(maxlen=history_size)
#         self.emergence_history = deque(maxlen=history_size)
#         self.threshold_history = deque(maxlen=history_size)  # Track threshold changes
# 
#         # Emergence state
#         self.last_emergence = None
#         self.emergence_active = False
#         self.emergence_cooldown = 0
#         self.emergence_counter = 0
# 
#         # Adaptation parameters
#         self.adaptation_rate = 0.02
#         self.threshold_momentum = 0.0
#         self.stability_factor = 1.0
#         self.field_intensity = 1.0
# 
#         # Performance metrics
#         self.avg_time_between_emergences = 120.0  # Default assumption (seconds)
#         self.successful_emergences = 0
#         self.failed_emergences = 0
#         self.emergence_duration_history = deque(maxlen=20)
# 
#         # Initialize default component weights
#         self._initialize_component_weights()
# 
#     def _initialize_component_weights(self):
#         """Initialize default importance weights for different components."""
#         self.component_weights = {
#             'surplus': 1.0,       # Surplus dynamics is a primary contributor
#             'distinction': 0.8,   # Distinction dynamics
#             'quantum': 0.9,       # Quantum state
#             'cognitive': 0.7,     # Cognitive structures
#             'field': 0.5          # Ontological field
#         }
# 
#         # Initialize probability modifiers
#         self.probability_modifiers = {
#             'system_complexity': 1.0,    # Higher complexity can increase probability
#             'environmental_noise': 1.0,  # More noise can increase probability
#             'recent_success_rate': 1.0,  # Success rate affects probability
#             'time_factor': 1.0,          # Time since last emergence
#         }
# 
#     def register_potential(self, component_id: str, potential: float,
#                       component_type: str = 'surplus', state_metrics: Optional[Dict] = None):
#         """
#         Register excess stability potential from a component with enhanced adaptivity.
# 
#         Args:
#             component_id: Unique identifier for the component
#             potential: Excess stability potential value
#             component_type: Type of component for weighting
#             state_metrics: Optional associated state metrics
# 
#         Returns:
#             True if an emergence event was triggered, False otherwise
#         """
#         try:
#             # Apply component weight to potential
#             weight = self.component_weights.get(component_type, 0.5)
#             weighted_potential = potential * weight
# 
#             # Store component potential
#             self.component_potentials[component_id] = {
#                 'raw_potential': potential,
#                 'weighted_potential': weighted_potential,
#                 'component_type': component_type,
#                 'timestamp': time.time(),
#                 'metrics': state_metrics or {}
#             }
# 
#             # Calculate new total potential
#             self._calculate_total_potential()
# 
#             # Adapt threshold based on system state
#             self._adapt_threshold_dynamic(state_metrics)
#             self.threshold_history.append(self.threshold)
# 
#             # Track history
#             self.potential_history.append({
#                 'timestamp': time.time(),
#                 'total_potential': self.total_potential,
#                 'components': self.component_potentials.copy(),
#                 'threshold': self.threshold
#             })
# 
#             # Calculate emergence probability with enhanced formula
#             emergence_probability = self._calculate_emergence_probability(state_metrics)
# 
#             # Update system stability based on new metrics
#             if state_metrics and 'stability' in state_metrics:
#                 self.system_stability = 0.9 * self.system_stability + 0.1 * state_metrics['stability']
# 
#             # Check for emergence threshold
#             triggered = False
#             if self.emergence_cooldown <= 0:
#                 # Use both threshold and random probability check
#                 if self.total_potential > self.threshold:
#                     triggered = self.trigger_emergence()
#                 elif random.random() < emergence_probability:
#                     print(f"RANDOM EMERGENCE: Probability triggered emergence with p={emergence_probability:.4f}")
#                     triggered = self.trigger_emergence()
# 
#                     # This was a spontaneous emergence
#                     if triggered and hasattr(self, 'last_emergence'):
#                         self.last_emergence['spontaneous'] = True
# 
#             # Update state even if not triggered
#             self._update_field_state()
# 
#             return triggered
# 
#         except Exception as e:
#             print(f"Error registering potential: {e}")
#             return False
# 
#     def _calculate_total_potential(self):
#         """Calculate the total potential across all components with enhanced time decay."""
#         try:
#             # Apply time decay to older potentials
#             current_time = time.time()
#             decay_factor = 0.9  # Decay rate for older potentials
# 
#             # Calculate weighted sum with time decay
#             total = 0.0
#             component_count = 0
# 
#             for component_id, data in list(self.component_potentials.items()):
#                 time_elapsed = current_time - data['timestamp']
#                 if time_elapsed > 60:  # Remove entries older than 60 seconds
#                     self.component_potentials.pop(component_id, None)
#                     continue
# 
#                 # Apply exponential time decay
#                 time_decay = decay_factor ** (time_elapsed / 10.0)  # Decay over time
# 
#                 # Extract component metrics if available
#                 metrics = data.get('metrics', {})
# 
#                 # Apply stability modifier if available in the metrics
#                 stability_modifier = 1.0
#                 if metrics and 'stability' in metrics:
#                     stability = metrics['stability']
#                     # Higher stability values lead to higher potentials
#                     stability_modifier = 0.5 + min(1.5, stability)
# 
#                 # Apply modified decay
#                 decayed_potential = data['weighted_potential'] * time_decay * stability_modifier
# 
#                 # Add to total and increment counter
#                 total += decayed_potential
#                 component_count += 1
# 
#             # Apply component diversity factor - more components = more potential
#             if component_count > 1:
#                 diversity_factor = min(1.5, 1.0 + (component_count - 1) * 0.1)
#                 total *= diversity_factor
# 
#             self.total_potential = total
# 
#         except Exception as e:
#             print(f"Error calculating total potential: {e}")
#             self.total_potential = sum(data.get('weighted_potential', 0.0)
#                                      for data in self.component_potentials.values())
# 
#     def _calculate_emergence_probability(self, state_metrics: Optional[Dict] = None) -> float:
#         """
#         Calculate enhanced probability of emergence based on multiple factors.
# 
#         Args:
#             state_metrics: Optional system state metrics
# 
#         Returns:
#             Probability of emergence (0.0 to 1.0)
#         """
#         try:
#             # Base probability is ratio of potential to threshold
#             base_probability = self.total_potential / (self.threshold * 1.2)
# 
#             # Apply probability modifiers
#             modifiers = 1.0
# 
#             # Time factor - more time since last emergence increases probability
#             if self.last_emergence:
#                 time_since_last = time.time() - self.last_emergence['timestamp']
#                 time_factor = min(2.0, 1.0 + time_since_last / self.avg_time_between_emergences)
#                 self.probability_modifiers['time_factor'] = time_factor
#                 modifiers *= time_factor
# 
#             # Recent success rate affects probability
#             if self.successful_emergences + self.failed_emergences > 0:
#                 success_rate = self.successful_emergences / (self.successful_emergences + self.failed_emergences)
#                 success_factor = 0.5 + success_rate
#                 self.probability_modifiers['recent_success_rate'] = success_factor
#                 modifiers *= success_factor
# 
#             # System state metrics can affect probability
#             if state_metrics:
#                 # System complexity
#                 if 'complexity' in state_metrics:
#                     complexity = state_metrics['complexity']
#                     complexity_factor = min(1.5, 0.8 + complexity)
#                     self.probability_modifiers['system_complexity'] = complexity_factor
#                     modifiers *= complexity_factor
# 
#                 # Environmental noise (entropy)
#                 if 'entropy' in state_metrics:
#                     entropy = state_metrics['entropy']
#                     noise_factor = min(1.5, 0.8 + entropy)
#                     self.probability_modifiers['environmental_noise'] = noise_factor
#                     modifiers *= noise_factor
# 
#             # Calculate final probability with all modifiers
#             final_probability = base_probability * modifiers
# 
#             # Apply cooldown reduction if in cooldown
#             if self.emergence_cooldown > 0:
#                 final_probability *= 0.1
# 
#             # Ensure probability is in valid range
#             return max(self.min_probability, min(0.95, final_probability))
# 
#         except Exception as e:
#             print(f"Error calculating emergence probability: {e}")
#             return self.min_probability
# 
#     def _adapt_threshold_dynamic(self, state_metrics: Optional[Dict] = None):
#         """
#         Adapt the emergence threshold dynamically based on system state, history, and performance.
# 
#         Args:
#             state_metrics: Optional system state metrics
#         """
#         try:
#             # Start with current threshold
#             new_threshold = self.threshold
# 
#             # Adjust based on emergence history and frequency
#             if len(self.emergence_history) >= 2:
#                 # Calculate average time between emergences
#                 timestamps = [e['timestamp'] for e in self.emergence_history]
#                 intervals = [timestamps[i] - timestamps[i-1] for i in range(1, len(timestamps))]
# 
#                 if intervals:
#                     self.avg_time_between_emergences = sum(intervals) / len(intervals)
# 
#                     # Target frequency adjustment
#                     target_interval = 60.0  # Target 1 minute between emergences
# 
#                     if self.avg_time_between_emergences < target_interval * 0.5:
#                         # Too frequent - increase threshold
#                         adjustment = +0.05
#                     elif self.avg_time_between_emergences > target_interval * 2.0:
#                         # Too infrequent - decrease threshold
#                         adjustment = -0.05
#                     else:
#                         # Within acceptable range - small adjustments for fine-tuning
#                         interval_ratio = target_interval / self.avg_time_between_emergences
#                         adjustment = 0.02 * (1.0 - interval_ratio)
# 
#                     # Apply adjustment with learning rate
#                     new_threshold += adjustment * self.threshold_adaptation_rate
# 
#             # Adjust based on system state metrics
#             if state_metrics:
#                 metrics_adjustment = 0.0
# 
#                 # Adjust based on stability
#                 if 'stability' in state_metrics:
#                     stability = state_metrics['stability']
#                     # Lower thresholds for highly stable systems to encourage emergence
#                     # Higher thresholds for unstable systems to prevent too much emergence
#                     metrics_adjustment -= (stability - 0.5) * 0.1
# 
#                 # Adjust based on complexity
#                 if 'complexity' in state_metrics:
#                     complexity = state_metrics['complexity']
#                     # More complex systems have lower thresholds (more likely to emerge)
#                     metrics_adjustment -= (complexity - 0.5) * 0.1
# 
#                 # Apply metrics-based adjustment
#                 new_threshold += metrics_adjustment * self.threshold_adaptation_rate
# 
#             # Apply threshold momentum for smoother transitions
#             self.threshold_momentum = 0.9 * self.threshold_momentum + 0.1 * (new_threshold - self.threshold)
#             new_threshold = self.threshold + self.threshold_momentum
# 
#             # Ensure threshold stays within bounds
#             self.threshold = max(self.min_threshold, min(self.max_threshold, new_threshold))
# 
#         except Exception as e:
#             print(f"Error adapting threshold: {e}")
#             # Keep current threshold on error
# 
#     def trigger_emergence(self):
#         """
#         Trigger system-wide emergence based on accumulated potential with enhanced tracking.
# 
#         Returns:
#             True if emergence was triggered, False otherwise
#         """
#         try:
#             # Safety check for cooldown
#             if self.emergence_cooldown > 0:
#                 return False
# 
#             # Prepare emergence data
#             self.last_emergence = {
#                 'timestamp': time.time(),
#                 'potential': self.total_potential,
#                 'threshold': self.threshold,
#                 'probability_modifiers': self.probability_modifiers.copy(),
#                 'contributors': {k: v.copy() for k, v in self.component_potentials.items()},
#                 'intensity': self._calculate_emergence_intensity(),
#                 'sequence_number': self.emergence_counter,
#                 'spontaneous': False  # Default - changed if random emergence
#             }
# 
#             # Store in history
#             self.emergence_history.append(self.last_emergence)
# 
#             # Update counters and state
#             self.emergence_counter += 1
#             self.emergence_active = True
# 
#             # Calculate adaptive cooldown based on emergence intensity
#             intensity = self.last_emergence['intensity']
#             base_cooldown = 10  # Minimum cooldown period
#             intensity_factor = intensity * 3  # Higher intensity = longer cooldown
# 
#             # Apply randomness for variability
#             random_factor = 0.8 + 0.4 * random.random()  # 0.8 to 1.2
# 
#             # Calculate final cooldown
#             self.emergence_cooldown = int((base_cooldown + intensity_factor) * random_factor)
# 
#             # Reset potentials after emergence (but not completely)
#             reset_factor = max(0.1, min(0.5, 0.3 / intensity))  # Stronger emergence = more complete reset
#             self.component_potentials = {k: {**v, 'weighted_potential': v['weighted_potential'] * reset_factor}
#                                        for k, v in self.component_potentials.items()}
#             self._calculate_total_potential()
# 
#             # Adapt threshold based on emergence history and this event
#             self._adapt_threshold()
# 
#             # Track successful emergence
#             self.successful_emergences += 1
# 
#             print(f"EMERGENCE TRIGGERED: Potential {self.total_potential:.4f}, Intensity {self.last_emergence['intensity']:.4f}, Threshold {self.threshold:.4f}")
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error triggering emergence: {e}")
#             self.failed_emergences += 1
#             return False
# 
#     def _calculate_emergence_intensity(self):
#         """
#         Calculate the intensity of the emergence event with enhanced dynamics.
# 
#         Returns:
#             Intensity value (1.0 to 5.0)
#         """
#         try:
#             # Base intensity is how much the potential exceeds the threshold
#             threshold_ratio = self.total_potential / max(self.threshold, 0.001)
# 
#             # Component diversity factor - more different components = more intense emergence
#             component_types = set(data['component_type'] for data in self.component_potentials.values())
#             diversity_factor = min(2.0, 1.0 + len(component_types) * 0.2)
# 
#             # System stability factor - less stable systems have more intense emergence
#             stability_factor = max(1.0, 2.0 - self.system_stability)
# 
#             # Calculate base intensity with these factors
#             base_intensity = threshold_ratio * diversity_factor * stability_factor
# 
#             # Scale to a reasonable range [1.0 - 5.0]
#             scaled_intensity = 1.0 + min(4.0, base_intensity - 1.0)
# 
#             # Add some randomness for variation (±10%)
#             randomness = 0.9 + 0.2 * random.random()
#             final_intensity = scaled_intensity * randomness
# 
#             # Track this intensity in history for learning
#             if hasattr(self, 'intensity_history'):
#                 self.intensity_history.append(final_intensity)
#             else:
#                 self.intensity_history = deque(maxlen=20)
#                 self.intensity_history.append(final_intensity)
# 
#             return float(final_intensity)
# 
#         except Exception as e:
#             print(f"Error calculating emergence intensity: {e}")
#             return 1.0
# 
#     def _adapt_threshold(self):
#         """
#         Adapt the emergence threshold based on emergence history with learning capabilities.
#         """
#         try:
#             # Get emergence timestamps and intensities
#             if len(self.emergence_history) < 2:
#                 return
# 
#             recent_emergences = list(self.emergence_history)[-10:]
#             timestamps = [e['timestamp'] for e in recent_emergences]
#             intensities = [e.get('intensity', 1.0) for e in recent_emergences]
# 
#             # Skip adaptation if learning is disabled
#             if not self.learning_enabled:
#                 return
# 
#             # Calculate time intervals
#             intervals = []
#             for i in range(1, len(timestamps)):
#                 interval = timestamps[i] - timestamps[i-1]
#                 intervals.append(interval)
# 
#             if not intervals:
#                 return
# 
#             # Calculate mean interval and intensity
#             mean_interval = np.mean(intervals)
#             mean_intensity = np.mean(intensities)
# 
#             # Learning parameters
#             target_interval = 60.0  # Target 1 minute between emergences
#             target_intensity = 2.5  # Target medium intensity
# 
#             # Adjustments based on targets
#             interval_adjustment = 0.0
#             intensity_adjustment = 0.0
# 
#             # Interval adjustment
#             if mean_interval < target_interval * 0.5:
#                 # Too frequent - increase threshold
#                 interval_adjustment = 0.05 * (1.0 - (mean_interval / target_interval))
#             elif mean_interval > target_interval * 2.0:
#                 # Too infrequent - decrease threshold
#                 interval_adjustment = -0.05 * ((mean_interval / target_interval) - 1.0)
# 
#             # Intensity adjustment
#             if mean_intensity < target_intensity * 0.7:
#                 # Too weak - decrease threshold
#                 intensity_adjustment = -0.03 * (1.0 - (mean_intensity / target_intensity))
#             elif mean_intensity > target_intensity * 1.3:
#                 # Too strong - increase threshold
#                 intensity_adjustment = 0.03 * ((mean_intensity / target_intensity) - 1.0)
# 
#             # Calculate combined adjustment
#             combined_adjustment = (interval_adjustment * 0.7) + (intensity_adjustment * 0.3)
# 
#             # Apply momentum to changes
#             self.threshold_momentum = 0.9 * self.threshold_momentum + 0.1 * combined_adjustment
# 
#             # Update threshold with momentum and learning rate
#             threshold_change = self.threshold_momentum * self.learning_rate
#             self.threshold = max(self.min_threshold, min(self.max_threshold, self.threshold + threshold_change))
# 
#             # Add some small random walk for exploration
#             random_adjust = 0.002 * (random.random() - 0.5)
#             self.threshold += random_adjust
# 
#             # Record this threshold change
#             if hasattr(self, 'threshold_changes'):
#                 self.threshold_changes.append({
#                     'timestamp': time.time(),
#                     'new_threshold': self.threshold,
#                     'adjustment': combined_adjustment,
#                     'momentum': self.threshold_momentum
#                 })
# 
#         except Exception as e:
#             print(f"Error adapting threshold: {e}")
# 
#     def _update_field_state(self):
#         """
#         Update internal field state based on potentials and recent history with enhanced dynamics.
#         """
#         try:
#             # Decrease cooldown if active
#             if self.emergence_cooldown > 0:
#                 self.emergence_cooldown -= 1
# 
#                 # Deactivate emergence when cooldown reaches 0
#                 if self.emergence_cooldown == 0 and self.emergence_active:
#                     self.emergence_active = False
# 
#                     # Calculate duration of this emergence
#                     if self.last_emergence:
#                         duration = time.time() - self.last_emergence['timestamp']
#                         self.emergence_duration_history.append(duration)
# 
#                         # Update performance metrics
#                         if hasattr(self, 'performance'):
#                             self.performance = {
#                                 'avg_duration': np.mean(self.emergence_duration_history),
#                                 'max_duration': max(self.emergence_duration_history),
#                                 'min_duration': min(self.emergence_duration_history)
#                             }
# 
#                     print(f"EMERGENCE DEACTIVATED: Field now stable. Threshold at {self.threshold:.4f}")
# 
#             # Update field intensity based on potential with more dynamic range
#             target_intensity = 1.0 + min(4.0, self.total_potential * 2.0)
#             self.field_intensity = 0.9 * self.field_intensity + 0.1 * target_intensity
# 
#             # Update stability factor - higher potentials decrease stability
#             target_stability = 1.0
# 
#             if self.total_potential > (self.threshold * 0.7):
#                 # Approaching threshold - decrease stability
#                 target_stability = max(0.3, 1.0 - (self.total_potential / self.threshold))
#             else:
#                 # Below threshold - maintain high stability
#                 potential_ratio = self.total_potential / self.threshold
#                 target_stability = min(1.0, 0.8 + (1.0 - potential_ratio) * 0.2)
# 
#             # Apply adjustment with smoothing
#             self.stability_factor = 0.95 * self.stability_factor + 0.05 * target_stability
# 
#             # Update learning rate based on system stability
#             # More stable systems can learn faster
#             target_learning_rate = min(0.05, max(0.001, self.stability_factor * 0.05))
#             self.learning_rate = 0.95 * self.learning_rate + 0.05 * target_learning_rate
# 
#         except Exception as e:
#             print(f"Error updating field state: {e}")
# 
#     def update_component_weights(self, component_type: str, weight_change: float):
#         """
#         Dynamically update the importance weights for different components.
# 
#         Args:
#             component_type: Type of component to update
#             weight_change: Change in weight (positive or negative)
#         """
#         if component_type in self.component_weights:
#             current_weight = self.component_weights[component_type]
#             new_weight = max(0.1, min(2.0, current_weight + weight_change))
#             self.component_weights[component_type] = new_weight
#             print(f"Updated component weight for {component_type}: {current_weight:.2f} -> {new_weight:.2f}")
# 
#     def set_system_state(self, state_metrics: Dict[str, float]):
#         """
#         Update system state metrics to influence emergence behavior.
# 
#         Args:
#             state_metrics: Dictionary of system state metrics
#         """
#         try:
#             if 'stability' in state_metrics:
#                 self.system_stability = state_metrics['stability']
# 
#             # Update probability modifiers
#             if 'complexity' in state_metrics:
#                 self.probability_modifiers['system_complexity'] = 0.5 + state_metrics['complexity']
# 
#             if 'entropy' in state_metrics:
#                 self.probability_modifiers['environmental_noise'] = 0.5 + state_metrics['entropy']
# 
#             # Adapt threshold based on new state
#             self._adapt_threshold_dynamic(state_metrics)
# 
#         except Exception as e:
#             print(f"Error setting system state: {e}")
# 
#     def get_field_state(self):
#         """
#         Get the current state of the emergent potential field with enhanced metrics.
# 
#         Returns:
#             Dictionary with field state metrics
#         """
#         try:
#             current_time = time.time()
# 
#             # Calculate time since last emergence
#             time_since_emergence = 0
#             if self.last_emergence:
#                 time_since_emergence = current_time - self.last_emergence['timestamp']
# 
#             # Calculate average emergence duration
#             avg_duration = 0
#             if self.emergence_duration_history:
#                 avg_duration = sum(self.emergence_duration_history) / len(self.emergence_duration_history)
# 
#             # Calculate emergence probability with current system state
#             emergence_probability = self._calculate_emergence_probability()
# 
#             # Calculate threshold trend
#             threshold_trend = 0
#             if len(self.threshold_history) > 5:
#                 recent_thresholds = list(self.threshold_history)[-5:]
#                 first_avg = sum(recent_thresholds[:2]) / 2
#                 last_avg = sum(recent_thresholds[-2:]) / 2
#                 threshold_trend = last_avg - first_avg
# 
#             # Prepare state summary
#             state = {
#                 'total_potential': self.total_potential,
#                 'threshold': self.threshold,
#                 'threshold_trend': threshold_trend,
#                 'threshold_momentum': self.threshold_momentum,
#                 'emergence_active': self.emergence_active,
#                 'emergence_cooldown': self.emergence_cooldown,
#                 'stability_factor': self.stability_factor,
#                 'field_intensity': self.field_intensity,
#                 'emergence_count': self.emergence_counter,
#                 'time_since_emergence': time_since_emergence,
#                 'avg_emergence_duration': avg_duration,
#                 'component_count': len(self.component_potentials),
#                 'component_types': list(set(data['component_type'] for data in self.component_potentials.values())),
#                 'strongest_component': self._get_strongest_component(),
#                 'emergence_probability': emergence_probability,
#                 'probability_modifiers': self.probability_modifiers,
#                 'learning_rate': self.learning_rate,
#                 'system_stability': self.system_stability
#             }
# 
#             return state
# 
#         except Exception as e:
#             print(f"Error getting field state: {e}")
#             return {
#                 'error': str(e),
#                 'total_potential': self.total_potential,
#                 'emergence_active': self.emergence_active,
#                 'stability_factor': self.stability_factor,
#                 'threshold': self.threshold
#             }
# 
#     def _get_strongest_component(self):
#         """Get the strongest contributing component to the potential field."""
#         try:
#             if not self.component_potentials:
#                 return None
# 
#             # Find component with highest weighted potential
#             strongest = max(self.component_potentials.items(),
#                           key=lambda x: x[1]['weighted_potential'])
# 
#             return {
#                 'id': strongest[0],
#                 'type': strongest[1]['component_type'],
#                 'potential': strongest[1]['weighted_potential'],
#                 'raw_potential': strongest[1]['raw_potential']
#             }
# 
#         except Exception as e:
#             print(f"Error finding strongest component: {e}")
#             return None

"""# 10. Agent Classes"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile agent_classes.py
# """
# Enhanced Agent Module
# ----------------------
# This module defines classes for an enhanced quantum‐aware agent. It includes a
# base agent class, an agent class with prediction capabilities, and a final evolution
# agent that integrates quantum state management, recursive memory, transformer-based
# decision making, surplus regulation, and advanced adaptation mechanisms.
# """
# import math
# import time
# import random
# import numpy as np
# import traceback
# from collections import deque, defaultdict
# from typing import Any, Dict, List, Optional, Tuple, Union
# 
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# import torch.optim as optim
# 
# from qiskit.quantum_info import Statevector
# from qiskit_aer.library import SaveStatevector
# from qiskit import QuantumCircuit
# from qiskit_aer import AerSimulator
# 
# from emergent_potential import EmergentPotentialField
# from logging_setup import setup_logging
# from emergence_monitor import EmergenceTracker, DimensionMonitor, EmergenceEvent
# from transformer_modules import FourDimTransformerAdapter, RecursiveDistinctionTransformer
# from core_quantum import EnhancedQuantumState
# from surplusdynamics import EnhancedDistinctionDynamics, EnhancedSurplusDynamics
# from memory_field import RecursiveDistinctionMemory, OntologicalField
# from training_pipeline import (
#     QuantumAwareOptimizer,
#     EnhancedTrainingPipeline,
#     EnhancedErrorRecovery,
#     OptimizationCoordinator,
#     QuantumStateValidator,
#     StateSynchronizationManager,
#     EnhancedQuantumSelfOptimization
# )
# from data_classes import TransformerOutput, SurplusState
# from utilities import (
#     _initialize_circuit,
#     MOMENTUM_DECAY,
#     MINIMUM_COHERENCE_FLOOR,
#     INSTABILITY_GRACE_PERIOD,
#     adapt_tensor_shape,
#     NUM_QUBITS_PER_AGENT,
#     COLLAPSE_DISSIPATION_THRESHOLD,
#     EXPULSION_RECOVERY_RATE,
#     CORE_DISTINCTION_UPDATE_RATE,
#     TARGET_DISTINCTION,
#     DISTINCTION_ANCHOR_WEIGHT,
#     NUM_TRANSFORMER_LAYERS,
#     NUM_TRANSFORMER_HEADS,
#     HIDDEN_DIM
# )
# from training_pipeline import MetricValidator, QuantumStateValidator
# from cognitive_structures import RecursiveCognitiveStructuring
# from analysis import QuantumAnalyzer, CausalityAnalysis, BayesianAnalysis
# from symbolic_output import SymbolicOutput
# 
# 
# 
# # Global constants
# NUM_QUBITS_PER_AGENT = 4
# LEARNING_RATE = 1e-4
# LEARNING_RATE_MIN = 1e-5
# LEARNING_RATE_MAX = 1e-3
# WEIGHT_DECAY = 0.01
# GRADIENT_CLIP_VALUE = 1.0
# REWARD_SCALING = 1.0
# EVOLUTION_TIME = 0.1
# 
# # Device configuration
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 
# # -----------------------------------------------------------------------------
# # Agent-Level Classes
# # -----------------------------------------------------------------------------
# class EnhancedSingleAgentBase:
#     """
#     Base class for an enhanced quantum-aware agent.
#     Integrates an enhanced quantum state, recursive memory, and dynamics for computing distinction and regulating surplus.
#     """
#     def __init__(self, num_qubits: int = NUM_QUBITS_PER_AGENT):
#         self.num_qubits = num_qubits
#         self.qc, self.simulator = _initialize_circuit(self.num_qubits)
#         self.minimum_coherence = MINIMUM_COHERENCE_FLOOR
# 
#         # Initialize quantum state first
#         self.quantum_state = EnhancedQuantumState(agent=self, num_qubits=num_qubits)
# 
#         # Initialize distinction and surplus dynamics
#         self.distinction_dynamics = EnhancedDistinctionDynamics()
#         self.surplus_dynamics = EnhancedSurplusDynamics()
#         print(f"DEBUG: Surplus Dynamics Initialized - Surplus State: {self.surplus_dynamics.surplus_state}")
# 
#         # Initialize memory system
#         self.memory = RecursiveDistinctionMemory(max_size=10000, hierarchy_levels=4)
# 
#         # Initialize optimizer (with None model for now - will be set later)
#         self.quantum_optimizer = QuantumAwareOptimizer(model=None)
# 
#         # History tracking
#         self.distinction_history = deque(maxlen=1000)
#         self.quantum_metric_history = deque(maxlen=1000)
# 
#         # State initialization
#         self.distinction_level = 0.5  # Initialize with moderate distinction
#         self.learning_rate = 0.005
#         self.phase = 0.0
#         self.stability_factor = 1.0
# 
#         # Initialize field
#         self.ontological_field = OntologicalField()
# 
#         # Initialize statevector
#         self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#         if self.qc is None or self.simulator is None:
#             raise RuntimeError("❌ Failed to initialize quantum circuit and simulator.")
# 
#     def compute_distinction(self) -> float:
#         """
#         Compute the agent's distinction level by combining:
#           - Phase-based distinction
#           - Entropy-based distinction (1 - normalized entropy)
#           - Coherence-based distinction
#           - The effect of field resistance
#         The result is scaled by the quantum-surplus coupling.
#         """
#         try:
#             # Make sure stability_factor and other attributes are set
#             self.stability_factor = getattr(self.distinction_dynamics, 'stability_factor', 1.0)
#             self.learning_rate = getattr(self.distinction_dynamics, 'learning_rate', 0.005)
#             self.minimum_coherence = getattr(self.distinction_dynamics, 'minimum_distinction', MINIMUM_COHERENCE_FLOOR)
# 
# 
#             # Use the existing surplus state
#             self.surplus_state = self.surplus_dynamics.surplus_state
# 
#             # Update quantum state parameters
#             self.quantum_state.phase_coherence = getattr(self.distinction_dynamics, 'quantum_influence', MINIMUM_COHERENCE_FLOOR)
#             self.quantum_state.minimum_coherence = self.minimum_coherence
# 
#             # Get quantum metrics and calculate distinction
#             metrics = self.quantum_state.get_quantum_metrics()
#             coupling = self.quantum_state.compute_quantum_surplus_coupling(self.surplus_state.values)
#             field_resistance = self.ontological_field.resistance(self.distinction_level)
# 
#             # Calculate the total distinction from various components
#             total_distinction = (
#                 0.3 * metrics.get('phase_distinction', 0.5) +
#                 0.3 * (1 - metrics.get('normalized_entropy', 0.5)) +
#                 0.2 * metrics.get('coherence_distinction', 0.5) +
#                 0.2 * (1 - field_resistance)
#             )
# 
#             # Scale by quantum coupling
#             total_distinction *= (0.5 + 0.5 * coupling)
# 
#             return float(np.clip(total_distinction, 0, 1))
# 
#         except Exception as e:
#             print(f"Error in computing distinction: {e}")
#             traceback.print_exc()
#             return self.distinction_level  # Return current value if error occurs
# 
#     def _update_quantum_metrics(self):
#         """Update and store quantum metrics from the current quantum state."""
#         try:
#             metrics = self.quantum_state.get_quantum_metrics() or {}  # Ensure it's a dict
#             self.quantum_metric_history.append(metrics)
# 
#             self.phase = metrics.get('phase', 0.0)  # Avoid KeyError
#             self.quantum_state.phase_coherence = metrics.get('phase_coherence', self.minimum_coherence)
# 
#         except Exception as e:
#             print(f"Error updating quantum metrics: {e}")
#             traceback.print_exc()
# 
#     def update_distinction_and_surplus(self) -> None:
#         """Update distinction level and adjust surplus values accordingly."""
#         try:
#             # Get current metrics
#             metrics = self.quantum_state.get_quantum_metrics()
#             field_resistance = self.ontological_field.resistance(self.distinction_level)
# 
#             # Calculate new distinction level
#             self.distinction_level = self.distinction_dynamics.compute_distinction(
#                 metrics, field_resistance, self.surplus_dynamics.surplus_state
#             )
# 
#             # Validate surplus state before modifying
#             if not isinstance(self.surplus_dynamics.surplus_state, SurplusState) or self.surplus_dynamics.surplus_state is None:
#                 print(f"Warning: Reinitializing surplus state as SurplusState()")
#                 self.surplus_dynamics.surplus_state = SurplusState()
# 
#             # Update surplus based on quantum metrics
#             self.surplus_dynamics.update_surplus(
#                 metrics.get('phase_coherence', self.quantum_state.minimum_coherence),
#                 metrics.get('normalized_entropy', 0.0)
#             )
# 
#             # Check if expulsion is needed
#             if self.surplus_dynamics.check_expulsion_needed(self.distinction_level):
#                 expelled, magnitude = self.surplus_dynamics.perform_expulsion(self.quantum_state)
#                 print(f"Performed surplus expulsion with magnitude {magnitude:.4f}")
# 
#             # Process recovery if active
#             self.surplus_dynamics.process_recovery(self.quantum_state, self.distinction_level)
# 
#         except Exception as e:
#             print(f"Error updating distinction and surplus: {e}")
#             traceback.print_exc()
# 
#     def update_state(self):
#         """Update the quantum state and store the current distinction."""
#         try:
#             self.quantum_state.update_state()
#             self.distinction_level = self.compute_distinction()
#             self._update_quantum_metrics()
# 
#             # Store the current state in memory
#             self.memory.store(
#                 self.quantum_state.phase_coherence,
#                 self.distinction_level,
#                 self.surplus_dynamics.surplus_state.copy()
#             )
# 
#             # Track distinction history
#             self.distinction_history.append(self.distinction_level)
# 
#         except Exception as e:
#             print(f"Error updating state: {e}")
#             traceback.print_exc()
# 
#     def get_state_summary(self) -> Dict[str, Any]:
#         """
#         Get comprehensive state summary with proper integration of all components.
# 
#         Returns:
#             Dictionary containing current state metrics
#         """
#         try:
#             # Get basic metrics
#             metrics = self.quantum_state.get_quantum_metrics()
#             distinction_mean = np.mean(list(self.distinction_history)) if self.distinction_history else self.distinction_level
# 
#             # Create core state summary
#             summary = {
#                 'distinction_level': self.distinction_level,
#                 'distinction_mean': distinction_mean,
#                 'stability_factor': self.stability_factor,
#                 'adaptation_momentum': self.adaptation_momentum,
#                 'collapse_prevention_active': self.collapse_prevention_active,
#                 'recovery_mode': self.recovery_mode,
#                 'quantum_metrics': metrics,
#                 'phase': self.phase,
#                 'coherence': metrics.get('phase_coherence', MINIMUM_COHERENCE_FLOOR)
#             }
# 
#             # Add surplus state if available
#             if hasattr(self, 'surplus_dynamics') and hasattr(self.surplus_dynamics, 'surplus_state'):
#                 summary['surplus'] = self.surplus_dynamics.surplus_state.copy()
# 
#             # Add cognitive state if available
#             if hasattr(self, 'recursive_cognition'):
#                 summary['cognitive_state'] = self.recursive_cognition.get_cognitive_state()
# 
#             # Add training summary if available
#             if hasattr(self, 'training_pipeline'):
#                 summary['training_summary'] = self.training_pipeline.get_training_summary()
# 
#             # Add learning rate if available
#             if hasattr(self, 'learning_rate'):
#                 summary['learning_rate'] = self.learning_rate
# 
#             # Add emergent potential field data if available
#             if hasattr(self, 'emergent_potential_field'):
#                 field_state = self.emergent_potential_field.get_field_state()
#                 summary['emergent_potential'] = {
#                     'total_potential': field_state.get('total_potential', 0.0),
#                     'emergence_probability': field_state.get('emergence_probability', 0.0),
#                     'emergence_active': field_state.get('emergence_active', False),
#                     'field_intensity': field_state.get('field_intensity', 1.0)
#                 }
# 
#             return summary
#         except Exception as e:
#             print(f"Error getting state summary: {e}")
#             # Return minimal state information
#             return {
#                 'distinction_level': getattr(self, 'distinction_level', 0.5),
#                 'error': str(e)
#             }
# 
#     def ensure_minimum_mutation(self):
#         """Ensure the quantum state undergoes at least some mutation to prevent stagnation."""
#         try:
#             # Apply a very small random rotation to all qubits
#             for qubit in range(self.num_qubits):
#                 # Vary the rotation angle for each qubit
#                 random_angle = (0.01 + 0.05 * random.random()) * np.pi
#                 # Randomly choose rotation axis (rx, ry, or rz)
#                 rotation_choice = random.choice(['rx', 'ry', 'rz'])
# 
#                 if rotation_choice == 'rx':
#                     self.quantum_state.apply_gate('rx', [qubit], {'theta': random_angle})
#                 elif rotation_choice == 'ry':
#                     # Use rz with equivalent params if ry not available
#                     self.quantum_state.apply_gate('rz', [qubit], {'phi': random_angle})
#                 else:
#                     self.quantum_state.apply_gate('rz', [qubit], {'phi': random_angle})
# 
#             # Add a small amount of phase shift that varies each time
#             phase_shift = 0.02 * np.pi * (random.random() - 0.5)
#             self.quantum_state.apply_phase_shift(phase_shift)
# 
#             # Update state
#             self.quantum_state.update_state()
# 
#             # Log the mutation
#             if hasattr(self, 'logger'):
#                 self.logger.debug(f"Applied minimum quantum mutation with phase shift: {phase_shift:.4f}")
#         except Exception as e:
#             if hasattr(self, 'logger'):
#                 self.logger.error(f"Error ensuring minimum mutation: {e}")
#             else:
#                 print(f"Error ensuring minimum mutation: {e}")
# 
# class EnhancedSingleAgentWithPrediction(EnhancedSingleAgentBase):
#     """
#     Extends the base agent with a transformer-based decision network.
#     The transformer predicts future distinction levels based on current quantum and surplus metrics.
#     """
#     def __init__(self, num_qubits: int = NUM_QUBITS_PER_AGENT):
#         super().__init__(num_qubits)
# 
#         # Initialize memory with hierarchical levels
#         self.memory = RecursiveDistinctionMemory(max_size=10000, hierarchy_levels=4)
# 
#         # Initialize transformer - ensure d_model matches input_size
#         self.transformer = RecursiveDistinctionTransformer(
#             input_size=20,
#             d_model=20,  # Match input_size
#             nhead=NUM_TRANSFORMER_HEADS,
#             num_layers=NUM_TRANSFORMER_LAYERS
#         ).to(DEVICE)
# 
#         # Initialize optimizer and loss criterion
#         self.optimizer = optim.Adam(self.transformer.parameters(), lr=LEARNING_RATE)
#         self.loss_criterion = nn.MSELoss()
# 
#         # Experience buffer for training
#         self.experience_buffer = deque(maxlen=1000)
# 
#         # Initialize training pipeline
#         self.training_pipeline = EnhancedTrainingPipeline(self.transformer)
# 
#         # Set minimum coherence
#         self.minimum_coherence = MINIMUM_COHERENCE_FLOOR
# 
#     def _initialize_transformer(self) -> nn.Module:
#         """Initialize transformer with 4D input adapter."""
#         print("Initializing transformer with 4D input adapter...")
# 
#         # Create base transformer
#         base_transformer = RecursiveDistinctionTransformer(
#             input_size=20,
#             d_model=20,
#             nhead=NUM_TRANSFORMER_HEADS,
#             num_layers=NUM_TRANSFORMER_LAYERS,
#             output_size=1
#         ).to(DEVICE)
# 
#         # Initialize weights
#         for p in base_transformer.parameters():
#             if p.dim() > 1:
#                 nn.init.xavier_uniform_(p, gain=0.1)
# 
#         # Wrap with adapter
#         transformer = FourDimTransformerAdapter(base_transformer, merge_strategy="merge")
#         print("Transformer initialized with adaptive 4D handling.")
#         return transformer
# 
#     def prepare_transformer_input(self) -> torch.Tensor:
#         """
#         Prepare transformer input with improved emergence detection.
#         """
#         try:
#             # Get current quantum metrics
#             metrics = self.quantum_state.get_quantum_metrics()
#             if not isinstance(metrics, dict):
#                 self.logger.warning("Invalid metrics returned")
#                 return torch.zeros((1, 1, 20), dtype=torch.float32, device=DEVICE)
# 
#             # Get distinction momentum safely
#             distinction_momentum = float(getattr(self.distinction_dynamics, 'adjustment_momentum', 0.0))
# 
#             # Get stability factor safely
#             distinction_stability = float(getattr(self.distinction_dynamics, 'stability_factor', 1.0))
# 
#             # Validate surplus state
#             if not hasattr(self.surplus_dynamics, 'surplus_state') or not isinstance(self.surplus_dynamics.surplus_state, SurplusState):
#                 self.logger.warning("Warning: Invalid surplus state detected; reinitializing.")
#                 self.surplus_dynamics.surplus_state = SurplusState()
# 
#             # Ensure distinction_level exists
#             if not hasattr(self, 'distinction_level') or self.distinction_level is None:
#                 self.distinction_level = 0.5
# 
#             # Build the feature vector - exactly 20 features
#             features = [
#                 float(metrics.get('phase_coherence', 0.0)),
#                 float(metrics.get('normalized_entropy', 0.0)),
#                 float(metrics.get('phase_stability', 0.0)),
#                 float(metrics.get('phase_distinction', 0.0)),
#                 float(self.distinction_level),
#                 distinction_momentum,
#                 distinction_stability,
#                 float(getattr(self.distinction_dynamics, 'quantum_influence', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.values.get('basal', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.values.get('cognitive', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.values.get('predictive', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.values.get('ontological', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.stability),
#                 float(self.surplus_dynamics.surplus_state.quantum_coupling),
#                 float(self.surplus_dynamics.surplus_state.accumulation_rate.get('basal', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.accumulation_rate.get('cognitive', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.accumulation_rate.get('predictive', 0.0)),
#                 float(self.surplus_dynamics.surplus_state.accumulation_rate.get('ontological', 0.0)),
#                 float(getattr(self, 'learning_rate', 0.0)),
#                 float(self.ontological_field.resistance(self.distinction_level))
#             ]
# 
#             # Create tensor with shape [1, 1, 20]
#             input_tensor = torch.tensor(features, dtype=torch.float32, device=DEVICE)
#             input_tensor = input_tensor.view(1, 1, -1)
# 
#             # Ensure the last dimension is exactly 20
#             if input_tensor.shape[-1] < 20:
#                 padding = (0, 20 - input_tensor.shape[-1])
#                 input_tensor = F.pad(input_tensor, padding)
#             elif input_tensor.shape[-1] > 20:
#                 input_tensor = input_tensor[:, :, :20]
# 
#             # Create emergence if conditions are right
#             emergence_probability = 0.0
# 
#             # Increase emergence probability based on metrics
#             if self.distinction_level > 0.8:
#                 emergence_probability += 0.2
#             if metrics.get('phase_coherence', 0.0) > 0.8:
#                 emergence_probability += 0.2
#             if metrics.get('normalized_entropy', 0.0) < 0.2:
#                 emergence_probability += 0.1
#             if self.surplus_dynamics.surplus_state.total_surplus() > 5.0:
#                 emergence_probability += 0.2
# 
#             # Check for cognitive complexity
#             if hasattr(self, 'recursive_cognition'):
#                 cognitive_state = self.recursive_cognition.get_cognitive_state()
#                 if cognitive_state.get('mean_strength', 0.0) > 1.5:
#                     emergence_probability += 0.1
#                 if cognitive_state.get('mean_stability', 0.0) > 0.8:
#                     emergence_probability += 0.1
# 
#             # Check for periodic emergence patterns at certain steps
#             step_counter = getattr(self, 'step_counter', 0)
#             if step_counter > 0 and step_counter % 100 == 0:
#                 emergence_probability += 0.1
# 
#             # Apply emergence if probability threshold met
#             if random.random() < emergence_probability and not getattr(self, 'dimension_increase_detected', False):
#                 # Create a 4D tensor by adding an extra dimension
#                 # This simulates the emergence of a new dimension in the system
#                 expanded_tensor = input_tensor.unsqueeze(1)  # Shape becomes [1, 1, 1, 20]
# 
#                 # Duplicate along the emergent dimension to create meaningful structure
#                 expanded_tensor = expanded_tensor.repeat(1, 4, 1, 1)  # Shape becomes [1, 4, 1, 20]
# 
#                 # Handle the emergent dimension
#                 self.handle_emergent_dimension(
#                     tensor_shape=expanded_tensor.shape,
#                     source="prepare_transformer_input"
#                 )
# 
#                 # Return the emergent tensor
#                 return expanded_tensor
# 
#             # Adapt the tensor shape if needed (standard case)
#             return adapt_tensor_shape(input_tensor, expected_dim=3, expected_last_dim=20)
# 
#         except Exception as e:
#             self.logger.error(f"Error preparing transformer input: {e}")
#             traceback.print_exc()
#             return torch.zeros((1, 1, 20), dtype=torch.float32, device=DEVICE)
# 
#     def optimize_decision_network(self):
#         """
#         Optimizes the decision network using recent experiences.
#         Prepares input-target pairs, computes loss, backpropagates, and updates the transformer.
#         Also processes a separate training step from the experience buffer.
#         """
#         if len(self.memory.memory[0]) < 15:
#             return
# 
#         try:
#             input_data = []
#             targets = []
# 
#             # Get recent states from memory
#             past_states = self.memory.retrieve_recent(10, level=2)
# 
#             for i in range(len(past_states) - 1):
#                 # Prepare input
#                 state_input = self.prepare_transformer_input()
#                 input_data.append(state_input.squeeze(0).cpu().numpy())
# 
#                 # Prepare target
#                 metrics = self.quantum_state.get_quantum_metrics()
#                 field_resistance = self.ontological_field.resistance(self.distinction_level)
#                 target_dist = self.distinction_dynamics.compute_distinction(
#                     metrics, field_resistance, self.surplus_dynamics.surplus_state
#                 )
#                 targets.append([target_dist])
# 
#             if not input_data:
#                 return
# 
#             # Convert to tensors
#             input_tensor = torch.from_numpy(np.array(input_data)).float().to(DEVICE)
#             target_tensor = torch.from_numpy(np.array(targets)).float().to(DEVICE)
# 
#             # Ensure correct shapes
#             if input_tensor.dim() == 2:
#                 input_tensor = input_tensor.unsqueeze(1)  # Add sequence length dimension
#             if target_tensor.dim() == 1:
#                 target_tensor = target_tensor.unsqueeze(1)
# 
#             # Forward pass
#             predictions = self.transformer(input_tensor)
#             if hasattr(predictions, "prediction"):
#                 predictions = predictions.prediction
#             else:
#                 print("Warning: Transformer output does not have 'prediction' attribute. Using zeros.")
#                 predictions = torch.zeros_like(target_tensor).to(DEVICE)
# 
#             predictions = predictions.view_as(target_tensor)
# 
#             # Compute loss
#             distinction_loss = self.loss_criterion(predictions, target_tensor)
#             stability_penalty = -0.01 * self.surplus_dynamics.surplus_state.stability
#             quantum_reg = -0.01 * self.distinction_dynamics.quantum_influence
#             total_loss = distinction_loss + stability_penalty + quantum_reg
# 
#             # Backpropagation
#             self.optimizer.zero_grad()
#             total_loss.backward()
#             torch.nn.utils.clip_grad_norm_(self.transformer.parameters(), 1.0)
#             self.optimizer.step()
# 
#             # Adjust learning rate based on prediction error
#             avg_pred = predictions.mean().item()
#             avg_target = target_tensor.mean().item()
#             prediction_error = abs(avg_pred - avg_target)
#             if prediction_error > 0.2:
#                 self.learning_rate *= 1.05
#             else:
#                 self.learning_rate *= 0.95
#             self.learning_rate = np.clip(self.learning_rate, 1e-5, 0.01)
# 
#         except Exception as e:
#             print(f"Error in optimize_decision_network: {e}")
#             traceback.print_exc()
# 
#         # Process additional training step if enough experiences
#         if len(self.experience_buffer) < self.training_pipeline.batch_size:
#             return
# 
#         try:
#             # Get metrics for training
#             metrics = self.quantum_state.get_quantum_metrics()
#             metrics.update({
#                 'stability': self.surplus_dynamics.surplus_state.stability,
#                 'quantum_coupling': self.surplus_dynamics.surplus_state.quantum_coupling
#             })
# 
#             # Sample batch and train
#             batch = random.sample(self.experience_buffer, self.training_pipeline.batch_size)
#             loss_components = self.training_pipeline.train_step(batch, metrics)
# 
#             # Log training metrics
#             if loss_components:
#                 print("\nTraining metrics:")
#                 for key, value in loss_components.items():
#                     print(f"{key}: {value:.4f}")
# 
#         except Exception as e:
#             print(f"Error in training step: {e}")
#             traceback.print_exc()
# 
# class EnhancedSingleAgentFinalEvolution(EnhancedSingleAgentWithPrediction):
#     """
#     Final enhanced agent class that integrates quantum state management,
#     recursive memory, transformer-based decision making, surplus regulation,
#     and advanced adaptation mechanisms.
#     """
#     def __init__(self, agent=None, num_qubits=NUM_QUBITS_PER_AGENT, **kwargs):
#         """Initialize the final evolution agent with enhanced error handling and integration.
# 
#         Args:
#             agent: Optional reference to parent agent
#             num_qubits: Number of qubits to use
#             **kwargs: Additional configuration parameters including:
#                 - expression_threshold: Threshold for significant distinction change (default: 0.1)
#                 - expression_cooldown_period: Number of steps to wait between expressions (default: 10)
#                 - expression_periodic_interval: Steps between periodic expressions (default: 100)
#                 - symbolic_history_size: Size of symbolic expression history (default: 100)
#         """
#         # Import required modules
#         import random
#         import time
#         import traceback
#         import logging
#         import numpy as np
#         import torch
# 
#         # Initialize logging first to capture all initialization messages
#         self.logger = setup_logging()
#         self.num_qubits = NUM_QUBITS_PER_AGENT
#         try:
#             # Store and validate configuration parameters
#             self._initialize_config_parameters(kwargs)
# 
#             # Initialize state tracking variables early
#             self.step_counter = 0
#             self.stability_factor = 1.0
#             self.distinction_level = 0.5
#             self.phase = 0.0
#             self.dimension_increase_detected = False
#             self.recovery_mode = False
#             self.recovery_steps = 0
#             self.collapse_prevention_active = False
#             self.consecutive_failures = 0
#             self.max_failures = 3
#             self.adaptation_momentum = 0.0
#             self.learning_rate = LEARNING_RATE
# 
#             # Pre-initialization check to ensure basic components can be created
#             self.logger.info("🔹 Running pre-initialization checks...")
#             if not self._pre_init_check():
#                 self.logger.error("❌ Pre-initialization checks failed")
#                 raise RuntimeError("Pre-initialization checks failed")
# 
#             # Initialize base class (sets up basic components)
#             self.logger.info("🔹 Initializing base components...")
#             super().__init__(num_qubits)
#             self.agent = agent
# 
#             # Clear redundant initializations from parent class
#             self.transformer = None  # We'll initialize a custom one below
# 
#             # Initialize core quantum components
#             self.logger.info("🔹 Initializing quantum components...")
#             self._initialize_quantum_components()
# 
#             # Initialize cognitive components
#             self.logger.info("🔹 Initializing cognitive components...")
#             self._initialize_cognitive_components()
# 
#             # Initialize memory and field components
#             self.logger.info("🔹 Initializing memory and field components...")
#             self._initialize_memory_components()
#             # Initialize emergent potential field
#             try:
#                 self.emergent_potential_field = EmergentPotentialField()
#                 self.logger.info("✅ Emergent Potential Field initialized")
#             except Exception as e:
#                 self.logger.error(f"❌ Error initializing Emergent Potential Field: {e}")
#                 traceback.print_exc()
# 
#             # Initialize transformer with 4D support
#             self.logger.info("🔹 Initializing transformer...")
#             self.transformer = self._initialize_transformer()
# 
#             # Initialize training components
#             self.logger.info("🔹 Initializing training pipeline...")
#             self._initialize_training_components()
# 
#             # Initialize analysis components
#             self.logger.info("🔹 Initializing analysis components...")
#             self._initialize_analysis_components()
# 
#             # Initialize error recovery and validation
#             self.logger.info("🔹 Initializing error recovery...")
#             self._initialize_error_recovery()
# 
#             # Initialize history tracking
#             self.logger.info("🔹 Setting up history tracking...")
#             self._initialize_history_tracking()
# 
#             # Initialize symbolic output system
#             self.logger.info("🔹 Initializing symbolic output system...")
#             self._initialize_symbolic_system()
# 
#             # Perform initial state synchronization with improved retry logic
#             self.logger.info("🔹 Performing initial state synchronization...")
#             if not self._perform_initial_synchronization():
#                 self.logger.error("❌ Failed to achieve initial state synchronization after recovery")
#                 raise RuntimeError("Failed to achieve initial state synchronization after recovery")
# 
#             # Important: Integrate training with quantum state as final initialization step
#             self.logger.info("🔹 Integrating training pipeline with quantum components...")
#             if not self._integrate_training_with_quantum():
#                 self.logger.warning("⚠️ Training integration incomplete - may affect performance")
#             else:
#                 self.logger.info("✅ Training pipeline successfully integrated with quantum state")
# 
#             # Verify the initial system state
#             self.logger.info("🔹 Verifying initial system state...")
#             if self.verify_system_state():
#                 self.logger.info("✅ Initial system state verification successful")
#             else:
#                 self.logger.warning("⚠️ Initial system state verification failed - some components may be misconfigured")
# 
#         except Exception as e:
#             self.logger.error(f"❌ Error initializing agent: {e}")
#             traceback.print_exc()
#             raise
# 
#     def _initialize_config_parameters(self, kwargs):
#         """Initialize and validate configuration parameters."""
#         # Set default values first
#         self.expression_threshold = 0.1
#         self.expression_cooldown = 0
#         self.expression_cooldown_period = 10
#         self.expression_periodic_interval = 100
#         self.symbolic_history_maxlen = 100
# 
#         # Override with provided values
#         if 'expression_threshold' in kwargs:
#             threshold = kwargs['expression_threshold']
#             if not isinstance(threshold, (int, float)) or threshold <= 0 or threshold >= 1:
#                 self.logger.warning(f"Invalid expression_threshold: {threshold}, using default: 0.1")
#             else:
#                 self.expression_threshold = threshold
# 
#         if 'expression_cooldown_period' in kwargs:
#             period = kwargs['expression_cooldown_period']
#             if not isinstance(period, int) or period < 0:
#                 self.logger.warning(f"Invalid expression_cooldown_period: {period}, using default: 10")
#             else:
#                 self.expression_cooldown_period = period
# 
#         if 'expression_periodic_interval' in kwargs:
#             interval = kwargs['expression_periodic_interval']
#             if not isinstance(interval, int) or interval <= 0:
#                 self.logger.warning(f"Invalid expression_periodic_interval: {interval}, using default: 100")
#             else:
#                 self.expression_periodic_interval = interval
# 
#         if 'symbolic_history_size' in kwargs:
#             size = kwargs['symbolic_history_size']
#             if not isinstance(size, int) or size <= 0:
#                 self.logger.warning(f"Invalid symbolic_history_size: {size}, using default: 100")
#             else:
#                 self.symbolic_history_maxlen = size
# 
#     def _initialize_quantum_components(self):
#         """Initialize quantum state and related components."""
#         # Set up simulator if not already done
#         if not hasattr(self, 'simulator') or self.simulator is None:
#             self.simulator = AerSimulator()
# 
#         # Initialize quantum state parameters
#         if not hasattr(self.quantum_state, 'phase_coherence'):
#             self.quantum_state.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#         # Initialize the quantum circuit
#         self.qc, returned_simulator = _initialize_circuit(self.num_qubits)
# 
#         # Use existing simulator if possible, otherwise use the returned one
#         if not self.simulator:
#             self.simulator = returned_simulator
# 
#         # Initialize statevector
#         self.statevector = Statevector.from_label('0' * self.num_qubits)
# 
#         # Initialize quantum optimizer
#         self.quantum_optimizer = EnhancedQuantumSelfOptimization(self.num_qubits)
# 
#         # Set minimum coherence across components
#         self.minimum_coherence = MINIMUM_COHERENCE_FLOOR
# 
#     def _initialize_cognitive_components(self):
#         """Initialize cognitive structures and dynamics."""
#         # Initialize recursive cognitive structure
#         self.recursive_cognition = RecursiveCognitiveStructuring()
# 
#         # Verify or initialize distinction dynamics
#         if not isinstance(self.distinction_dynamics, EnhancedDistinctionDynamics):
#             self.logger.warning("Reinitializing distinction dynamics")
#             self.distinction_dynamics = EnhancedDistinctionDynamics()
# 
#         # Verify or initialize surplus dynamics and state
#         if not hasattr(self.surplus_dynamics, 'surplus_state') or \
#           not isinstance(self.surplus_dynamics.surplus_state, SurplusState):
#             self.logger.warning("Reinitializing surplus state as SurplusState()")
#             self.surplus_dynamics.surplus_state = SurplusState()
# 
#     def _initialize_memory_components(self):
#         """Initialize memory structures and ontological field."""
#         # Initialize recursive memory with hierarchical levels
#         self.memory = RecursiveDistinctionMemory(max_size=10000, hierarchy_levels=4)
# 
#         # Initialize ontological field
#         self.ontological_field = OntologicalField()
# 
#     def _initialize_training_components(self):
#         """Initialize training pipeline and related components."""
#         # Create training pipeline with the transformer
#         self.training_pipeline = EnhancedTrainingPipeline(self.transformer)
# 
#         # Initialize experience buffer if not present
#         if not hasattr(self, 'experience_buffer'):
#             self.experience_buffer = deque(maxlen=1000)
# 
#     def _initialize_analysis_components(self):
#         """Initialize analysis components with proper initialization of trackers."""
#         try:
#             # Initialize quantum analyzer with properly initialized oscillation detection
#             self.analyzer = QuantumAnalyzer(self.num_qubits)
# 
#             # Manually initialize oscillation detection if missing
#             if not hasattr(self.analyzer, 'oscillation_detection'):
#                 self.analyzer.oscillation_detection = {
#                     'coherence_history': deque(maxlen=100),
#                     'entropy_history': deque(maxlen=100),
#                     'distinction_history': deque(maxlen=100)
#                 }
# 
#             # Initialize phase transition tracking if missing
#             if not hasattr(self.analyzer, 'phase_transitions'):
#                 self.analyzer.phase_transitions = 0
#                 self.analyzer.transition_magnitudes = []
#                 self.analyzer.last_state = None
# 
#             # Initialize optimization coordinator
#             self.optimization_coordinator = OptimizationCoordinator(self)
# 
#             # Initialize emergence tracking
#             self.emergence_tracker = EmergenceTracker()
#             self.dimension_monitor = DimensionMonitor()
# 
#             self.logger.info("Analysis components initialized successfully")
# 
#         except Exception as e:
#             self.logger.error(f"Error initializing analysis components: {e}")
#             traceback.print_exc()
# 
#             # Create minimal functional components even if initialization fails
#             self.analyzer = QuantumAnalyzer(self.num_qubits)
#             self.emergence_tracker = EmergenceTracker()
#             self.dimension_monitor = DimensionMonitor()
# 
#     def _initialize_error_recovery(self):
#         """Initialize error recovery and validation components."""
#         # Initialize error recovery
#         self.error_recovery = EnhancedErrorRecovery(self)
# 
#         # Initialize validation components
#         self.state_validator = QuantumStateValidator()
# 
#         # Initialize synchronization manager
#         self.sync_manager = StateSynchronizationManager(
#             self.quantum_state,
#             self.surplus_dynamics,
#             self.distinction_dynamics
#         )
# 
#     def _initialize_history_tracking(self):
#         """Initialize history tracking structures."""
#         # Initialize main history trackers
#         self.distinction_history = deque(maxlen=1000)
#         self.quantum_metric_history = deque(maxlen=1000)
# 
#         # Initialize analysis history
#         self.analysis_history = {
#             'coherence': deque(maxlen=1000),
#             'distinction': deque(maxlen=1000),
#             'entropy': deque(maxlen=1000),
#             'analysis_results': deque(maxlen=1000)
#         }
# 
#         # Initialize adaptation history
#         self.adaptation_history = deque(maxlen=1000)
# 
#         # Initialize tensor shape history for emergence tracking
#         self.tensor_shape_history = []
# 
#     def _initialize_symbolic_system(self):
#         """Initialize symbolic output system and related components."""
#         # Initialize symbolic output generator
#         self.symbolic_system = SymbolicOutput()
# 
#         # Initialize symbolic history tracker
#         self.symbolic_history = deque(maxlen=self.symbolic_history_maxlen)
# 
#         # Initialize expression-related state
#         self.previous_distinction = self.distinction_level
#         self.last_symbolic_expression = None
# 
#     def _perform_initial_synchronization(self):
#         """Perform initial state synchronization with improved retry logic."""
#         sync_success = False
#         sync_attempts = 0
#         max_sync_attempts = 3
# 
#         while not sync_success and sync_attempts < max_sync_attempts:
#             try:
#                 if self.sync_manager.synchronize_states():
#                     self.logger.info("✅ Initial state synchronization successful")
#                     sync_success = True
#                     break
# 
#                 sync_attempts += 1
#                 self.logger.warning(f"⚠️ Synchronization attempt {sync_attempts} failed, retrying...")
# 
#                 # Try basic recovery between attempts with exponential backoff
#                 self._basic_state_recovery()
#                 time.sleep(0.1 * (2 ** sync_attempts))  # Exponential backoff
# 
#             except Exception as sync_err:
#                 self.logger.error(f"Error during synchronization attempt {sync_attempts}: {sync_err}")
#                 sync_attempts += 1
# 
#         if not sync_success:
#             self.logger.error("Failed to achieve initial state synchronization after multiple attempts")
#             self.logger.info("🔄 Attempting full recovery as last resort...")
#             if not self._attempt_state_recovery():
#                 self.logger.error("Failed to achieve initial state synchronization after recovery")
#                 return False
# 
#             self.logger.info("✅ Full recovery successful, attempting final synchronization")
# 
#             # Final synchronization attempt after recovery
#             try:
#                 if self.sync_manager.synchronize_states():
#                     self.logger.info("✅ Post-recovery synchronization successful")
#                     return True
#                 else:
#                     self.logger.error("Failed post-recovery synchronization")
#                     return False
#             except Exception as e:
#                 self.logger.error(f"Error in post-recovery synchronization: {e}")
#                 return False
# 
#         return sync_success
# 
#     def _pre_init_check(self) -> bool:
#         """Perform pre-initialization validation and setup."""
#         try:
#             # Verify quantum components
#             self.quantum_state = EnhancedQuantumState(num_qubits=self.num_qubits)
#             if not hasattr(self.quantum_state, 'phase_coherence'):
#                 print("Setting initial phase coherence...")
#                 self.quantum_state.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#             # Initialize and verify surplus state
#             if not self._initialize_surplus_state():
#                 print("Failed to initialize surplus state")
#                 return False
# 
#             # Initialize base metrics with error handling
#             try:
#                 metrics = self.quantum_state.get_quantum_metrics()
#                 if not metrics.get('phase_coherence'):
#                     print("Setting initial metrics...")
#                     self.quantum_state.update_phase_coherence()
#             except Exception as metrics_error:
#                 print(f"Error getting initial metrics: {metrics_error}")
#                 # Don't fail on metrics error, continue initialization
#                 pass
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error in pre-initialization check: {e}")
#             traceback.print_exc()
#             return False
# 
#     def verify_system_state(self) -> bool:
#         """
#         Verify that all system components are in a consistent state.
#         Returns True if all checks pass, False otherwise.
#         """
#         try:
#             verification_results = {}
# 
#             # 1. Check quantum state
#             verification_results['quantum_state'] = self.state_validator.validate_quantum_state(self.quantum_state)
# 
#             # 2. Check surplus state
#             verification_results['surplus_state'] = (
#                 isinstance(self.surplus_dynamics.surplus_state, SurplusState) and
#                 self.surplus_dynamics.surplus_state.validate()
#             )
# 
#             # 3. Check distinction dynamics
#             verification_results['distinction'] = (
#                 hasattr(self.distinction_dynamics, 'distinction_level') and
#                 0 <= self.distinction_level <= 1.0
#             )
# 
#             # 4. Check transformer
#             verification_results['transformer'] = (
#                 self.transformer is not None and
#                 isinstance(self.transformer, nn.Module)
#             )
# 
#             # 5. Check training pipeline
#             verification_results['training_pipeline'] = (
#                 hasattr(self, 'training_pipeline') and
#                 hasattr(self.training_pipeline, 'optimizer')
#             )
# 
#             # Check overall verification results
#             all_verified = all(verification_results.values())
#             if not all_verified:
#                 self.logger.warning("System state verification failed:")
#                 for component, status in verification_results.items():
#                     if not status:
#                         self.logger.warning(f"  - {component}: FAILED")
# 
#             return all_verified
# 
#         except Exception as e:
#             self.logger.error(f"Error verifying system state: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _initialize_surplus_state(self) -> bool:
#         """Initialize surplus state with proper validation."""
#         try:
#             # Import random explicitly here to ensure it's available
#             import random
# 
#             if not hasattr(self, 'surplus_dynamics'):
#                 self.surplus_dynamics = EnhancedSurplusDynamics()
#                 print(f"DEBUG: Surplus Dynamics Initialized - Surplus State: {self.surplus_dynamics.surplus_state}")
# 
#             # Ensure proper SurplusState initialization
#             if not isinstance(self.surplus_dynamics.surplus_state, SurplusState) or self.surplus_dynamics.surplus_state is None:
#                 print(f"Warning: Reinitializing surplus state as SurplusState()")
#                 self.surplus_dynamics.surplus_state = SurplusState()
# 
#             # Initialize with slightly different starting values to break symmetry
#             self.surplus_dynamics.surplus_state.values = {
#                 'basal': 1.0,
#                 'cognitive': 1.1,
#                 'predictive': 0.9,
#                 'ontological': 1.05
#             }
# 
#             # Initialize accumulation rates with more variation
#             self.surplus_dynamics.surplus_state.accumulation_rate = {
#                 'basal': 0.01,
#                 'cognitive': 0.02,
#                 'predictive': 0.015,
#                 'ontological': 0.005
#             }
# 
#             # Explicitly initialize accumulation momentum with different values
#             self.surplus_dynamics.accumulation_momentum = {
#                 'basal': 0.01,
#                 'cognitive': 0.0,
#                 'predictive': -0.01,
#                 'ontological': 0.005
#             }
# 
#             # Set initial stability
#             self.surplus_dynamics.surplus_state.stability = 1.0
#             self.surplus_dynamics.surplus_state.quantum_coupling = 1.0
# 
#             # Ensure the surplus update method will use these values
#             # by initializing key tracking variables
#             if hasattr(self.surplus_dynamics, 'track_emergence'):
#                 self.surplus_dynamics.track_emergence(self.surplus_dynamics.surplus_state.values)
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error initializing surplus state: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _basic_state_recovery(self):
#         """Perform basic state recovery between synchronization attempts."""
#         try:
#             self.logger.info("Performing basic state recovery...")
# 
#             # Reset quantum state phase and coherence
#             self.quantum_state.phase = 0.0
#             self.quantum_state.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#             # Apply a simple quantum gate to refresh state
#             self.quantum_state.apply_gate('h', [0])
# 
#             # Ensure surplus state is valid
#             if not isinstance(self.surplus_dynamics.surplus_state, SurplusState):
#                 self.surplus_dynamics.surplus_state = SurplusState()
# 
#             # Reset distinction level
#             self.distinction_level = 0.5
# 
#             return True
#         except Exception as e:
#             self.logger.error(f"Error in basic state recovery: {e}")
#             return False
# 
#     def _attempt_state_recovery(self) -> bool:
#         """Enhanced state recovery with better integration."""
#         try:
#             self.logger.info("\n🔄 Starting comprehensive state recovery process...")
#             recovery_successful = False
# 
#             # First, validate the current system state
#             validation_results = self.state_validator.validate_system_state(self)
# 
#             # Log validation results
#             self.logger.info("System validation results:")
#             for component, status in validation_results.items():
#                 if component != 'overall':
#                     self.logger.info(f"  - {component}: {'PASSED' if status else 'FAILED'}")
# 
#             # If overall validation passed, no need for recovery
#             if validation_results.get('overall', False):
#                 self.logger.info("✅ System validation passed, no recovery needed")
#                 return True
# 
#             # Use error recovery to perform component or full recovery
#             if hasattr(self, 'error_recovery'):
#                 # Get list of failed components
#                 failed_components = [comp for comp, status in validation_results.items()
#                                     if not status and comp != 'overall']
# 
#                 self.logger.info(f"Initiating recovery for failed components: {failed_components}")
# 
#                 # Check if critical components have failed
#                 critical_failure = any(comp in ['quantum_state', 'surplus_state', 'distinction']
#                                       for comp in failed_components)
# 
#                 if critical_failure or len(failed_components) > 2:
#                     self.logger.warning("Critical failure detected, performing full recovery")
#                     recovery_successful = self.error_recovery.initiate_full_recovery()
#                 else:
#                     # Recover each failed component
#                     recovery_successful = True
#                     for component in failed_components:
#                         if not self.error_recovery.initiate_component_recovery(component):
#                             recovery_successful = False
#                             self.logger.error(f"Failed to recover component: {component}")
#             else:
#                 # Fallback to basic recovery if error_recovery not available
#                 self.logger.warning("Error recovery not initialized, falling back to basic recovery")
#                 recovery_successful = self._basic_state_recovery()
# 
#             # Verify recovery was successful
#             if recovery_successful:
#                 self.logger.info("✅ State recovery completed successfully")
#                 # Reset recovery-related counters
#                 self.consecutive_failures = 0
#                 if hasattr(self, 'recovery_mode'):
#                     self.recovery_mode = False
#             else:
#                 self.logger.error("❌ State recovery failed")
# 
#             return recovery_successful
# 
#         except Exception as e:
#             self.logger.error(f"Error in state recovery attempt: {e}")
#             traceback.print_exc()
#             return False
# 
#     def handle_emergent_dimension(self, tensor_shape: Tuple, source: str) -> None:
#         """
#         Handle detection of emergent dimensions with improved integration.
# 
#         Args:
#             tensor_shape: The tensor shape where emergence was detected
#             source: Source of the emergence detection
#         """
#         try:
#             self.logger.info(f"\n🌟 EMERGENT DIMENSION DETECTED: {len(tensor_shape)}D tensor")
#             self.logger.info(f"Shape: {tensor_shape}, Source: {source}")
# 
#             # Store previous state for comparison
#             previous_distinction = self.distinction_level
#             previous_coherence = self.quantum_state.phase_coherence
# 
#             # Mark emergence detected for other systems to respond
#             self.dimension_increase_detected = True
#             self.emergence_tracker = getattr(self, 'emergence_tracker', EmergenceTracker())
# 
#             # Record emergence in tracker
#             metrics = self.quantum_state.get_quantum_metrics()
#             resource_usage = {
#                 'cpu_percent': 0,  # Would come from monitoring in real implementation
#                 'memory_percent': 0  # Would come from monitoring in real implementation
#             }
#             self.emergence_tracker.record_emergence(
#                 tensor_shape=tensor_shape,
#                 timestamp=time.time(),
#                 resource_usage=resource_usage,
#                 agent_metrics=metrics
#             )
# 
#             # Connect with emergent potential field if available
#             if hasattr(self, 'emergent_potential_field'):
#                 # Register emergence with the field for future correlations
#                 self.emergent_potential_field.register_potential(
#                     component_id=f"emergence_{source}",
#                     potential=0.5,  # Base potential for dimensional emergence
#                     component_type='quantum',
#                     state_metrics={
#                         'tensor_shape': tensor_shape,
#                         'dimensionality': len(tensor_shape),
#                         'source': source,
#                         'distinction': self.distinction_level,
#                         'coherence': metrics.get('phase_coherence', 0.5)
#                     }
#                 )
# 
#             # Generate symbolic expression for the emergence event
#             if hasattr(self, 'symbolic_system'):
#                 surplus = self.surplus_dynamics.surplus_state.total_surplus()
#                 distinction = self.distinction_level
#                 coherence = metrics.get('phase_coherence', 0.5)
#                 entropy = metrics.get('normalized_entropy', 0.5)
# 
#                 expression = self.symbolic_system.handle_post_emergence(
#                     surplus=surplus,
#                     distinction=distinction,
#                     coherence=coherence,
#                     dimensionality=len(tensor_shape),
#                     entropy=entropy
#                 )
# 
#                 self.logger.info(f"\n🔹 Emergent Dimension Symbolic Output: {expression}\n")
# 
#                 # Store the expression with additional context
#                 if hasattr(self, 'symbolic_history'):
#                     self.symbolic_history.append({
#                         'expression': expression,
#                         'type': 'emergence',
#                         'step': getattr(self, 'step_counter', 0),
#                         'distinction': distinction,
#                         'distinction_delta': distinction - previous_distinction,
#                         'coherence': coherence,
#                         'coherence_delta': coherence - previous_coherence,
#                         'entropy': entropy,
#                         'dimensionality': len(tensor_shape),
#                         'tensor_shape': tensor_shape,
#                         'timestamp': time.time()
#                     })
# 
#             # Enhance the quantum state in response to emergence
#             self._enhance_quantum_state_for_emergence()
# 
#             # Notify cognitive structures of emergence
#             if hasattr(self, 'recursive_cognition'):
#                 cognitive_state = self.recursive_cognition.get_cognitive_state()
#                 self.recursive_cognition.update(
#                     phase_coherence=metrics['phase_coherence'],
#                     distinction_level=self.distinction_level,
#                     surplus=self.surplus_dynamics.surplus_state.values,
#                     prediction_error=0.0,  # No prediction error for emergence
#                     quantum_metrics=metrics
#                 )
# 
#             # Track emergent tensor shapes
#             self.tensor_shape_history = getattr(self, 'tensor_shape_history', [])
#             self.tensor_shape_history.append({
#                 'shape': tensor_shape,
#                 'dimensionality': len(tensor_shape),
#                 'timestamp': time.time(),
#                 'source': source
#             })
# 
#         except Exception as e:
#             self.logger.error(f"Error handling emergent dimension: {e}")
#             traceback.print_exc()
# 
#     def dump_emergent_field_data(self, filepath: str = "emergent_field_data.json") -> bool:
#         """
#         Save emergent potential field data to a JSON file for external analysis.
# 
#         Args:
#             filepath: Path to save the JSON file
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             if not hasattr(self, 'emergent_potential_field'):
#                 self.logger.error("No emergent potential field to dump data from")
#                 return False
# 
#             import json
# 
#             # Prepare data structure
#             field_data = {
#                 'field_state': self.emergent_potential_field.get_field_state(),
#                 'potential_history': [
#                     {
#                         'timestamp': entry['timestamp'],
#                         'total_potential': entry['total_potential'],
#                         'threshold': entry['threshold']
#                     }
#                     for entry in list(self.emergent_potential_field.potential_history)
#                 ],
#                 'emergence_history': [
#                     {
#                         'timestamp': event['timestamp'],
#                         'potential': event['potential'],
#                         'intensity': event['intensity'],
#                         'threshold': event['threshold'],
#                         'sequence': event['sequence_number']
#                     }
#                     for event in list(self.emergent_potential_field.emergence_history)
#                 ],
#                 'component_weights': self.emergent_potential_field.component_weights
#             }
# 
#             # Save to file
#             with open(filepath, 'w') as f:
#                 json.dump(field_data, f, indent=2)
# 
#             self.logger.info(f"Emergent field data saved to {filepath}")
#             return True
# 
#         except Exception as e:
#             self.logger.error(f"Error dumping emergent field data: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _enhance_quantum_state_for_emergence(self) -> None:
#         """Enhance quantum state in response to emergence detection."""
#         try:
#             # Apply a phase shift to enhance coherence
#             self.quantum_state.apply_phase_shift(0.2 * np.pi)
# 
#             # Apply Hadamard gates to key qubits to increase superposition
#             self.quantum_state.apply_gate('h', [0, 1])
# 
#             # Reinforce coherence to stabilize the emerged state
#             self.quantum_optimizer.reinforce_coherence(
#                 self.quantum_state.qc,
#                 0.1,  # Low distinction variance for stability
#                 self.quantum_state.phase_coherence
#             )
# 
#             # Update quantum state
#             self.quantum_state.update_state()
#             self._update_quantum_metrics()
# 
#             self.logger.info("✅ Quantum state enhanced for emergence")
# 
#         except Exception as e:
#             self.logger.error(f"Error enhancing quantum state for emergence: {e}")
# 
#     def adapt(self):
#         """
#         Main adaptation pipeline with enhanced stability and collapse prevention.
#         """
#         try:
#             # Get current state metrics
#             metrics = self.quantum_state.get_quantum_metrics()
# 
#             # Check for potential collapse
#             collapse_prob = self.recursive_cognition.predict_cognitive_collapse()
# 
#             # Activate collapse prevention if needed
#             if collapse_prob > COLLAPSE_DISSIPATION_THRESHOLD and not self.collapse_prevention_active:
#                 print(f"⚠️ High collapse probability detected: {collapse_prob:.4f}")
#                 self.activate_collapse_prevention()
# 
#             # Update recursive cognitive structure - FIXED: pass full metrics dictionary
#             self.recursive_cognition.update(
#                 phase_coherence=metrics['phase_coherence'],
#                 distinction_level=self.distinction_level,
#                 surplus=self.surplus_dynamics.surplus_state.values,
#                 prediction_error=self.training_pipeline.get_training_summary().get(
#                     'avg_distinction_loss', 0.0
#                 ),
#                 quantum_metrics=metrics  # Pass the complete metrics dictionary
#             )
# 
#             # Get cognitive state for optimization decisions
#             cognitive_state = self.recursive_cognition.get_cognitive_state()
# 
#             # Optimize quantum state if not in recovery
#             if not self.recovery_mode:
#                 self.quantum_optimizer.optimize_quantum_state(
#                     self.quantum_state,
#                     self.distinction_level,
#                     cognitive_state
#                 )
# 
#             # Update distinction and surplus
#             self.update_distinction_and_surplus()
# 
#             # Apply quantum feedback refinement if not in recovery
#             if not self.recovery_mode:
#                 self.quantum_feedback_refinement()
#                 self.reinforce_quantum_coherence()
# 
#             # Optimize decision network
#             self.optimize_decision_network()
# 
#             # Apply meta-adaptation if stable
#             if self.stability_factor > 0.7:
#                 self.recursive_meta_adaptation()
# 
#             # Update state and metrics
#             self.update_state()
# 
#             # Track adaptation
#             self._track_adaptation()
# 
#         except Exception as e:
#             print(f"Error in adaptation pipeline: {e}")
#             traceback.print_exc()
#             if not self.recovery_mode:
#                 self.enter_recovery_mode()
# 
#     def activate_collapse_prevention(self):
#         """Activate collapse prevention mechanisms."""
#         try:
#             self.collapse_prevention_active = True
# 
#             # Recycle surplus for stability
#             recycled_surplus = self.recursive_cognition.dissipate_collapse(
#                 self.surplus_dynamics.surplus_state.values
#             )
# 
#             # Use recycled surplus to boost stability
#             if recycled_surplus:
#                 self.stability_factor *= (1.0 + sum(recycled_surplus.values()) * 0.1)
# 
#             # Apply corrective quantum operations
#             self.quantum_state.apply_phase_shift(0.1 * np.pi)
# 
#             # Increase coherence preservation
#             self.quantum_optimizer.reinforce_coherence(
#                 self.quantum_state.qc,
#                 self.distinction_dynamics.get_distinction_metrics()['stability_factor'],
#                 self.quantum_state.phase_coherence
#             )
# 
#             print("Collapse prevention mechanisms activated.")
# 
#         except Exception as e:
#             print(f"Error in collapse prevention: {e}")
#             traceback.print_exc()
# 
#     def compute_distinction(self) -> float:
#         """
#         Compute the agent's distinction level by combining multiple factors.
#         """
#         self.logger.info("Calculating distinction level...")
#         try:
#             metrics = self.quantum_state.get_quantum_metrics()
# 
# 
#             # Calculate coupling and field resistance
#             coupling = self.quantum_state.compute_quantum_surplus_coupling(self.surplus_dynamics.surplus_state.values)
#             field_resistance = self.ontological_field.resistance(self.distinction_level)
# 
#             # Calculate total distinction from multiple components
#             total_distinction = (
#                 0.3 * metrics.get('phase_distinction', 0.5) +
#                 0.3 * (1 - metrics.get('normalized_entropy', 0.5)) +
#                 0.2 * metrics.get('phase_coherence', 0.5) +
#                 0.2 * (1 - field_resistance)
#             )
# 
#             # Scale by quantum coupling
#             total_distinction *= (0.5 + 0.5 * coupling)
# 
#             return float(np.clip(total_distinction, 0, 1))
# 
#         except Exception as e:
#             print(f"Error in computing distinction: {e}")
#             traceback.print_exc()
#             return self.distinction_level
# 
#     def update_distinction_and_surplus(self) -> None:
#         """Update distinction level and adjust surplus values."""
#         try:
#             # Get metrics and calculate field resistance
#             metrics = self.quantum_state.get_quantum_metrics()
#             field_resistance = self.ontological_field.resistance(self.distinction_level)
# 
#             # Update distinction level
#             self.distinction_level = self.distinction_dynamics.compute_distinction(
#                 metrics, field_resistance, self.surplus_dynamics.surplus_state
#             )
# 
#             # Ensure valid surplus state
#             if not isinstance(self.surplus_dynamics.surplus_state, SurplusState):
#                 print("Warning: surplus_state not properly initialized, reinitializing as SurplusState()")
#                 self.surplus_dynamics.surplus_state = SurplusState()
# 
#             # Update surplus based on quantum metrics
#             self.surplus_dynamics.update_surplus(
#                 metrics.get('phase_coherence', self.quantum_state.minimum_coherence),
#                 metrics.get('normalized_entropy', 0.0)
#             )
# 
#             # Check for surplus expulsion
#             if self.surplus_dynamics.check_expulsion_needed(self.distinction_level):
#                 expelled, magnitude = self.surplus_dynamics.perform_expulsion(self.quantum_state)
#                 print(f"Performed surplus expulsion with magnitude {magnitude:.4f}")
# 
#             # Process recovery if active
#             self.surplus_dynamics.process_recovery(self.quantum_state, self.distinction_level)
# 
#         except Exception as e:
#             print(f"Error updating distinction and surplus: {e}")
#             traceback.print_exc()
# 
#     def adjust_surplus(self):
#         """
#         Adjust surplus values based on current metrics with enhanced dynamics
#         to promote emergence.
#         """
#         try:
#             # Get quantum metrics and field resistance
#             metrics = self.quantum_state.get_quantum_metrics()
#             field_resistance = self.ontological_field.resistance(self.distinction_level)
# 
#             # First, update surplus using basic method
#             self.surplus_dynamics.update_surplus(
#                 phase_coherence=metrics.get('phase_coherence', 1.0),
#                 normalized_entropy=metrics.get('normalized_entropy', 0.0)
#             )
# 
#             # Then also call adjust_surplus for additional dynamic behavior
#             self.surplus_dynamics.adjust_surplus(
#                 distinction_level=self.distinction_level,
#                 quantum_metrics=metrics,
#                 field_resistance=field_resistance
#             )
# 
#             # Add additional random variation every few steps to break synchronization
#             if hasattr(self, 'step_counter') and self.step_counter % 5 == 0:
#                 self._add_surplus_variation()
# 
#         except Exception as e:
#             print(f"Error adjusting surplus: {e}")
#             traceback.print_exc()
# 
#     def _add_surplus_variation(self):
#         """
#         Add small random variations to surplus values to break synchronization
#         and promote emergence.
#         """
#         try:
#             if not hasattr(self.surplus_dynamics, 'surplus_state') or \
#               not isinstance(self.surplus_dynamics.surplus_state, SurplusState):
#                 return
# 
#             # Calculate the mean surplus for scaling purposes
#             mean_surplus = sum(self.surplus_dynamics.surplus_state.values.values()) / \
#                           max(1, len(self.surplus_dynamics.surplus_state.values))
# 
#             # Add small random variations, scaled by the mean surplus
#             variation_scale = 0.05 * mean_surplus  # 5% variation
# 
#             for key in self.surplus_dynamics.surplus_state.values:
#                 # Generate random variation, more likely to be positive than negative
#                 variation = variation_scale * (np.random.random() * 2 - 0.8)
# 
#                 # Apply variation
#                 current_value = self.surplus_dynamics.surplus_state.values[key]
#                 new_value = current_value + variation
# 
#                 # Ensure value stays within bounds
#                 self.surplus_dynamics.surplus_state.values[key] = np.clip(
#                     new_value, 0.1, MAX_SURPLUS
#                 )
# 
#             # Log the variation if it's significant
#             if variation_scale > 0.1:
#                 print(f"Added surplus variation of scale {variation_scale:.4f}")
# 
#         except Exception as e:
#             print(f"Error adding surplus variation: {e}")
# 
#     def quantum_feedback_refinement(self):
#         """Apply quantum feedback refinement based on metrics."""
#         try:
#             metrics = self.quantum_state.get_quantum_metrics()
# 
#             # Apply phase shift for high entropy
#             if metrics.get('normalized_entropy', 0.0) > 0.6:
#                 phase_shift = float((1 - float(metrics.get('phase_coherence', 0.5))) * float(np.pi))
#                 phase_shift *= float(1.0 + 0.1 * float(self.adaptation_momentum))
#                 self.quantum_state.apply_phase_shift(phase_shift)
# 
#             # Apply corrective gates for low coherence
#             if metrics.get('phase_coherence', 1.0) < 0.3:
#                 self.quantum_state.apply_gate('x', [0])
#                 self.quantum_state.apply_phase_shift(float(0.1 * np.pi))
# 
#             # Update state and metrics
#             self.quantum_state.update_state()
#             self._update_quantum_metrics()
# 
#         except Exception as e:
#             print(f"Error in quantum feedback refinement: {e}")
#             traceback.print_exc()
# 
#     def _integrate_training_with_quantum(self) -> bool:
#         """
#         Ensure training pipeline is correctly integrated with quantum state for better
#         initialization and gradient flow between quantum and classical components.
# 
#         Returns:
#             bool: True if integration successful, False otherwise
#         """
#         try:
#             self.logger.info("Integrating training pipeline with quantum state...")
# 
#             # Get comprehensive initial metrics from quantum state
#             initial_metrics = self.quantum_state.get_quantum_metrics()
#             distinction_metrics = self.quantum_state.get_quantum_distinction_metrics()
# 
#             # Combine metrics for more comprehensive initialization
#             combined_metrics = {**initial_metrics, **distinction_metrics}
# 
#             # Prepare initial input with proper error handling
#             try:
#                 initial_input = self.prepare_transformer_input()
#                 if not isinstance(initial_input, torch.Tensor):
#                     self.logger.warning("Initial input is not a tensor, creating default")
#                     initial_input = torch.zeros((1, 1, 20), device=DEVICE)
#             except Exception as input_err:
#                 self.logger.error(f"Error preparing initial input: {input_err}")
#                 initial_input = torch.zeros((1, 1, 20), device=DEVICE)
# 
#             # Create multiple initial experiences with slight variations to improve
#             # initial training stability and prevent overfitting to a single point
#             for variation in range(5):
#                 # Add small random variation to create diversity in initial training data
#                 distinction_variation = self.distinction_level * (1.0 + 0.05 * (random.random() - 0.5))
#                 coherence_variation = initial_metrics['phase_coherence'] * (1.0 + 0.05 * (random.random() - 0.5))
# 
#                 # Create varied metrics for each experience
#                 varied_metrics = combined_metrics.copy()
#                 varied_metrics['phase_coherence'] = coherence_variation
#                 varied_metrics['coherence_distinction'] = coherence_variation
# 
#                 # Create varied initial experience
#                 varied_experience = {
#                     'state': initial_input.cpu().numpy() if isinstance(initial_input, torch.Tensor) else np.zeros((1, 1, 20)),
#                     'prediction': float(distinction_variation),
#                     'actual': float(self.distinction_level),
#                     'quantum_metrics': varied_metrics,
#                     'reward': 0.0,  # Initial reward neutral
#                     'stability': float(self.stability_factor),
#                     'adaptation_momentum': float(self.adaptation_momentum),
#                     'next_distinction': float(self.distinction_level)  # For temporal learning
#                 }
# 
#                 # Add initial experience to buffer with appropriate priority
#                 priority = 0.5 if variation == 0 else 0.3  # Higher priority for base experience
#                 self.training_pipeline.add_experience(varied_experience, priority)
# 
#             # Ensure the optimizer is aware of the model's parameters
#             if hasattr(self.training_pipeline, 'optimizer'):
#                 if self.transformer is not None:
#                     self.training_pipeline.optimizer = QuantumAwareOptimizer(self.transformer)
# 
#             # Validate metrics with proper error reporting
#             validated_metrics = self.training_pipeline.metric_validator.validate_metrics(combined_metrics)
#             if len(validated_metrics) < len(combined_metrics):
#                 missing = set(combined_metrics.keys()) - set(validated_metrics.keys())
#                 self.logger.warning(f"Some metrics were not validated: {missing}")
# 
#             # Perform an initial optimization step if we have enough experiences
#             if len(self.training_pipeline.experience_buffer) >= self.training_pipeline.batch_size:
#                 self.logger.info("Performing initial training step to establish gradients")
#                 batch = random.sample(
#                     self.training_pipeline.experience_buffer,
#                     self.training_pipeline.batch_size
#                 )
#                 loss_components = self.training_pipeline.train_step(batch, validated_metrics)
#                 self.logger.info(f"Initial training loss: {loss_components}")
# 
#             # Set up the synchronization manager for training pipeline
#             if hasattr(self, 'sync_manager'):
#                 self.sync_manager.training_pipeline = self.training_pipeline
# 
#             self.logger.info("✅ Quantum-training integration complete")
#             return True
# 
#         except Exception as e:
#             self.logger.error(f"Error integrating training with quantum: {e}")
#             traceback.print_exc()
# 
#             # Try basic recovery to maintain functionality
#             try:
#                 self.training_pipeline.reset_to_baseline = True  # Flag for future reset
#                 self.training_pipeline.add_experience({
#                     'state': np.zeros((1, 1, 20)),
#                     'prediction': 0.5,
#                     'actual': 0.5,
#                     'quantum_metrics': {'phase_coherence': MINIMUM_COHERENCE_FLOOR}
#                 })
#                 self.logger.warning("Applied minimal recovery to training pipeline")
#                 return False
#             except:
#                 self.logger.error("Complete failure in recovery attempt")
#                 return False
# 
#     def optimize_decision_network(self):
#         """Optimize decision network with enhanced stability and proper emergent dimension handling."""
#         try:
#             # Skip optimization if in recovery
#             if self.recovery_mode:
#                 return
# 
#             # Prepare input data
#             input_tensor = self.prepare_transformer_input()
# 
#             # Forward pass
#             output = self.transformer(input_tensor)
#             if not hasattr(output, "prediction") or output.prediction is None:
#                 self.logger.warning("Transformer output is missing prediction attribute")
#                 # Create a default prediction tensor
#                 prediction = torch.tensor([0.5], device=input_tensor.device)
#             else:
#                 prediction = output.prediction  # keep as tensor for gradient flow
# 
#             # Handle multidimensional predictions properly
#             if prediction.numel() == 1:
#                 prediction_value = prediction.item()  # simple scalar case
#             else:
#                 # For emergent dimensions, take the mean to get a single value
#                 prediction_value = prediction.mean().item()
#                 self.logger.info(f"Handling emergent dimension prediction with shape {prediction.shape}")
# 
#             # Apply quantum feedback based on prediction
#             if self.stability_factor > 0.5:
#                 if prediction_value > 0.5:
#                     phase_shift = prediction_value * np.pi
#                     self.quantum_state.apply_phase_shift(phase_shift)
#                 else:
#                     self.quantum_state.apply_gate('x', [0])
#                     self.quantum_state.apply_phase_shift(prediction_value * np.pi / 2)
# 
#             # Update quantum state
#             self.quantum_state.update_state()
#             self._update_quantum_metrics()
# 
#             # Store experience
#             experience = self._prepare_experience(prediction_value)
#             self.training_pipeline.add_experience(experience)
# 
#             # Get cognitive state to adjust training
#             cognitive_state = self.recursive_cognition.get_cognitive_state()
# 
#             # Train if enough experiences
#             if hasattr(self.training_pipeline, 'experience_buffer') and hasattr(self.training_pipeline, 'batch_size'):
#                 if len(self.training_pipeline.experience_buffer) >= self.training_pipeline.batch_size:
#                     metrics = self.quantum_state.get_quantum_metrics()
#                     batch = random.sample(
#                         self.training_pipeline.experience_buffer,
#                         self.training_pipeline.batch_size
#                     )
#                     self.training_pipeline.train_step(batch, metrics)
# 
#                     # Apply cognitive-based training adjustments
#                     if hasattr(self.training_pipeline, 'adjust_training_with_cognitive_state'):
#                         self.training_pipeline.adjust_training_with_cognitive_state(cognitive_state)
# 
#         except Exception as e:
#             self.logger.error(f"Error optimizing decision network: {e}")
#             traceback.print_exc()
# 
#     def _adjust_learning_rates(self):
#         """
#         Adjust learning rates based on current system state.
#         This provides a unified point for all learning rate adjustments.
#         """
#         try:
#             # Get current metrics for adjustment
#             metrics = self.quantum_state.get_quantum_metrics()
#             cognitive_state = self.recursive_cognition.get_cognitive_state()
# 
#             # Calculate adjustment factors
#             stability = cognitive_state.get('mean_stability', 0.5)
#             coherence = metrics.get('phase_coherence', 0.5)
#             prediction_error = self.training_pipeline.get_training_summary().get('avg_distinction_loss', 0.0)
# 
#             # Combined factor - higher values indicate more stable conditions
#             adjustment_factor = (0.4 * stability + 0.3 * coherence + 0.3 * (1.0 - prediction_error))
# 
#             # Adjust based on factor
#             if adjustment_factor > 0.7:  # Very stable, can increase learning rate
#                 new_lr = min(self.learning_rate * 1.05, LEARNING_RATE_MAX)
#             elif adjustment_factor < 0.3:  # Unstable, decrease learning rate
#                 new_lr = max(self.learning_rate * 0.8, LEARNING_RATE_MIN)
#             else:  # Maintain with small adjustments
#                 new_lr = self.learning_rate * (0.95 + 0.1 * adjustment_factor)
# 
#             # Apply the adjustment
#             self.learning_rate = new_lr
# 
#             # Apply to training pipeline if available
#             if hasattr(self, 'training_pipeline') and hasattr(self.training_pipeline, 'optimizer'):
#                 for param_group in self.training_pipeline.optimizer.param_groups:
#                     param_group['lr'] = new_lr
# 
#             # Log adjustment
#             self.logger.debug(f"Learning rate adjusted: {self.learning_rate:.6f} (factor: {adjustment_factor:.3f})")
# 
#         except Exception as e:
#             self.logger.error(f"Error adjusting learning rates: {e}")
#             # Don't raise - keep default learning rate
# 
#     def get_cognitive_feedback_visualization(self) -> Dict[str, Any]:
#         """
#         Get visualization data for cognitive feedback matrix.
# 
#         Returns:
#             Dictionary with feedback matrix data and metrics
#         """
#         try:
#             # Get feedback visualization from cognitive structure
#             feedback_data = self.recursive_cognition.get_feedback_matrix_visualization()
# 
#             # Add additional agent metrics for context
#             metrics = self.quantum_state.get_quantum_metrics()
#             feedback_data.update({
#                 'agent_distinction': float(self.distinction_level),
#                 'agent_coherence': float(metrics.get('phase_coherence', 0.5)),
#                 'agent_entropy': float(metrics.get('normalized_entropy', 0.5)),
#                 'agent_stability': float(self.stability_factor)
#             })
# 
#             return feedback_data
# 
#         except Exception as e:
#             print(f"Error getting cognitive feedback visualization: {e}")
#             return {
#                 'error': str(e),
#                 'matrix': [[0.0]]
#             }
# 
#     def recursive_meta_adaptation(self):
#         """Apply meta-adaptation to transformer parameters based on distinction variance."""
#         try:
#             # Get recent distinction variance
#             past_states = self.memory.retrieve_recent(10, level=1)
#             if not past_states:
#                 print("Warning: No past states available for meta-adaptation")
#                 return
# 
#             # Calculate distinction variance
#             distinction_values = [s[1] for s in past_states if isinstance(s, tuple) and len(s) > 1]
#             if not distinction_values:
#                 print("Warning: No valid distinction values for meta-adaptation")
#                 return
# 
#             distinction_var = float(np.var(distinction_values))
# 
#             # Apply meta-adaptation if variance is high
#             if distinction_var > 0.03 and self.stability_factor > 0.5:
#                 # Perturb transformer parameters
#                 with torch.no_grad():
#                     for param in self.transformer.parameters():
#                         if param.requires_grad:
#                             noise_scale = 0.001 * (1.0 - self.stability_factor)
#                             param.data += torch.randn_like(param) * noise_scale
# 
#                 # Update adaptation momentum
#                 self.adaptation_momentum = (
#                     MOMENTUM_DECAY * self.adaptation_momentum +
#                     (1 - MOMENTUM_DECAY) * distinction_var
#                 )
# 
#                 print("Meta-adaptation applied.")
# 
#         except Exception as e:
#             print(f"Error in meta-adaptation: {e}")
#             traceback.print_exc()
# 
#     def reinforce_quantum_coherence(self):
#         """Reinforce quantum coherence based on metrics."""
#         try:
#             quantum_metrics = self.quantum_state.get_quantum_distinction_metrics()
# 
#             # Check if reinforcement is needed
#             if quantum_metrics.get('phase_coherence', 1.0) < 0.5 or self.surplus_dynamics.surplus_state.stability < 0.4:
#                 # Calculate distinction variance from memory
#                 memory_entries = self.memory.retrieve_recent(10, level=1)
#                 distinction_var = np.var([s[1] for s in memory_entries]) if memory_entries else 0.0
#                 distinction_var = float(distinction_var)  # Ensure distinction variance is a float
# 
#                 # Apply reinforcement
#                 self.quantum_optimizer.reinforce_coherence(
#                     self.quantum_state.qc,
#                     distinction_var,
#                     quantum_metrics.get('phase_coherence', MINIMUM_COHERENCE_FLOOR)
#                 )
# 
#                 # Update state
#                 self.quantum_state.update_state()
#                 self._update_quantum_metrics()
# 
#         except Exception as e:
#             print(f"Error in reinforce_quantum_coherence: {e}")
#             traceback.print_exc()
# 
#     def _track_adaptation(self):
#         """Track adaptation stability and update metrics."""
#         try:
#             # Initialize adaptation_momentum if needed
#             if not hasattr(self, 'adaptation_momentum'):
#                 self.adaptation_momentum = 0.0
# 
#             # Calculate adaptation momentum
#             if len(self.distinction_history) > 0:
#                 # Measure deviation from recent mean
#                 current_distinction = self.distinction_level
#                 recent_distinctions = list(self.distinction_history)
#                 mean_distinction = np.mean(recent_distinctions)
# 
#                 # Update momentum based on deviation
#                 self.adaptation_momentum = (
#                     0.9 * self.adaptation_momentum +
#                     0.1 * abs(current_distinction - mean_distinction)
#                 )
# 
#                 # Apply coherence reinforcement if needed
#                 if self.adaptation_momentum > 0.05:
#                     print("Adaptation stability fluctuating, adjusting quantum coherence.")
#                     self.quantum_optimizer.reinforce_coherence(
#                         self.quantum_state.qc,
#                         self.distinction_level,
#                         self.quantum_state.phase_coherence
#                     )
# 
#             # Update stability metrics
#             stability_metrics = getattr(self, 'stability_metrics', {})
#             stability_metrics = {
#                 'adaptation_stability': 1.0 / (1.0 + self.adaptation_momentum),
#                 'quantum_coupling': 1.0,
#                 'surplus_stability': 1.0
#             }
#             self.stability_metrics = stability_metrics
# 
#             # Track adaptation history
#             adaptation_history = getattr(self, 'adaptation_history', deque(maxlen=1000))
#             adaptation_history.append({
#                 'momentum': float(self.adaptation_momentum),
#                 'distinction_level': float(self.distinction_level),
#                 'stability': float(stability_metrics['adaptation_stability']),
#                 'timestamp': time.time()
#             })
#             self.adaptation_history = adaptation_history
# 
#         except Exception as e:
#             print(f"Error in adaptation tracking: {e}")
#             if not hasattr(self, 'adaptation_momentum'):
#                 self.adaptation_momentum = 0.0
# 
#     def enter_recovery_mode(self):
#         """Enter recovery mode with reduced activity."""
#         try:
#             self.recovery_mode = True
#             self.stability_factor *= 0.5
#             print("Entering agent recovery mode.")
# 
#             # Apply recovery operations
#             self.adaptation_momentum = 0.0  # Reset momentum
# 
#             # Apply stabilizing quantum operations
#             self.quantum_state.apply_phase_shift(0.1 * np.pi)
# 
#             # Increase coherence preservation
#             self.quantum_optimizer.reinforce_coherence(
#                 self.quantum_state.qc,
#                 0.5,  # Reduced distinction variance
#                 self.quantum_state.phase_coherence
#             )
# 
#             # Schedule recovery exit
#             self.recovery_steps = 50
# 
#         except Exception as e:
#             print(f"Error applying recovery operations: {e}")
#             traceback.print_exc()
# 
#     def update_analysis_history(self) -> None:
#         """Update history with the current metrics."""
#         try:
#             # Initialize analysis_history if it doesn't exist
#             if not hasattr(self, 'analysis_history'):
#                 self.analysis_history = {
#                     'coherence': deque(maxlen=1000),
#                     'distinction': deque(maxlen=1000),
#                     'entropy': deque(maxlen=1000),
#                     'phase': deque(maxlen=1000),  # Added phase key
#                     'analysis_results': deque(maxlen=1000)
#                 }
# 
#             # Also check if specific key is missing and add it
#             if 'phase' not in self.analysis_history:
#                 self.analysis_history['phase'] = deque(maxlen=1000)
# 
#             # Get current metrics
#             metrics = self.quantum_state.get_quantum_metrics()
# 
#             # Ensure metrics has valid values
#             coherence = metrics.get('phase_coherence', MINIMUM_COHERENCE_FLOOR)
#             entropy = metrics.get('normalized_entropy', 0.0)
#             phase = metrics.get('phase', 0.0)
# 
#             # Update history with properly validated values
#             self.analysis_history['coherence'].append(float(coherence))
#             self.analysis_history['distinction'].append(float(self.distinction_level))
#             self.analysis_history['entropy'].append(float(entropy))
#             self.analysis_history['phase'].append(float(phase))
# 
#             # Log progress periodically
#             if hasattr(self, 'step_counter') and self.step_counter % 100 == 0:
#                 self.logger.info(f"Analysis history updated: coherence={len(self.analysis_history['coherence'])}, "
#                               f"distinction={len(self.analysis_history['distinction'])}, "
#                               f"entropy={len(self.analysis_history['entropy'])}, "
#                               f"phase={len(self.analysis_history['phase'])}")
# 
#         except Exception as e:
#             self.logger.error(f"Error updating analysis history: {e}")
#             traceback.print_exc()
# 
#     def perform_analysis(self) -> Dict[str, Any]:
#         """Perform comprehensive analysis and store results with improved error handling."""
#         try:
#             # Ensure analyzer is properly initialized
#             if not hasattr(self, 'analyzer') or self.analyzer is None:
#                 self.logger.warning("Analyzer not initialized. Creating a new one.")
#                 self.analyzer = QuantumAnalyzer(self.num_qubits)
# 
#             # Ensure analysis_history is properly initialized
#             if not hasattr(self, 'analysis_history') or not isinstance(self.analysis_history, dict):
#                 self.logger.warning("Analysis history not properly initialized. Initializing now.")
#                 self.analysis_history = {
#                     'coherence': deque(maxlen=1000),
#                     'distinction': deque(maxlen=1000),
#                     'entropy': deque(maxlen=1000),
#                     'phase': deque(maxlen=1000),
#                     'analysis_results': deque(maxlen=1000)
#                 }
# 
#             # Check if we have enough data for analysis
#             required_keys = ['coherence', 'distinction', 'entropy']
#             if not all(key in self.analysis_history for key in required_keys):
#                 self.logger.warning("Analysis history missing required keys. Cannot perform analysis.")
#                 return {"status": "Missing required history keys"}
# 
#             if not all(len(self.analysis_history[key]) > 5 for key in required_keys):
#                 self.logger.warning("Not enough data points in analysis history. Cannot perform analysis.")
#                 return {"status": "Insufficient data points for analysis"}
# 
#             # Log the history sizes for debugging
#             self.logger.info(f"Analysis history sizes: coherence={len(self.analysis_history['coherence'])}, "
#                           f"distinction={len(self.analysis_history['distinction'])}, "
#                           f"entropy={len(self.analysis_history['entropy'])}")
# 
#             # Perform analysis
#             results = self.analyzer.analyze_quantum_evolution(self.quantum_state, self.analysis_history)
# 
#             # Store results in history
#             if 'analysis_results' in self.analysis_history:
#                 self.analysis_history['analysis_results'].append(results)
# 
#             # Log analysis completion
#             analysis_status = results.get('status', 'complete')
#             self.logger.info(f"Analysis completed with status: {analysis_status}")
# 
#             return results
# 
#         except Exception as e:
#             self.logger.error(f"Error performing analysis: {e}")
#             traceback.print_exc()
#             return {
#                 "status": "error",
#                 "error_message": str(e),
#                 "traceback": traceback.format_exc()
#             }
# 
#     def step(self):
#         """Execute a single simulation step with comprehensive integration and monitoring."""
#         try:
#             # Ensure we have a step counter
#             if not hasattr(self, 'step_counter'):
#                 self.step_counter = 0
#             self.step_counter += 1
# 
#             # Initialize emergent potential field if not existing
#             if not hasattr(self, 'emergent_potential_field'):
#                 from emergent_potential import EmergentPotentialField
#                 self.emergent_potential_field = EmergentPotentialField()
#                 self.logger.info("Initialized Emergent Potential Field")
# 
#             # Apply minimum mutation to prevent stasis
#             self.ensure_minimum_mutation()
# 
#             # CRITICAL FIX - DIRECT QUANTUM STATE INTERVENTION
#             # Force statevector modification to break out of static state
#             if hasattr(self, 'quantum_state') and hasattr(self.quantum_state, 'statevector'):
#                 # Get current statevector
#                 if isinstance(self.quantum_state.statevector, np.ndarray):
#                     current_state = self.quantum_state.statevector.copy()
#                 elif hasattr(self.quantum_state.statevector, 'data'):
#                     try:
#                         current_state = np.array(self.quantum_state.statevector.data)
#                     except:
#                         # Create a fresh statevector if all else fails
#                         from qiskit.quantum_info import Statevector
#                         self.quantum_state.statevector = Statevector.from_label('0' * self.quantum_state.num_qubits)
#                         current_state = np.array(self.quantum_state.statevector.data)
# 
#                 # Apply meaningful modification to break stasis
#                 # Add a small random phase to each amplitude
#                 random_phases = np.exp(1j * 0.1 * np.random.random(current_state.shape))
#                 modified_state = current_state * random_phases
# 
#                 # Normalize the modified state
#                 norm = np.linalg.norm(modified_state)
#                 if norm > 0:
#                     modified_state = modified_state / norm
# 
#                 # Update the statevector directly
#                 try:
#                     from qiskit.quantum_info import Statevector
#                     self.quantum_state.statevector = Statevector(modified_state)
#                 except:
#                     # Fallback to direct array assignment
#                     self.quantum_state.statevector = modified_state
# 
#             # Verify system state periodically (every 20 steps)
#             if self.step_counter % 20 == 0:
#                 if hasattr(self, 'verify_system_state'):
#                     self.verify_system_state()
# 
#             # Apply quantum operations to ensure state evolution
#             # Apply different gates based on step to create variation
#             qubit_indices = list(range(self.quantum_state.num_qubits))
#             random.shuffle(qubit_indices)  # Add randomization
# 
#             # Mix different quantum operations for better evolution dynamics
#             if self.step_counter % 3 == 0:
#                 # Apply Hadamard gates periodically to increase entropy
#                 for q in qubit_indices[:2]:  # Apply to subset of qubits
#                     self.quantum_state.apply_gate('h', [q])
#             elif self.step_counter % 3 == 1:
#                 # Apply rotation gates with dynamic angles
#                 angle = 0.1 * np.pi * (0.5 + 0.5 * np.sin(self.step_counter * 0.1))
#                 for q in qubit_indices[:2]:
#                     self.quantum_state.apply_gate('rx', [q], {'theta': angle})
#             else:
#                 # Apply phase rotation with varying angle
#                 angle = 0.1 * np.pi * (0.5 + 0.5 * np.cos(self.step_counter * 0.07))
#                 for q in qubit_indices[:2]:
#                     self.quantum_state.apply_gate('rz', [q], {'phi': angle})
# 
#             # Always apply a small phase shift with varying angle to ensure evolution
#             phase_angle = 0.05 * np.pi * np.sin(self.step_counter * 0.05)
#             self.quantum_state.apply_phase_shift(phase_angle)
# 
#             # Force quantum state update
#             if hasattr(self.quantum_state, 'update_state'):
#                 self.quantum_state.update_state()
# 
#             # Force entropy and coherence recalculation
#             if hasattr(self.quantum_state, 'update_phase_coherence'):
#                 self.quantum_state.update_phase_coherence()
# 
#             # Break out of extreme entropy states if needed
#             try:
#                 metrics = self.quantum_state.get_quantum_metrics()
#                 if abs(metrics['normalized_entropy'] - 1.0) < 0.01 or metrics['normalized_entropy'] < 0.001:
#                     # Apply a series of gates to shift entropy away from extremes
#                     for i, q in enumerate(qubit_indices[:3]):
#                         if i % 3 == 0:
#                             self.quantum_state.apply_gate('h', [q])
#                         elif i % 3 == 1:
#                             self.quantum_state.apply_gate('x', [q])
#                         else:
#                             angle = 0.3 * np.pi * np.random.random()
#                             self.quantum_state.apply_gate('rx', [q], {'theta': angle})
# 
#                     # Apply controlled operation between qubits
#                     if len(qubit_indices) >= 2:
#                         self.quantum_state.apply_gate('cx', [qubit_indices[0], qubit_indices[1]])
# 
#                     # Force update again
#                     self.quantum_state.update_state()
#                     # Re-check metrics
#                     metrics = self.quantum_state.get_quantum_metrics()
#             except Exception as entropy_error:
#                 self.logger.warning(f"Error adjusting entropy: {entropy_error}")
# 
#             # Now execute the adaptation pipeline
#             if hasattr(self, 'adapt'):
#                 self.adapt()
# 
#             # Process recovery if active
#             if hasattr(self, 'recovery_mode') and self.recovery_mode:
#                 if not hasattr(self, 'recovery_steps'):
#                     self.recovery_steps = 10  # Default if not set
#                 self.recovery_steps -= 1
#                 if self.recovery_steps <= 0:
#                     self.recovery_mode = False
#                     if hasattr(self, 'stability_factor'):
#                         self.stability_factor = min(self.stability_factor * 1.5, 1.0)
#                     if hasattr(self, 'logger'):
#                         self.logger.info("Exiting recovery mode.")
#                     else:
#                         print("Exiting recovery mode.")
# 
#             # Get current metrics - with error handling
#             try:
#                 metrics = self.quantum_state.get_quantum_metrics()
# 
#                 # Safety check for extreme entropy values
#                 if abs(metrics['normalized_entropy'] - 1.0) < 0.01:
#                     metrics['normalized_entropy'] = 0.95  # Slightly away from 1.0
#                 elif metrics['normalized_entropy'] < 0.01:
#                     metrics['normalized_entropy'] = 0.05  # Slightly above 0.0
#             except Exception as metrics_error:
#                 if hasattr(self, 'logger'):
#                     self.logger.error(f"Error getting quantum metrics: {metrics_error}")
#                 else:
#                     print(f"Error getting quantum metrics: {metrics_error}")
#                 metrics = {
#                     'phase_coherence': 0.5,
#                     'normalized_entropy': 0.5,
#                     'phase': 0.0,
#                     'quantum_surplus_coupling': 0.5
#                 }
# 
#             # Process excess stability and emergent potential field
#             surplus_metrics = self.surplus_dynamics.get_surplus_metrics()
#             if hasattr(self.surplus_dynamics, 'excess_stability_potential'):
#                 excess_stability = self.surplus_dynamics.excess_stability_potential
# 
#                 # Register with emergent potential field
#                 emergence_triggered = self.emergent_potential_field.register_potential(
#                     component_id='surplus_dynamics',
#                     potential=excess_stability,
#                     component_type='surplus',
#                     state_metrics=surplus_metrics
#                 )
# 
#                 # If emergence was triggered by the potential field
#                 if emergence_triggered:
#                     self.logger.info("\n🔆 EMERGENT POTENTIAL TRIGGERED EMERGENCE EVENT")
# 
#                     # Get field state for context
#                     field_state = self.emergent_potential_field.get_field_state()
# 
#                     # Create emergence event with field information
#                     self.handle_emergent_dimension(
#                         tensor_shape=(4, 4, 4, field_state['field_intensity']),
#                         source="emergent_potential_field"
#                     )
# 
#                     # Generate special symbolic expression
#                     if hasattr(self, 'symbolic_system'):
#                         surplus = self.surplus_dynamics.surplus_state.total_surplus()
#                         distinction = self.distinction_level
#                         coherence = self.quantum_state.phase_coherence
# 
#                         expression = self.symbolic_system.handle_post_emergence(
#                             surplus=surplus,
#                             distinction=distinction,
#                             coherence=coherence,
#                             dimensionality=int(2 + field_state['field_intensity']),
#                             entropy=metrics.get('normalized_entropy', 0.5)
#                         )
# 
#                         self.logger.info(f"\n🌌 Emergent Potential Field Expression: {expression}")
# 
#                 # Apply excess stability to ontological field
#                 if hasattr(self, 'ontological_field'):
#                     field_resistance = self.ontological_field.resistance(self.distinction_level)
#                     self.ontological_field.adapt_to_agent(
#                         self.distinction_level,
#                         quantum_coupling=metrics.get('quantum_surplus_coupling', 1.0),
#                         field_threshold=0.1,
#                         excess_stability=excess_stability
#                     )
# 
#             # Update field state and print status regularly
#             if hasattr(self, 'emergent_potential_field') and self.step_counter % 20 == 0:
#                 field_state = self.emergent_potential_field.get_field_state()
#                 self.logger.info(f"\nEmergent Potential Field State: Total={field_state['total_potential']:.4f}, "
#                               f"Threshold={field_state['threshold']:.4f}, "
#                               f"Emergence Prob={field_state.get('emergence_probability', 0.0):.2f}")
# 
#             # Handle symbolic expression logic
#             if not hasattr(self, 'expression_cooldown'):
#                 self.expression_cooldown = 0
#                 self.expression_cooldown_period = 50  # Default if not set
# 
#             if self.expression_cooldown <= 0:
#                 try:
#                     if hasattr(self, '_check_expression_triggers'):
#                         should_express = self._check_expression_triggers(metrics)
#                     else:
#                         # Default expression trigger logic if method missing
#                         random_factor = 0.1 if self.step_counter < 100 else 0.02
#                         should_express = random.random() < random_factor
# 
#                     if should_express:
#                         if hasattr(self, 'generate_advanced_symbolic_expression'):
#                             expression = self.generate_advanced_symbolic_expression()
#                         else:
#                             # Fallback to basic symbolic generation
#                             expression = self._generate_basic_symbolic_expression(metrics)
# 
#                         if hasattr(self, 'logger'):
#                             self.logger.info(f"\n🔮 Symbolic Expression: {expression}")
#                         else:
#                             print(f"\n🔮 Symbolic Expression: {expression}")
# 
#                         # Get cognitive state with proper error handling
#                         try:
#                             cognitive_state = self.recursive_cognition.get_cognitive_state() if hasattr(self, 'recursive_cognition') else {}
#                         except Exception:
#                             cognitive_state = {}
# 
#                         # Check for significance and add analysis if appropriate
#                         if (self.distinction_level > 0.8 or
#                             metrics.get('phase_coherence', 0.5) < 0.3 or
#                             cognitive_state.get('collapse_probability', 0.0) > 0.5):
# 
#                             # Generate analysis with patterns
#                             if hasattr(self, 'symbolic_system') and hasattr(self.symbolic_system, 'analyze_emergence_patterns'):
#                                 try:
#                                     patterns = self.symbolic_system.analyze_emergence_patterns()
#                                     if patterns and 'dominant_patterns' in patterns:
#                                         dominant = patterns.get('dominant_patterns', {})
#                                         analysis = (f"\n📊 Pattern Analysis: {dominant.get('descriptor', 'Unknown')} "
#                                                   f"pattern with {patterns.get('component_diversity', {}).get('overall', 0):.3f} diversity")
#                                         if hasattr(self, 'logger'):
#                                             self.logger.info(analysis)
#                                         else:
#                                             print(analysis)
#                                 except Exception as pattern_error:
#                                     if hasattr(self, 'logger'):
#                                         self.logger.warning(f"Error analyzing patterns: {pattern_error}")
#                                     else:
#                                         print(f"Error analyzing patterns: {pattern_error}")
# 
#                         self.expression_cooldown = self.expression_cooldown_period
#                 except Exception as expr_error:
#                     if hasattr(self, 'logger'):
#                         self.logger.warning(f"Error in symbolic expression generation: {expr_error}")
#                     else:
#                         print(f"Error in symbolic expression generation: {expr_error}")
#                     self.expression_cooldown = 10  # Shorter cooldown after error
#             else:
#                 self.expression_cooldown -= 1
# 
#             # Update state with ontological field interaction - with error handling
#             try:
#                 if hasattr(self, 'ontological_field'):
#                     field_resistance = self.ontological_field.resistance(self.distinction_level)
#                     self.ontological_field.adapt_to_agent(
#                         self.distinction_level,
#                         quantum_coupling=metrics.get('quantum_surplus_coupling', 1.0),
#                         field_threshold=0.1
#                     )
#             except Exception as field_error:
#                 if hasattr(self, 'logger'):
#                     self.logger.warning(f"Error in ontological field interaction: {field_error}")
#                 else:
#                     print(f"Error in ontological field interaction: {field_error}")
# 
#             # Update analysis history
#             try:
#                 if hasattr(self, 'update_analysis_history'):
#                     self.update_analysis_history()
#             except Exception as analysis_error:
#                 if hasattr(self, 'logger'):
#                     self.logger.warning(f"Error updating analysis history: {analysis_error}")
#                 else:
#                     print(f"Error updating analysis history: {analysis_error}")
# 
#             # Interact with emergence tracking
#             try:
#                 if hasattr(self, 'dimension_increase_detected') and self.dimension_increase_detected and hasattr(self, 'emergence_tracker'):
#                     # Handle emergence explicitly
#                     sample_shape = (4, 4, 4, 4)  # Default shape
#                     # Try to get actual shape if possible
#                     if hasattr(self, 'current_tensor_shape'):
#                         tensor_shape = self.current_tensor_shape
#                     elif hasattr(self.quantum_state, 'statevector') and hasattr(self.quantum_state.statevector, 'shape'):
#                         tensor_shape = self.quantum_state.statevector.shape
#                     else:
#                         tensor_shape = sample_shape
# 
#                     self.emergence_tracker.record_emergence(
#                         tensor_shape=tensor_shape,
#                         timestamp=time.time(),
#                         resource_usage={'cpu_percent': 0, 'memory_percent': 0},  # Would come from monitoring
#                         agent_metrics=metrics
#                     )
#             except Exception as emergence_error:
#                 if hasattr(self, 'logger'):
#                     self.logger.warning(f"Error tracking emergence: {emergence_error}")
#                 else:
#                     print(f"Error tracking emergence: {emergence_error}")
# 
#             # Perform periodic analysis every 10 steps
#             if self.step_counter % 10 == 0:
#                 try:
#                     if hasattr(self, 'perform_analysis'):
#                         self.logger.info("\nPerforming analysis...")
#                         results = self.perform_analysis()
# 
#                         if results and results.get('status') != 'error':
#                             self.logger.info("\nAnalysis Report:")
#                             if hasattr(self, 'analyzer') and hasattr(self.analyzer, 'generate_analysis_report'):
#                                 report = self.analyzer.generate_analysis_report(results)
#                                 self.logger.info(report)
#                             else:
#                                 # Fallback if generate_analysis_report isn't available
#                                 for key, value in results.items():
#                                     if isinstance(value, (int, float, str, bool)):
#                                         self.logger.info(f"{key}: {value}")
#                         else:
#                             self.logger.info("\nAnalysis results: " + (results.get('status', 'No results') if results else "No results"))
#                             if results and 'error_message' in results:
#                                 self.logger.info(f"Error: {results['error_message']}")
#                 except Exception as analysis_error:
#                     self.logger.warning(f"Error performing analysis: {analysis_error}")
#                     traceback.print_exc()
#                     self.logger.info("No analysis results available due to error.")
# 
#             # Print status periodically
#             if self.step_counter % 50 == 0 or self.step_counter == 1:
#                 status_message = (f"\n[Step {self.step_counter}] "
#                               f"Distinction: {self.distinction_level:.3f}, "
#                               f"Coherence: {metrics['phase_coherence']:.3f}, "
#                               f"Entropy: {metrics['normalized_entropy']:.3f}, "
#                               f"Stability: {getattr(self, 'stability_factor', 1.0):.3f}, "
#                               f"SurplusStab: {self.surplus_dynamics.surplus_state.stability:.3f}")
# 
#                 if hasattr(self, 'logger'):
#                     self.logger.info(status_message)
#                 else:
#                     print(status_message)
# 
#             # Add a helper method for fallback symbolic expressions if needed
#             if not hasattr(self, '_generate_basic_symbolic_expression'):
#                 def _generate_basic_symbolic_expression(self, metrics):
#                     """Basic fallback for symbolic expression generation."""
#                     descriptors = ["Flux", "Equilibrium", "Distinction", "Recursion", "Convergence", "Divergence"]
#                     relations = ["aligns with", "dissolves across", "bends toward", "extends beyond", "contracts into"]
#                     concepts = ["stability", "recursion", "entropy", "phase shift", "emergence", "ontology"]
# 
#                     descriptor = random.choice(descriptors)
#                     relation = random.choice(relations)
#                     concept = random.choice(concepts)
# 
#                     return f"{descriptor} {relation} {concept}"
# 
#                 # Add the method to the instance
#                 setattr(self, '_generate_basic_symbolic_expression', _generate_basic_symbolic_expression.__get__(self, type(self)))
# 
#             return True  # Return success
# 
#         except Exception as e:
#             error_message = f"Error in step execution: {e}"
#             if hasattr(self, 'logger'):
#                 self.logger.error(error_message)
#             else:
#                 print(error_message)
#             traceback.print_exc()
# 
#             if hasattr(self, 'recovery_mode') and not self.recovery_mode:
#                 if hasattr(self, 'enter_recovery_mode'):
#                     self.enter_recovery_mode()
# 
#             return False  # Return failure
# 
#     def get_emergent_potential_visualization(self) -> Dict[str, Any]:
#         """
#         Get visualization data for the emergent potential field.
# 
#         Returns:
#             Dictionary with visualization data
#         """
#         try:
#             if not hasattr(self, 'emergent_potential_field'):
#                 return {'error': 'Emergent potential field not initialized'}
# 
#             # Get field state
#             field_state = self.emergent_potential_field.get_field_state()
# 
#             # Get component contributions
#             components = []
#             for component_id, data in self.emergent_potential_field.component_potentials.items():
#                 components.append({
#                     'id': component_id,
#                     'type': data['component_type'],
#                     'potential': data['weighted_potential'],
#                     'raw_potential': data['raw_potential'],
#                     'timestamp': data['timestamp']
#                 })
# 
#             # Sort components by potential
#             components.sort(key=lambda x: x['potential'], reverse=True)
# 
#             # Get emergence history
#             emergence_events = []
#             for event in list(self.emergent_potential_field.emergence_history)[-10:]:  # Last 10 events
#                 emergence_events.append({
#                     'timestamp': event['timestamp'],
#                     'intensity': event['intensity'],
#                     'potential': event['potential'],
#                     'threshold': event['threshold'],
#                     'sequence': event['sequence_number']
#                 })
# 
#             # Prepare visualization data
#             visualization_data = {
#                 'field_state': field_state,
#                 'components': components,
#                 'emergence_events': emergence_events,
#                 'threshold_history': [
#                     {
#                         'timestamp': entry['timestamp'],
#                         'threshold': entry['threshold'],
#                         'total_potential': entry['total_potential']
#                     }
#                     for entry in list(self.emergent_potential_field.potential_history)[-20:]  # Last 20 entries
#                 ]
#             }
# 
#             # If we have cognitive state, add correlation data
#             if hasattr(self, 'recursive_cognition'):
#                 cognitive_state = self.recursive_cognition.get_cognitive_state()
#                 visualization_data['cognitive_correlation'] = {
#                     'collapse_probability': cognitive_state.get('collapse_probability', 0.0),
#                     'mean_stability': cognitive_state.get('mean_stability', 0.0),
#                     'emergence_probability': field_state.get('emergence_probability', 0.0),
#                     'field_stability': field_state.get('stability_factor', 1.0)
#                 }
# 
#             return visualization_data
# 
#         except Exception as e:
#             if hasattr(self, 'logger'):
#                 self.logger.error(f"Error getting emergent potential visualization: {e}")
#             else:
#                 print(f"Error getting emergent potential visualization: {e}")
#             return {'error': str(e)}
# 
#     def _check_expression_triggers(self, metrics: Dict[str, float]) -> bool:
#         """
#         Check if the agent should generate a symbolic expression based on
#         comprehensive state evaluation across multiple systems.
# 
#         Args:
#             metrics: Dictionary of quantum metrics
# 
#         Returns:
#             Boolean indicating whether an expression should be generated
#         """
#         try:
#             # Track distinction change
#             distinction_change = abs(self.distinction_level - self.previous_distinction)
#             self.previous_distinction = self.distinction_level
# 
#             # Get cognitive state for enhanced detection
#             cognitive_state = self.recursive_cognition.get_cognitive_state()
#             collapse_probability = cognitive_state.get('collapse_probability', 0.0)
#             cognitive_stability = cognitive_state.get('mean_stability', 0.5)
# 
#             # Extract key metrics
#             coherence = metrics.get('phase_coherence', 0.5)
#             entropy = metrics.get('normalized_entropy', 0.5)
#             phase_stability = metrics.get('phase_stability', 1.0)
# 
#             # Emergence and dimension detection
#             dimension_change = self.dimension_increase_detected
#             emergence_active = hasattr(self, 'emergence_tracker') and getattr(self.emergence_tracker, 'is_emergence_active', False)
# 
#             # Calculate enriched triggers using cognitive state
# 
#             # Critical cognitive transitions
#             cognitive_transition = False
#             if hasattr(self, 'recursive_cognition') and hasattr(self.recursive_cognition, 'history'):
#                 if len(self.recursive_cognition.history) >= 2:
#                     # Compare current and previous cognitive states
#                     current = self.recursive_cognition.get_cognitive_state()
#                     previous = self.recursive_cognition.history[-1]
# 
#                     # Check for significant changes in key metrics
#                     strength_change = abs(current.get('mean_strength', 1.0) -
#                                         getattr(previous[0], 'strength', 1.0))
#                     identity_change = abs(current.get('mean_identity', 1.0) -
#                                         getattr(previous[0], 'core_identity', 1.0))
# 
#                     cognitive_transition = (strength_change > 0.2 or identity_change > 0.2)
# 
#             # Enhanced detection criteria
#             significant_distinction_change = distinction_change > self.expression_threshold
#             high_coherence = coherence > 0.7 and entropy < 0.4
#             low_coherence = coherence < 0.3 and entropy > 0.6
#             high_entropy = entropy > 0.7 and coherence < 0.5
#             instability_risk = phase_stability < 0.3 or cognitive_stability < 0.3
#             collapse_risk = collapse_probability > COLLAPSE_DISSIPATION_THRESHOLD * 0.8
# 
#             # System state changes
#             system_change = (
#                 self.recovery_mode or
#                 self.collapse_prevention_active or
#                 emergence_active
#             )
# 
#             # Periodic expression with randomness for more natural timing
#             base_interval = self.expression_periodic_interval
#             random_factor = random.random() * 0.2 + 0.9  # 0.9-1.1 multiplier
#             adjusted_interval = int(base_interval * random_factor)
#             periodic_trigger = (self.step_counter % adjusted_interval == 0)
# 
#             # Decision to express with weighting for more interesting expressions
#             should_express = any([
#                 significant_distinction_change,  # Basic trigger
#                 high_coherence and random.random() < 0.7,  # High coherence is interesting but not always
#                 low_coherence and random.random() < 0.9,  # Low coherence is usually interesting
#                 high_entropy and random.random() < 0.8,   # High entropy is often interesting
#                 instability_risk and random.random() < 0.8,  # Instability is often interesting
#                 collapse_risk,  # Always interesting
#                 cognitive_transition and random.random() < 0.9,  # Cognitive shifts are important
#                 dimension_change,  # Always express on dimension changes
#                 emergence_active and random.random() < 0.8,  # Often express during emergence
#                 system_change and random.random() < 0.7,  # Sometimes express on system changes
#                 periodic_trigger  # Regular expressions
#             ])
# 
#             return should_express
# 
#         except Exception as e:
#             self.logger.error(f"Error checking expression triggers: {e}")
#             return False
# 
#     def generate_advanced_symbolic_expression(self) -> str:
#         """
#         Generate a sophisticated symbolic expression that integrates multiple
#         components of the agent's cognitive state, quantum state, and surplus dynamics.
# 
#         Returns:
#             A symbolic expression representing the agent's current ontological state
#         """
#         try:
#             # Get comprehensive metrics from different systems
#             quantum_metrics = self.quantum_state.get_quantum_metrics()
#             cognitive_state = self.recursive_cognition.get_cognitive_state()
#             surplus_metrics = self.surplus_dynamics.get_surplus_metrics()
# 
#             # Extract key values with proper error handling
#             coherence = quantum_metrics.get('phase_coherence', 0.5)
#             entropy = quantum_metrics.get('normalized_entropy', 0.5)
#             phase_stability = quantum_metrics.get('phase_stability', 1.0)
#             distinction = self.distinction_level
# 
#             # Cognitive components
#             cognitive_strength = cognitive_state.get('mean_strength', 1.0)
#             cognitive_stability = cognitive_state.get('mean_stability', 0.5)
#             collapse_probability = cognitive_state.get('collapse_probability', 0.0)
#             mean_identity = cognitive_state.get('mean_identity', 1.0)
#             quantum_influence = cognitive_state.get('quantum_influence', 0.5)
# 
#             # Get dimensionality if available
#             dimensionality = None
#             if hasattr(self, 'dimension_monitor') and hasattr(self.dimension_monitor, 'last_dimensionality'):
#                 dimensionality = self.dimension_monitor.last_dimensionality
# 
#             # Get causality metrics if available
#             directionality = 0.0
#             causality_strength = 0.0
#             if hasattr(self, 'analyzer') and hasattr(self.analyzer, 'causality_analysis'):
#                 directionality = getattr(self.analyzer.causality_analysis, 'directionality', 0.0)
#                 causality_strength = getattr(self.analyzer.causality_analysis, 'strength', 0.0)
# 
#             # Calculate enriched parameters for symbolic system
#             # Surplus is weighted by cognitive components
#             enriched_surplus = self.surplus_dynamics.surplus_state.total_surplus() * (
#                 0.6 + 0.4 * cognitive_strength + 0.2 * quantum_influence
#             )
# 
#             # Distinction is modulated by cognitive factors
#             enriched_distinction = distinction * (
#                 0.7 + 0.3 * mean_identity + 0.1 * (1.0 - collapse_probability)
#             )
# 
#             # Coherence is enhanced by stability factors
#             enriched_coherence = coherence * (
#                 0.8 + 0.2 * cognitive_stability + 0.1 * phase_stability
#             )
# 
#             # Generate expression using enriched parameters
#             expression = self.symbolic_system.generate_symbolic_expression(
#                 surplus=enriched_surplus,
#                 distinction=enriched_distinction,
#                 coherence=enriched_coherence,
#                 entropy=entropy,
#                 dimensionality=dimensionality
#             )
# 
#             # Store comprehensive metrics with expression for later analysis
#             self.symbolic_history.append({
#                 'expression': expression,
#                 'step': self.step_counter,
#                 'distinction': distinction,
#                 'enriched_distinction': enriched_distinction,
#                 'coherence': coherence,
#                 'enriched_coherence': enriched_coherence,
#                 'entropy': entropy,
#                 'dimensionality': dimensionality,
#                 'collapse_probability': collapse_probability,
#                 'mean_strength': cognitive_strength,
#                 'mean_stability': cognitive_stability,
#                 'mean_identity': mean_identity,
#                 'quantum_influence': quantum_influence,
#                 'directionality': directionality,
#                 'causality_strength': causality_strength,
#                 'timestamp': time.time()
#             })
# 
#             return expression
# 
#         except Exception as e:
#             self.logger.error(f"Error generating advanced symbolic expression: {e}")
#             traceback.print_exc()
#             return "Flux aligns with stability."  # Safe default
# 
#     def get_symbolic_insights(self, limit: int = 5) -> Dict[str, Any]:
#         """
#         Get insights from symbolic expressions history with pattern analysis
#         for a deeper understanding of the agent's cognitive evolution.
# 
#         Args:
#             limit: Maximum number of recent expressions to analyze
# 
#         Returns:
#             Dictionary with symbolic insights and patterns
#         """
#         try:
#             if not hasattr(self, 'symbolic_history') or not self.symbolic_history:
#                 return {"error": "No symbolic history available"}
# 
#             # Get pattern analysis
#             patterns = self.symbolic_system.analyze_emergence_patterns()
# 
#             # Get recent expressions
#             recent = list(self.symbolic_history)[-limit:]
# 
#             # Calculate expression trajectory
#             trajectory = "stable"
#             if len(recent) >= 3:
#                 # Analyze distinction and coherence trends
#                 distinction_trend = [entry.get('distinction', 0.5) for entry in recent]
#                 coherence_trend = [entry.get('coherence', 0.5) for entry in recent]
# 
#                 # Simple trajectory classification
#                 d_trend = sum(np.diff(distinction_trend)) / (len(distinction_trend) - 1) if len(distinction_trend) > 1 else 0
#                 c_trend = sum(np.diff(coherence_trend)) / (len(coherence_trend) - 1) if len(coherence_trend) > 1 else 0
# 
#                 if d_trend > 0.05 and c_trend > 0.05:
#                     trajectory = "ascending"
#                 elif d_trend < -0.05 and c_trend < -0.05:
#                     trajectory = "descending"
#                 elif abs(d_trend) > 0.1 or abs(c_trend) > 0.1:
#                     trajectory = "oscillating"
# 
#             # Extract pattern information
#             dominant_pattern = "none"
#             pattern_stability = 0.0
#             component_diversity = 0.0
# 
#             if patterns:
#                 dominant_pattern = patterns.get('dominant_patterns', {}).get('descriptor', 'none')
#                 pattern_stability = patterns.get('coherence_stability', 0.0)
#                 component_diversity = patterns.get('component_diversity', {}).get('overall', 0.0)
# 
#             # Construct insights
#             insights = {
#                 "recent_expressions": [entry.get('expression') for entry in recent],
#                 "expression_count": len(self.symbolic_history),
#                 "trajectory": trajectory,
#                 "dominant_pattern": dominant_pattern,
#                 "pattern_stability": pattern_stability,
#                 "component_diversity": component_diversity,
#                 "expression_frequency": len(self.symbolic_history) / max(1, self.step_counter),
#                 "patterns": patterns
#             }
# 
#             return insights
# 
#         except Exception as e:
#             self.logger.error(f"Error getting symbolic insights: {e}")
#             traceback.print_exc()
#             return {"error": str(e)}
# 
#     def _log_step_summary(self, metrics: Dict[str, float],
#                           surplus_state: SurplusState,
#                           field_resistance: float):
#         """Log comprehensive step summary."""
#         try:
#             self.logger.info(f"\nStep Summary:")
#             self.logger.info(f"Coherence: {metrics.get('phase_coherence', 0.0):.4f}")
#             self.logger.info(f"Distinction: {self.distinction_level:.4f}")
#             self.logger.info(f"Stability: {self.stability_factor:.4f}")
#             self.logger.info(f"Field Resistance: {field_resistance:.4f}")
# 
#             # Log special modes
#             if self.collapse_prevention_active:
#                 self.logger.info("Collapse Prevention: Active")
#             if self.recovery_mode:
#                 self.logger.info(f"Recovery Mode: {self.recovery_steps} steps remaining")
# 
#             # Log surplus state
#             self.logger.info("\nSurplus State:")
#             for key, value in surplus_state.values.items():
#                 self.logger.info(f"  {key}: {value:.4f}")
#         except Exception as e:
#             self.logger.error(f"Error logging step summary: {e}")
# 
#     def _generate_symbolic_expression(self) -> None:
#         """Generate and log a symbolic expression based on internal state."""
#         try:
#             # Get current metrics
#             metrics = self.quantum_state.get_quantum_metrics()
# 
#             # Get cognitive state for enhanced expressions
#             cognitive_state = self.recursive_cognition.get_cognitive_state()
# 
#             # Generate expression using surplus, distinction, and coherence from the agent itself
#             surplus = self.surplus_dynamics.surplus_state.total_surplus()
#             distinction = self.distinction_level
#             coherence = metrics.get('phase_coherence', 0.5)
#             entropy = metrics.get('normalized_entropy', 0.5)
# 
#             # Get dimensionality if available
#             dimensionality = None
#             if hasattr(self, 'dimension_monitor') and hasattr(self.dimension_monitor, 'last_dimensionality'):
#                 dimensionality = self.dimension_monitor.last_dimensionality
# 
#             # Generate the expression
#             expression = self.symbolic_system.generate_symbolic_expression(
#                 surplus, distinction, coherence, entropy, dimensionality
#             )
# 
#             # Store expression with enhanced metrics
#             self.last_symbolic_expression = expression
#             self.symbolic_history.append({
#                 'expression': expression,
#                 'step': self.step_counter,
#                 'distinction': distinction,
#                 'coherence': coherence,
#                 'entropy': entropy,
#                 'dimensionality': dimensionality,
#                 'collapse_probability': cognitive_state.get('collapse_probability', 0.0),
#                 'mean_strength': cognitive_state.get('mean_strength', 1.0),
#                 'mean_stability': cognitive_state.get('mean_stability', 0.5),
#                 'mean_identity': cognitive_state.get('mean_identity', 1.0),
#                 'timestamp': time.time()
#             })
# 
#             # Log the expression
#             self.logger.info(f"\n🔮 Symbolic Expression: {expression}")
# 
#             # If expression seems particularly significant (high distinction or low stability),
#             # generate an additional analysis
#             if (distinction > 0.8 or
#                 cognitive_state.get('mean_stability', 1.0) < 0.3 or
#                 entropy > 0.7 or
#                 cognitive_state.get('collapse_probability', 0.0) > 0.5):
#                 patterns = self.symbolic_system.analyze_emergence_patterns()
#                 if patterns and 'dominant_pattern' in patterns:
#                     analysis = f"\n📊 Pattern Analysis: {patterns.get('dominant_pattern', 'Unknown')} pattern emerging with {patterns.get('coherence_stability', 0.0):.3f} stability."
#                     self.logger.info(analysis)
# 
#         except Exception as e:
#             self.logger.error(f"Error generating symbolic expression: {e}")
# 
#     def get_symbolic_history(self, limit: int = 10) -> List[Dict[str, Any]]:
#         """
#         Get the recent symbolic expression history.
# 
#         Args:
#             limit: Maximum number of expressions to return
# 
#         Returns:
#             List of recent expressions with associated metrics
#         """
#         try:
#             if not hasattr(self, 'symbolic_history'):
#                 return []
# 
#             recent = list(self.symbolic_history)[-limit:]
#             return recent
# 
#         except Exception as e:
#             self.logger.error(f"Error getting symbolic history: {e}")
#             return []
# 
#     def _prepare_experience(self, prediction: float) -> Dict[str, Any]:
#         """Prepare experience for training with enhanced data and dimension handling."""
#         try:
#             # Get input tensor
#             input_tensor = self.prepare_transformer_input()
# 
#             # Handle dimensionality properly
#             has_emergent_dim = False
#             if hasattr(input_tensor, 'dim') and input_tensor.dim() > 3:
#                 # For emergent dimensions, store the full tensor shape
#                 # but convert to numpy in a safe way that preserves dimensionality
#                 state_np = input_tensor.cpu().detach().numpy()
#                 has_emergent_dim = True
# 
#                 # Log the emergence handling
#                 self.logger.debug(f"Preparing experience with emergent dimensions: {input_tensor.shape}")
#             else:
#                 # Standard case
#                 state_np = input_tensor.cpu().numpy()
# 
#             return {
#                 'state': state_np,
#                 'prediction': float(prediction),
#                 'actual': float(self.distinction_level),
#                 'quantum_metrics': self.quantum_state.get_quantum_metrics(),
#                 'stability': float(self.stability_factor),
#                 'adaptation_momentum': float(self.adaptation_momentum),
#                 'has_emergent_dim': has_emergent_dim
#             }
#         except Exception as e:
#             self.logger.error(f"Error preparing experience: {e}")
#             # Return safe default
#             return {
#                 'state': np.zeros((1, 1, 20)),
#                 'prediction': 0.0,
#                 'actual': 0.5,
#                 'quantum_metrics': {},
#                 'stability': 1.0,
#                 'adaptation_momentum': 0.0,
#                 'has_emergent_dim': False
#             }
# 
#     def get_state_summary(self) -> Dict[str, Any]:
#         """
#         Get comprehensive state summary with proper integration of all components.
# 
#         Returns:
#             Dictionary containing current state metrics
#         """
#         try:
#             # Get basic metrics
#             metrics = self.quantum_state.get_quantum_metrics()
#             distinction_mean = np.mean(list(self.distinction_history)) if self.distinction_history else self.distinction_level
# 
#             # Create core state summary
#             summary = {
#                 'distinction_level': self.distinction_level,
#                 'distinction_mean': distinction_mean,
#                 'stability_factor': self.stability_factor,
#                 'adaptation_momentum': self.adaptation_momentum,
#                 'collapse_prevention_active': self.collapse_prevention_active,
#                 'recovery_mode': self.recovery_mode,
#                 'quantum_metrics': metrics,
#                 'phase': self.phase,
#                 'coherence': metrics.get('phase_coherence', MINIMUM_COHERENCE_FLOOR)
#             }
# 
#             # Add surplus state if available
#             if hasattr(self, 'surplus_dynamics') and hasattr(self.surplus_dynamics, 'surplus_state'):
#                 summary['surplus'] = self.surplus_dynamics.surplus_state.copy()
# 
#             # Add cognitive state if available
#             if hasattr(self, 'recursive_cognition'):
#                 summary['cognitive_state'] = self.recursive_cognition.get_cognitive_state()
# 
#             # Add training summary if available
#             if hasattr(self, 'training_pipeline'):
#                 summary['training_summary'] = self.training_pipeline.get_training_summary()
# 
#             # Add learning rate if available
#             if hasattr(self, 'learning_rate'):
#                 summary['learning_rate'] = self.learning_rate
# 
#             # Add emergent potential field data if available
#             if hasattr(self, 'emergent_potential_field'):
#                 field_state = self.emergent_potential_field.get_field_state()
#                 summary['emergent_potential'] = {
#                     'total_potential': field_state.get('total_potential', 0.0),
#                     'emergence_probability': field_state.get('emergence_probability', 0.0),
#                     'emergence_active': field_state.get('emergence_active', False),
#                     'field_intensity': field_state.get('field_intensity', 1.0)
#                 }
# 
#             return summary
#         except Exception as e:
#             print(f"Error getting state summary: {e}")
#             # Return minimal state information
#             return {
#                 'distinction_level': getattr(self, 'distinction_level', 0.5),
#                 'error': str(e)
#             }
# 
#     def _initialize_transformer(self) -> nn.Module:
#         """Initialize transformer with 4D input adapter."""
#         print("Initializing transformer with 4D input adapter...")
# 
#         try:
#             # Create base transformer
#             base_transformer = RecursiveDistinctionTransformer(
#                 input_size=20,
#                 d_model=20,
#                 nhead=NUM_TRANSFORMER_HEADS,
#                 num_layers=NUM_TRANSFORMER_LAYERS,
#                 output_size=1
#             ).to(DEVICE)
# 
#             # Initialize weights
#             for p in base_transformer.parameters():
#                 if p.dim() > 1:
#                     nn.init.xavier_uniform_(p, gain=0.1)
# 
#             # Wrap with adapter
#             transformer = FourDimTransformerAdapter(base_transformer, merge_strategy="merge")
#             print("Transformer initialized with adaptive 4D handling.")
#             return transformer
# 
#         except Exception as e:
#             print(f"Error initializing transformer: {e}")
#             traceback.print_exc()
# 
#             # Create fallback transformer
#             fallback_transformer = nn.Sequential(
#                 nn.Linear(20, 20),
#                 nn.ReLU(),
#                 nn.Linear(20, 1),
#                 nn.Sigmoid()
#             ).to(DEVICE)
# 
#             print("⚠️ Using fallback transformer due to initialization error")
# 
#             # Wrap with basic adapter to handle 4D inputs
#             class BasicAdapter(nn.Module):
#                 def __init__(self, base_model):
#                     super().__init__()
#                     self.base_model = base_model
# 
#                 def forward(self, x):
#                     # Handle 4D inputs by flattening to 2D
#                     if x.dim() > 2:
#                         # Preserve batch dimension
#                         batch_size = x.size(0)
#                         # Flatten all other dimensions
#                         x = x.view(batch_size, -1)
#                         # Ensure last dimension is 20
#                         if x.size(-1) != 20:
#                             x = F.pad(x, (0, 20 - x.size(-1) % 20))[:, :20]
#                     # Create output structure similar to TransformerOutput
#                     output = self.base_model(x)
#                     # Structure to mimic TransformerOutput
#                     class SimpleOutput:
#                         def __init__(self, pred):
#                             self.prediction = pred
#                             self.phase_prediction = pred * 0.5
#                             self.value_estimate = pred * 0.8
#                             self.entropy = torch.tensor(0.5, device=pred.device)
#                             self.coherence_estimate = torch.tensor(0.5, device=pred.device)
#                             self.attention_weights = {}
# 
#                     return SimpleOutput(output)
# 
#             return BasicAdapter(fallback_transformer)
# 
# # =============================================================================
# # Final Agent Class
# # =============================================================================
# class EnhancedSingleAgentFinal(EnhancedSingleAgentBase):
#     """
#     Final enhanced agent class that integrates quantum state management,
#     recursive memory, transformer-based decision making, surplus regulation,
#     and advanced adaptation mechanisms.
#     """
#     def __init__(self, num_qubits: int = NUM_QUBITS_PER_AGENT):
#         try:
#             # Core components
#             self.num_qubits = num_qubits
#             self.quantum_state = EnhancedQuantumState(num_qubits)
#             self.distinction_dynamics = EnhancedDistinctionDynamics()
#             self.surplus_dynamics = EnhancedSurplusDynamics()
#             print(f"DEBUG: Surplus Dynamics Initialized - Surplus State: {self.surplus_dynamics.surplus_state}")
#             self.recursive_cognition = RecursiveCognitiveStructuring()
#             self.quantum_optimizer = EnhancedQuantumSelfOptimization(num_qubits)
#             self.memory = RecursiveDistinctionMemory(max_size=10000, hierarchy_levels=4)
#             self.error_recovery = EnhancedErrorRecovery(agent=self)
#             self.minimum_coherence = MINIMUM_COHERENCE_FLOOR
# 
#             # Validation and synchronization
#             self.state_validator = QuantumStateValidator()
#             self.sync_manager = StateSynchronizationManager(
#                 self.quantum_state,
#                 self.surplus_dynamics,
#                 self.distinction_dynamics
#             )
# 
#             # Create a base transformer and assert input size matches d_model
#             assert HIDDEN_DIM == 20, "ERROR: Transformer `d_model` must match input_size (20)"
#             base_transformer = RecursiveDistinctionTransformer(
#                 input_size=20,
#                 d_model=20,
#                 nhead=NUM_TRANSFORMER_HEADS,
#                 num_layers=NUM_TRANSFORMER_LAYERS
#             ).to(DEVICE)
# 
#             # Wrap with 4D adapter
#             self.transformer = FourDimTransformerAdapter(
#                 base_transformer,
#                 merge_strategy="merge"  # or "separate" if you prefer
#             )
# 
#             # Training components
#             self.training_pipeline = EnhancedTrainingPipeline(self.transformer)
# 
#             # Analysis components
#             self.analyzer = QuantumAnalyzer(num_qubits)
#             self.optimization_coordinator = OptimizationCoordinator(self)
# 
#             # State tracking
#             self.distinction_level = 0.5  # Start with median distinction
#             self.phase = 0.0
#             self.stability_factor = 1.0
#             self.learning_rate = LEARNING_RATE
# 
#             # Recovery mode
#             self.recovery_mode = False
#             self.recovery_steps = 0
# 
#             # Ensure initial synchronization
#             if not self.sync_manager.synchronize_states():
#                 print("Warning: Initial state synchronization failed, attempting recovery")
#                 self.error_recovery.initiate_full_recovery()
#                 if not self.sync_manager.synchronize_states():
#                     raise RuntimeError("Failed to achieve initial state synchronization")
# 
#         except Exception as e:
#             print(f"Error initializing agent: {e}")
#             traceback.print_exc()
#             raise
#

"""# 11. Training Pipeline"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile training_pipeline.py
# """
# Training Pipeline Module
# -----------------------
# This module provides classes for training the Émile-2 agent's neural networks,
# including optimizers, loss functions, error recovery, and state validation.
# """
# 
# import os
# import math
# import time
# import random
# import traceback
# import numpy as np
# from collections import deque, defaultdict
# from typing import Optional, Dict, Tuple, List, Any, Union, Callable
# 
# import torch
# import torch.nn as nn
# import torch.nn.functional as F
# 
# from qiskit import QuantumCircuit
# from qiskit.circuit import Parameter
# from qiskit.circuit.library import EfficientSU2
# from qiskit_aer.noise import NoiseModel, amplitude_damping_error, phase_damping_error
# from qiskit.quantum_info import Statevector
# from qiskit_aer.library import SaveStatevector
# 
# from data_classes import SurplusState, TransformerOutput
# from utilities import (
#     MOMENTUM_DECAY,
#     NUM_QUBITS_PER_AGENT,
#     MINIMUM_COHERENCE_FLOOR,
#     DISTINCTION_ANCHOR_WEIGHT,
#     update_momentum,
#     compute_phase_coherence,
#     LEARNING_RATE,
#     LEARNING_RATE_MIN,
#     LEARNING_RATE_MAX,
#     WEIGHT_DECAY,
#     GRADIENT_CLIP_VALUE,
#     NUM_TRANSFORMER_HEADS,
#     NUM_TRANSFORMER_LAYERS,
#     HIDDEN_DIM
# )
# 
# # Determine if CUDA (GPU) is available, otherwise use CPU
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# 
# # -----------------------------------------------------------------------------
# # Transformer Wrapper
# # -----------------------------------------------------------------------------
# class EnhancedTransformerWrapper(nn.Module):
#     """
#     Wrapper to ensure consistent transformer output by converting any output type
#     to a standardized TransformerOutput object.
#     """
#     def __init__(self, base_transformer: nn.Module):
#         super().__init__()
#         self.base_transformer = base_transformer
# 
#     def forward(self, x: torch.Tensor, phase: Optional[torch.Tensor] = None) -> TransformerOutput:
#         """
#         Forward pass that ensures output is always a TransformerOutput object.
#         Handles various output types from the base transformer.
#         """
#         try:
#             # Get base output
#             base_output = self.base_transformer(x, phase)
# 
#             # If already TransformerOutput, return as is
#             if isinstance(base_output, TransformerOutput):
#                 return base_output
# 
#             # If tensor, wrap in TransformerOutput
#             if isinstance(base_output, torch.Tensor):
#                 return TransformerOutput(
#                     prediction=base_output,
#                     phase_prediction=phase,
#                     value_estimate=torch.tensor(0.0, device=base_output.device),
#                     attention_weights={},
#                     entropy=torch.tensor(0.0, device=base_output.device),
#                     coherence_estimate=torch.tensor(0.0, device=base_output.device)
#                 )
# 
#             # If dict, convert to TransformerOutput
#             if isinstance(base_output, dict):
#                 device = self.get_device()
#                 return TransformerOutput(
#                     prediction=base_output.get('prediction', torch.zeros(1, device=device)),
#                     phase_prediction=base_output.get('phase_prediction', None),
#                     value_estimate=base_output.get('value_estimate', None),
#                     attention_weights=base_output.get('attention_weights', {}),
#                     entropy=base_output.get('entropy', None),
#                     coherence_estimate=base_output.get('coherence_estimate', None)
#                 )
# 
#             raise ValueError(f"Unexpected output type: {type(base_output)}")
# 
#         except Exception as e:
#             print(f"Error in transformer forward pass: {e}")
#             traceback.print_exc()
#             return self._create_default_output()
# 
#     def _create_default_output(self) -> TransformerOutput:
#         """Create default output to use as fallback on error."""
#         device = self.get_device()
#         return TransformerOutput(
#             prediction=torch.zeros(1, device=device),
#             phase_prediction=torch.tensor(0.0, device=device),
#             value_estimate=torch.tensor(0.0, device=device),
#             attention_weights={},
#             entropy=torch.tensor(0.0, device=device),
#             coherence_estimate=torch.tensor(0.0, device=device)
#         )
# 
#     def get_device(self) -> torch.device:
#         """Get device of base transformer."""
#         try:
#             return next(self.base_transformer.parameters()).device
#         except Exception:
#             return torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 
# # -----------------------------------------------------------------------------
# # Metric Validator
# # -----------------------------------------------------------------------------
# class MetricValidator:
#     """
#     Validates and cleans metrics dictionaries, ensuring all required metrics
#     are present with valid values.
#     """
#     @staticmethod
#     def validate_metrics(metrics: Dict[str, float]) -> Dict[str, float]:
#         """
#         Validate and clean metrics dictionary with better error handling.
#         Ensures all required metrics have valid values.
#         """
#         try:
#             if not isinstance(metrics, dict):
#                 print("Warning: Metrics is not a dictionary, creating new one")
#                 metrics = {}
# 
#             # Define required metrics with defaults
#             default_metrics = {
#                 'phase_coherence': 0.5,
#                 'normalized_entropy': 0.5,
#                 'stability': 1.0,
#                 'quantum_coupling': 1.0,
#                 'phase': 0.0,
#                 'phase_distinction': 0.5,
#                 'coherence_distinction': 0.5,
#                 'quantum_surplus_coupling': 1.0
#             }
# 
#             validated_metrics = {}
# 
#             # Process each metric with proper validation
#             for key, default in default_metrics.items():
#                 try:
#                     if key in metrics:
#                         value = float(metrics[key])
#                         # Bound specific metrics
#                         if key in ['phase_coherence', 'normalized_entropy', 'stability', 'quantum_coupling']:
#                             value = np.clip(value, 0.0, 1.0)
#                         validated_metrics[key] = value
#                     else:
#                         print(f"Warning: Missing required metric {key}, using default value")
#                         validated_metrics[key] = default
#                 except (TypeError, ValueError) as e:
#                     print(f"Warning: Invalid value for {key}, using default. Error: {e}")
#                     validated_metrics[key] = default
# 
#             # Include any additional metrics not in defaults
#             for key, value in metrics.items():
#                 if key not in validated_metrics:
#                     try:
#                         validated_metrics[key] = float(value)
#                     except (TypeError, ValueError):
#                         print(f"Warning: Could not convert {key} to float, skipping")
# 
#             return validated_metrics
# 
#         except Exception as e:
#             print(f"Error in metric validation: {e}")
#             traceback.print_exc()
#             return default_metrics
# 
# # -----------------------------------------------------------------------------
# # Quantum-Aware Optimizer
# # -----------------------------------------------------------------------------
# class QuantumAwareOptimizer(torch.optim.Optimizer):
#     """
#     Optimizer that wraps an AdamW optimizer and rescales gradients based on
#     a moving average and variance of the gradient norms, with quantum-aware
#     adjustment based on coherence and stability metrics.
#     """
#     def __init__(self, model=None, device=None, learning_rate=1e-4, momentum_decay=0.7, gradient_clip_value=1.0):
#         """
#         Initialize the optimizer with learning rate and gradient tracking.
# 
#         Args:
#             model: The neural network model being optimized.
#             device: The computational device (CPU/GPU).
#             learning_rate: Learning rate for the optimizer (default: 1e-4)
#             momentum_decay: Decay factor for momentum calculations (default: 0.7)
#             gradient_clip_value: Maximum norm for gradient clipping (default: 1.0)
#         """
#         self.model = model
#         self.device = device if device is not None else torch.device('cuda' if torch.cuda.is_available() else 'cpu')
# 
#         # Store constants as instance variables for dynamic adjustment
#         self.learning_rate = learning_rate
#         self.momentum_decay = momentum_decay
#         self.gradient_clip_value = gradient_clip_value
# 
#         # Initialize parameters list correctly
#         if hasattr(self.model, 'parameters'):
#             try:
#                 # Convert parameters to list explicitly
#                 params = [p for p in self.model.parameters()]
#             except:
#                 params = []
#         else:
#             params = []
# 
#         if not params:
#             print("Warning: No parameters found in model for optimization.")
# 
#         # Create inner optimizer with default parameters if needed
#         self.optimizer = torch.optim.Adam(
#             [{'params': params}] if params else [{'params': [torch.nn.Parameter(torch.zeros(1))]}],
#             lr=self.learning_rate,
#             betas=(0.9, 0.999)
#         )
# 
#         # Gradient tracking dictionaries
#         self.grad_moving_avg = {}
#         self.grad_variance = {}
#         self.grad_momentum = {}
# 
#         # Stability tracking
#         self.stability_factor = 1.0
#         self.update_history = deque(maxlen=1000)
#         self.grad_norm_history = deque(maxlen=1000)
# 
#         # Recovery attributes
#         self.recovery_mode = False
#         self.recovery_steps = 0
#         self.recovery_lr_scale = 0.1
#         self.consecutive_failures = 0
#         self.max_failures = 5
# 
#     def step(self, loss: torch.Tensor, metrics: Dict[str, float], closure: Optional[callable] = None) -> None:
#         """
#         Take an optimization step with enhanced error handling and gradient tracking.
# 
#         Args:
#             loss: The loss tensor to optimize
#             metrics: Dictionary of metrics used to adjust optimization
#             closure: Optional closure for computing loss (used by some optimizers)
#         """
#         try:
#             # Ensure the loss requires gradient
#             if not loss.requires_grad:
#                 print("Warning: Loss does not require gradient. Creating differentiable copy.")
#                 loss = loss.detach().requires_grad_(True)
# 
#             # Compute gradients
#             loss.backward(retain_graph=True)
# 
#             with torch.no_grad():
#                 # Track gradient statistics
#                 total_grad_norm = 0.0
#                 param_count = 0
# 
#                 # Process each parameter's gradients
#                 for name, param in self.model.named_parameters():
#                     if param.requires_grad:
#                         param_count += 1
#                         if param.grad is not None:
#                             grad_norm = param.grad.norm().item()
#                             total_grad_norm += grad_norm ** 2
# 
#                             # Initialize tracking if needed
#                             if name not in self.grad_moving_avg:
#                                 self.grad_moving_avg[name] = grad_norm
#                                 self.grad_variance[name] = 0.0
#                                 self.grad_momentum[name] = 0.0
# 
#                             # Update tracking metrics
#                             self.grad_moving_avg[name] = (
#                                 self.momentum_decay * self.grad_moving_avg[name] +
#                                 (1 - self.momentum_decay) * grad_norm
#                             )
# 
#                             grad_diff = grad_norm - self.grad_moving_avg[name]
#                             self.grad_variance[name] = (
#                                 self.momentum_decay * self.grad_variance[name] +
#                                 (1 - self.momentum_decay) * (grad_diff ** 2)
#                             )
# 
#                             self.grad_momentum[name] = (
#                                 self.momentum_decay * self.grad_momentum[name] +
#                                 (1 - self.momentum_decay) * grad_diff
#                             )
# 
#                             # Scale gradients based on metrics
#                             stability = metrics.get('stability', 1.0)
#                             coherence = metrics.get('phase_coherence', 0.5)
# 
#                             base_scale = 1.0 / (1.0 + math.sqrt(self.grad_variance[name]))
#                             momentum_scale = 1.0 + 0.1 * self.grad_momentum[name]
#                             metric_scale = 1.0 + 0.1 * (stability * coherence - 0.5)
# 
#                             final_scale = base_scale * momentum_scale * metric_scale
#                             param.grad.mul_(final_scale)
# 
#                 if param_count == 0:
#                     print("Warning: No parameters require gradients")
#                     return
# 
#                 total_norm = 0.0
#                 for p in self.model.parameters():
#                     if p.grad is not None:
#                         param_norm = p.grad.data.norm(2)  # L2 norm
#                         total_norm += param_norm.item() ** 2
#                 total_norm = total_norm ** 0.5
#                 print(f"DEBUG: Total gradient norm (before clipping): {total_norm:.4f}")
# 
#                 # Clip gradients
#                 torch.nn.utils.clip_grad_norm_(
#                     [p for p in self.model.parameters() if p.requires_grad],
#                     self.gradient_clip_value
#                 )
# 
#                 # Update parameters
#                 self.optimizer.step()
#                 self.optimizer.zero_grad()
# 
#                 # Store update history
#                 self.update_history.append({
#                     'timestamp': time.time(),
#                     'grad_norm': float(total_grad_norm),
#                     'loss': float(loss.item()),
#                     'stability': float(stability) if 'stability' in locals() else 1.0,
#                     'coherence': float(coherence) if 'coherence' in locals() else 0.5
#                 })
# 
#                 # Reset failure counter on successful update
#                 self.consecutive_failures = 0
# 
#         except Exception as e:
#             print(f"Error in optimizer step: {e}")
#             traceback.print_exc()
# 
#             # Track failures
#             self.consecutive_failures += 1
#             if self.consecutive_failures >= self.max_failures and not self.recovery_mode:
#                 self.enter_recovery_mode()
# 
#     def enter_recovery_mode(self):
#         """Enters recovery mode when training instability is detected."""
#         self.recovery_mode = True
#         self.recovery_steps = 50
# 
#         # Reduce learning rate
#         for param_group in self.optimizer.param_groups:
#             param_group['lr'] *= self.recovery_lr_scale
# 
#         print("Entering optimizer recovery mode.")
# 
#         # Reset momentum
#         for name in self.grad_momentum:
#             self.grad_momentum[name] *= 0.1
# 
#     def __getattr__(self, attr):
#         """Delegate attribute access to the underlying optimizer."""
#         try:
#             return getattr(self.optimizer, attr)
#         except AttributeError:
#             raise AttributeError(f"'{self.__class__.__name__}' object has no attribute '{attr}'")
# 
#     def get_optimizer_stats(self) -> Dict[str, float]:
#         """Retrieve statistics about the optimizer's state."""
#         try:
#             recent_history = list(self.update_history)[-100:]
#             stats = {
#                 'mean_grad_norm': np.mean([h['grad_norm'] for h in recent_history]) if recent_history else 0.0,
#                 'grad_norm_std': np.std([h['grad_norm'] for h in recent_history]) if recent_history else 0.0,
#                 'stability_factor': self.stability_factor,
#                 'recovery_mode': self.recovery_mode,
#                 'recovery_steps': self.recovery_steps if self.recovery_mode else 0,
#                 'current_lr': self.optimizer.param_groups[0]['lr'],
#                 'consecutive_failures': self.consecutive_failures
#             }
# 
#             if self.grad_momentum:
#                 momentum_values = list(self.grad_momentum.values())
#                 stats.update({
#                     'mean_momentum': float(np.mean(momentum_values)),
#                     'momentum_std': float(np.std(momentum_values))
#                 })
# 
#             return stats
#         except Exception as e:
#             print(f"Error getting optimizer stats: {e}")
#             traceback.print_exc()
#             return {}
# 
#     def process_recovery(self):
#         """Handle recovery mode processing and exit."""
#         if not self.recovery_mode:
#             return
# 
#         self.recovery_steps -= 1
# 
#         # Exit recovery mode if steps completed
#         if self.recovery_steps <= 0:
#             self.recovery_mode = False
# 
#             # Gradually restore learning rate
#             for param_group in self.optimizer.param_groups:
#                 param_group['lr'] = min(
#                     param_group['lr'] * 2.0,
#                     self.learning_rate
#                 )
# 
#             print("Exiting optimizer recovery mode")
# 
# # -----------------------------------------------------------------------------
# # Enhanced Loss Function
# # -----------------------------------------------------------------------------
# class EnhancedLossFunction:
#     """
#     Enhanced loss function with proper TransformerOutput handling and component-wise losses
#     that are weighted by quantum coherence and stability metrics.
#     """
#     def __init__(self):
#         """Initialize the loss function with tracking history."""
#         self.criterion = nn.MSELoss()
#         self.loss_history = deque(maxlen=1000)
# 
#     def compute_loss(self, output: TransformerOutput,
#                     targets: Dict[str, torch.Tensor],
#                     metrics: Dict[str, float]) -> Tuple[torch.Tensor, Dict[str, float]]:
#         """
#         Compute loss with TransformerOutput handling.
# 
#         Args:
#             output: The transformer output object
#             targets: Dictionary of target tensors
#             metrics: Dictionary of metrics for weighting
# 
#         Returns:
#             Tuple of (total loss tensor, dictionary of loss components)
#         """
#         try:
#             device = output.prediction.device
# 
#             # Get prediction from TransformerOutput
#             prediction = output.prediction
#             if prediction.dim() == 3:  # If (batch, seq_len, feature)
#                 prediction = prediction.squeeze(-1)
# 
#             # Get target tensor
#             target = targets.get('distinction')
#             if target is None:
#                 print("Warning: No distinction target provided")
#                 target = torch.zeros_like(prediction)
# 
#             # Ensure shapes match
#             if target.shape != prediction.shape:
#                 target = target.view_as(prediction)
# 
#             # Compute main distinction loss
#             distinction_loss = self.criterion(prediction, target)
# 
#             # Compute auxiliary losses if available
#             phase_loss = torch.tensor(0.0, device=device, requires_grad=True)
#             if output.phase_prediction is not None and 'phase' in targets:
#                 phase_pred = output.phase_prediction.squeeze(-1)
#                 phase_target = targets['phase'].view_as(phase_pred)
#                 phase_loss = self.criterion(phase_pred, phase_target)
# 
#             value_loss = torch.tensor(0.0, device=device, requires_grad=True)
#             if output.value_estimate is not None and 'value' in targets:
#                 value_pred = output.value_estimate.squeeze(-1)
#                 value_target = targets['value'].view_as(value_pred)
#                 value_loss = self.criterion(value_pred, value_target)
# 
#             # Weight losses based on metrics
#             coherence = metrics.get('phase_coherence', 0.5)
#             stability = metrics.get('stability', 0.5)
# 
#             # Combine losses with weights
#             total_loss = (
#                 distinction_loss +
#                 0.1 * phase_loss * coherence +
#                 0.1 * value_loss * stability
#             )
# 
#             # Collect loss components for tracking
#             loss_components = {
#                 'total_loss': float(total_loss.item()),
#                 'distinction_loss': float(distinction_loss.item()),
#                 'phase_loss': float(phase_loss.item()),
#                 'value_loss': float(value_loss.item())
#             }
# 
#             # Store history
#             self.loss_history.append({
#                 'components': loss_components,
#                 'metrics': metrics,
#                 'timestamp': time.time()
#             })
# 
#             return total_loss, loss_components
# 
#         except Exception as e:
#             print(f"Error computing loss: {e}")
#             traceback.print_exc()
# 
#             # Return safe default
#             default_loss = torch.tensor(1.0, device=output.prediction.device, requires_grad=True)
#             return default_loss, {'total_loss': 1.0, 'error': str(e)}
# 
#     def get_loss_stats(self) -> Dict[str, float]:
#         """Get statistics about recent losses."""
#         try:
#             if not self.loss_history:
#                 return {'no_history': True}
# 
#             recent = list(self.loss_history)[-100:]
# 
#             return {
#                 'mean_total_loss': np.mean([r['components']['total_loss'] for r in recent]),
#                 'mean_distinction_loss': np.mean([r['components']['distinction_loss'] for r in recent]),
#                 'mean_phase_loss': np.mean([r['components']['phase_loss'] for r in recent]),
#                 'mean_value_loss': np.mean([r['components']['value_loss'] for r in recent]),
#                 'loss_std': np.std([r['components']['total_loss'] for r in recent]),
#                 'recent_loss_trend': np.mean(np.diff([r['components']['total_loss'] for r in recent][-10:]))
#                 if len(recent) >= 10 else 0.0
#             }
#         except Exception as e:
#             print(f"Error getting loss stats: {e}")
#             return {'error': str(e)}
# 
# # -----------------------------------------------------------------------------
# # Training Pipeline
# # -----------------------------------------------------------------------------
# class EnhancedTrainingPipeline:
#     """
#     Handles experience buffering, batch preparation, and training steps.
#     Tracks loss, gradient norms, and other training statistics.
#     """
#     def __init__(self, model: nn.Module, batch_size: int = 32, buffer_size: int = 10000):
#         """
#         Initialize the training pipeline with enhanced validation.
# 
#         Args:
#             model: The model to train
#             batch_size: Batch size for training
#             buffer_size: Maximum size of experience buffer
#         """
#         # Validate model is a proper nn.Module
#         if not isinstance(model, nn.Module):
#             raise TypeError(f"Model must be an instance of nn.Module, got {type(model)}")
# 
#         # Validate batch and buffer sizes
#         if batch_size <= 0:
#             raise ValueError(f"Batch size must be positive, got {batch_size}")
#         if buffer_size <= batch_size:
#             raise ValueError(f"Buffer size must be greater than batch size, got {buffer_size} <= {batch_size}")
# 
#         self.model = model
#         self.transformer = model  # Alias for clarity
#         self.batch_size = batch_size
#         self.buffer_size = buffer_size
# 
#         # Initialize optimizer and loss function
#         self.optimizer = QuantumAwareOptimizer(model)
#         self.loss_function = EnhancedLossFunction()
# 
#         # Experience buffers
#         self.experience_buffer = deque(maxlen=buffer_size)
#         self.priority_buffer = []
#         self.priority_threshold = 0.8
# 
#         # Validation and metrics
#         self.metric_validator = MetricValidator()
# 
#         # Learning rate and training stats
#         self.learning_rate = LEARNING_RATE
#         self.training_stats = {
#             'loss_history': [],
#             'gradient_stats': [],
#             'priority_stats': [],
#             'stability_metrics': []
#         }
# 
#         # Training state
#         self.update_counter = 0
#         self.stability_factor = 1.0
#         self.training_momentum = 0.0
#         self.recovery_mode = False
#         self.recovery_steps = 0
#         self.min_experiences_for_training = batch_size * 2
# 
#         # Stability and recovery settings
#         self.stability_threshold = 0.3
#         self.consecutive_failures = 0
#         self.max_failures = 5
# 
#         # Create learning rate scheduler
#         self.lr_scheduler = EnhancedLearningRateScheduler(self.optimizer)
# 
#         # Log initialization
#         print(f"Training pipeline initialized with batch size {batch_size}, buffer size {buffer_size}")
#         print(f"Model architecture: {model.__class__.__name__}")
# 
#     def add_experience(self, experience: Dict[str, Any], priority: Optional[float] = None) -> None:
#         """
#         Add an experience to the buffer, optionally with priority.
# 
#         Args:
#             experience: Dictionary containing experience data
#             priority: Optional priority value (higher = more important)
#         """
#         try:
#             if not experience:
#                 return
# 
#             # Calculate priority if not provided
#             if priority is None:
#                 priority = self._compute_experience_priority(experience)
# 
#             # Add to priority buffer if above threshold
#             if priority > self.priority_threshold:
#                 self.priority_buffer.append({
#                     'experience': experience,
#                     'priority': priority,
#                     'timestamp': time.time()
#                 })
#                 # Sort by priority (highest first) and limit size
#                 self.priority_buffer.sort(key=lambda x: x['priority'], reverse=True)
#                 self.priority_buffer = self.priority_buffer[:self.buffer_size // 10]
#             else:
#                 # Otherwise add to regular buffer
#                 self.experience_buffer.append(experience)
# 
#             # Track priority stats
#             self.training_stats['priority_stats'].append({
#                 'priority': priority,
#                 'buffer_type': 'priority' if priority > self.priority_threshold else 'normal'
#             })
# 
#         except Exception as e:
#             print(f"Error adding experience: {e}")
#             traceback.print_exc()
# 
#     def adjust_training_with_cognitive_state(self, cognitive_state: Dict[str, float]) -> None:
#         """
#         Adjust training parameters based on cognitive state metrics.
# 
#         Args:
#             cognitive_state: Dictionary with cognitive state metrics
#         """
#         try:
#             # Extract relevant metrics
#             stability = cognitive_state.get('mean_stability', 0.5)
#             collapse_probability = cognitive_state.get('collapse_probability', 0.0)
#             feedback_strength = cognitive_state.get('mean_feedback_strength', 0.5)
#             quantum_influence = cognitive_state.get('quantum_influence', 0.5)
# 
#             # Adjust learning rate based on stability and collapse probability
#             current_lr = self.optimizer.param_groups[0]['lr']
# 
#             if collapse_probability > 0.3:
#                 # Reduce learning rate when collapse is likely
#                 new_lr = current_lr * 0.8
#                 print(f"Reducing learning rate due to high collapse probability: {current_lr:.6f} -> {new_lr:.6f}")
#             elif stability > 0.7 and feedback_strength > 0.6:
#                 # Slightly increase learning rate when very stable
#                 new_lr = min(current_lr * 1.05, LEARNING_RATE_MAX)
#                 print(f"Increasing learning rate due to high stability: {current_lr:.6f} -> {new_lr:.6f}")
#             else:
#                 # Maintain current learning rate with slight adjustments
#                 momentum_factor = 1.0 + 0.1 * (quantum_influence - 0.5)
#                 new_lr = current_lr * momentum_factor
# 
#             # Apply the new learning rate
#             for param_group in self.optimizer.param_groups:
#                 param_group['lr'] = np.clip(new_lr, LEARNING_RATE_MIN, LEARNING_RATE_MAX)
# 
#             # Adjust gradient clipping based on stability
#             if stability < 0.3:
#                 # More aggressive clipping for unstable states
#                 self.gradient_clip_value = GRADIENT_CLIP_VALUE * 0.7
#             elif stability > 0.8:
#                 # Less aggressive clipping for stable states
#                 self.gradient_clip_value = GRADIENT_CLIP_VALUE * 1.2
#             else:
#                 # Default clipping
#                 self.gradient_clip_value = GRADIENT_CLIP_VALUE
# 
#         except Exception as e:
#             print(f"Error adjusting training with cognitive state: {e}")
# 
#     def _compute_experience_priority(self, experience: Dict[str, Any]) -> float:
#         """
#         Compute priority value for an experience based on its characteristics.
# 
#         Args:
#             experience: The experience dictionary
# 
#         Returns:
#             Priority value between 0.0 and 1.0
#         """
#         try:
#             priority_factors = []
# 
#             # Prediction error
#             if 'prediction' in experience and 'actual' in experience:
#                 priority_factors.append(abs(experience['prediction'] - experience['actual']))
# 
#             # Distinction change
#             if 'distinction_level' in experience and 'next_distinction' in experience:
#                 priority_factors.append(abs(experience['next_distinction'] - experience['distinction_level']))
# 
#             # Quantum metrics influence
#             if 'quantum_metrics' in experience:
#                 metrics = experience['quantum_metrics']
#                 coherence = metrics.get('phase_coherence', 0.5)
#                 entropy = metrics.get('normalized_entropy', 0.5)
#                 priority_factors.append(coherence * (1 - entropy))
# 
#             return float(np.mean(priority_factors)) if priority_factors else 0.5
# 
#         except Exception as e:
#             print(f"Error computing experience priority: {e}")
#             traceback.print_exc()
#             return 0.5
# 
#     def prepare_batch(self, experiences: List[Dict]) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
#         """
#         Prepare a training batch from a list of experiences with robust handling of emergent dimensions.
# 
#         Args:
#             experiences: List of experience dictionaries
# 
#         Returns:
#             Tuple of (input tensor, target tensors dictionary)
#         """
#         try:
#             if not experiences:
#                 return self._create_empty_batch()
# 
#             states = []
#             targets = {'distinction': [], 'phase': [], 'value': []}
# 
#             # Extract data from experiences
#             for exp in experiences:
#                 state = exp.get('state')
# 
#                 # Skip invalid states
#                 if not isinstance(state, (np.ndarray, list, torch.Tensor)):
#                     print(f"Warning: Invalid state type: {type(state)}")
#                     continue
# 
#                 # Handle potential emergent dimensions
#                 has_emergent_dim = exp.get('has_emergent_dim', False)
# 
#                 # Append state with metadata about its shape
#                 states.append((state, has_emergent_dim))
# 
#                 # Extract targets with safe defaults
#                 targets['distinction'].append(float(exp.get('actual', 0.0)))
#                 # Extract phase from quantum_metrics or use default
#                 if 'quantum_metrics' in exp and 'phase' in exp['quantum_metrics']:
#                     targets['phase'].append(float(exp['quantum_metrics']['phase']))
#                 else:
#                     targets['phase'].append(0.0)
#                 targets['value'].append(float(exp.get('reward', 0.0)))
# 
#             if not states:
#                 return self._create_empty_batch()
# 
#             # Process states and handle inhomogeneous dimensions
#             processed_states = []
#             for state_data, has_emergent_dim in states:
#                 # Convert to numpy if it's a tensor
#                 if isinstance(state_data, torch.Tensor):
#                     state_data = state_data.cpu().numpy()
# 
#                 # Make state data have consistent shape by reshaping emergent dimensions
#                 if has_emergent_dim or len(np.array(state_data).shape) > 3:
#                     # Handle 4D or higher states by flattening extra dimensions
#                     state_array = np.array(state_data)
#                     # Get original shape
#                     original_shape = state_array.shape
# 
#                     if len(original_shape) > 3:
#                         # Reshape to [batch, seq_len, features]
#                         # Flatten all dimensions between batch and features
#                         flat_seq_len = np.prod(original_shape[1:-1])
#                         reshaped_state = state_array.reshape(original_shape[0], flat_seq_len, original_shape[-1])
#                         processed_states.append(reshaped_state)
#                     else:
#                         processed_states.append(state_array)
#                 else:
#                     # Regular 3D state - no reshaping needed
#                     processed_states.append(np.array(state_data))
# 
#             # Now that all shapes are standardized, find the maximum sequence length
#             max_seq_len = max(state.shape[1] if len(state.shape) > 1 else 1 for state in processed_states)
#             feature_dim = processed_states[0].shape[-1] if processed_states and len(processed_states[0].shape) > 0 else 20
# 
#             # Pad all states to the same sequence length
#             padded_states = []
#             for state in processed_states:
#                 if len(state.shape) < 2:
#                     # Handle 1D states
#                     padded = np.zeros((1, max_seq_len, feature_dim))
#                     padded[0, 0, :min(state.shape[0], feature_dim)] = state[:min(state.shape[0], feature_dim)]
#                     padded_states.append(padded)
#                 elif len(state.shape) < 3:
#                     # Handle 2D states
#                     padded = np.zeros((1, max_seq_len, feature_dim))
#                     padded[0, :min(state.shape[0], max_seq_len), :min(state.shape[1], feature_dim)] = \
#                         state[:min(state.shape[0], max_seq_len), :min(state.shape[1], feature_dim)]
#                     padded_states.append(padded)
#                 else:
#                     # Handle 3D states
#                     padded = np.zeros((state.shape[0], max_seq_len, feature_dim))
#                     padded[:, :min(state.shape[1], max_seq_len), :min(state.shape[2], feature_dim)] = \
#                         state[:, :min(state.shape[1], max_seq_len), :min(state.shape[2], feature_dim)]
#                     padded_states.append(padded)
# 
#             # Stack all states together
#             states_array = np.vstack(padded_states)
# 
#             # Convert to tensors
#             states_tensor = torch.tensor(states_array, dtype=torch.float32, device=DEVICE)
# 
#             # Ensure feature dimension is exactly 20
#             if states_tensor.size(-1) != 20:
#                 if states_tensor.size(-1) < 20:
#                     # Pad features
#                     padding = (0, 20 - states_tensor.size(-1), 0, 0, 0, 0)
#                     states_tensor = F.pad(states_tensor, padding)
#                 else:
#                     # Truncate features
#                     states_tensor = states_tensor[..., :20]
# 
#             # Convert targets to tensors
#             target_tensors = {
#                 key: torch.tensor(value, dtype=torch.float32, device=DEVICE).view(-1, 1)
#                 for key, value in targets.items()
#             }
# 
#             return states_tensor, target_tensors
# 
#         except Exception as e:
#             print(f"Error preparing batch: {e}")
#             traceback.print_exc()
#             return self._create_empty_batch()
# 
#     def _create_empty_batch(self) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
#         """Create an empty batch as a fallback."""
#         empty_state = torch.zeros((1, 1, 20), dtype=torch.float32, device=DEVICE)
#         empty_targets = {
#             'distinction': torch.zeros((1, 1), dtype=torch.float32, device=DEVICE),
#             'phase': torch.zeros((1, 1), dtype=torch.float32, device=DEVICE),
#             'value': torch.zeros((1, 1), dtype=torch.float32, device=DEVICE)
#         }
#         return empty_state, empty_targets
# 
#     def _validate_batch(self, experiences: List[Dict]) -> bool:
#         """
#         Validate a batch of experiences to ensure they have required fields.
# 
#         Args:
#             experiences: List of experience dictionaries
# 
#         Returns:
#             True if valid, False otherwise
#         """
#         try:
#             if not experiences:
#                 return False
# 
#             required_keys = ['state', 'prediction', 'actual', 'quantum_metrics']
# 
#             for exp in experiences:
#                 # Check all required keys exist
#                 if not all(key in exp for key in required_keys):
#                     print(f"Missing required keys in experience: {exp.keys()}")
#                     return False
# 
#                 # Check state is correct type
#                 if not isinstance(exp['state'], (np.ndarray, torch.Tensor)):
#                     print(f"Invalid state type: {type(exp['state'])}")
#                     return False
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error validating batch: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _handle_training_error(self, error: Exception):
#         """Handle training errors with potential recovery."""
#         print(f"Training error encountered: {error}")
#         traceback.print_exc()
# 
#         # Track error count
#         self.consecutive_failures = getattr(self, 'consecutive_failures', 0) + 1
# 
#         # Enter recovery if too many errors
#         if self.consecutive_failures > self.max_failures and not self.recovery_mode:
#             print("Multiple training errors detected. Entering recovery mode.")
#             self.enter_recovery_mode()
# 
#     def _update_training_stats(self, loss_components: Dict[str, float]):
#         """
#         Update training statistics with new loss values.
# 
#         Args:
#             loss_components: Dictionary of loss components
#         """
#         try:
#             # Initialize stats if not present
#             if not hasattr(self, 'training_stats'):
#                 self.training_stats = {
#                     'loss_history': [],
#                     'gradient_stats': [],
#                     'stability_metrics': []
#                 }
# 
#             # Add loss to history
#             self.training_stats['loss_history'].append(loss_components)
# 
#             # Update stability factor
#             current_stability = 1.0 / (1.0 + loss_components.get('total_loss', 0))
#             self.stability_factor = 0.95 * self.stability_factor + 0.05 * current_stability
# 
#             # Track stability
#             self.training_stats['stability_metrics'].append({
#                 'timestamp': time.time(),
#                 'stability_factor': self.stability_factor,
#                 'loss': loss_components.get('total_loss', 0.0)
#             })
# 
#         except Exception as e:
#             print(f"Error updating training stats: {e}")
#             traceback.print_exc()
# 
#     def _compute_gradient_norm(self) -> float:
#         """
#         Compute the total gradient norm across all parameters.
# 
#         Returns:
#             Total gradient norm as a float
#         """
#         try:
#             total_norm = 0.0
#             for param in self.model.parameters():
#                 if param.grad is not None:
#                     total_norm += param.grad.norm().item() ** 2
#             return float(np.sqrt(total_norm))
#         except Exception as e:
#             print(f"Error computing gradient norm: {e}")
#             traceback.print_exc()
#             return 0.0
# 
#     def _check_loss_stability(self, current_loss: float) -> bool:
#         """
#         Check if loss value is stable compared to recent history.
# 
#         Args:
#             current_loss: The current loss value
# 
#         Returns:
#             True if stable, False if unstable
#         """
#         try:
#             # Check for invalid loss
#             if not isinstance(current_loss, (int, float)) or math.isnan(current_loss) or math.isinf(current_loss):
#                 return False
# 
#             # Need enough history for comparison
#             if len(self.training_stats['loss_history']) < 10:
#                 return True
# 
#             # Get recent losses
#             recent_losses = [h.get('total_loss', 0.0) for h in self.training_stats['loss_history'][-10:]]
#             loss_mean = np.mean(recent_losses)
#             loss_std = np.std(recent_losses)
# 
#             # Check multiple stability conditions
#             return (
#                 current_loss <= loss_mean + 3 * loss_std and  # Not too high
#                 current_loss >= loss_mean - 3 * loss_std and  # Not too low
#                 current_loss < 10 * loss_mean  # Not diverging
#             )
# 
#         except Exception as e:
#             print(f"Error checking loss stability: {e}")
#             traceback.print_exc()
#             return False
# 
#     def enter_recovery_mode(self):
#         """Enter recovery mode to stabilize training."""
#         try:
#             self.recovery_mode = True
#             self.recovery_steps = 50
#             self.consecutive_failures = 0
# 
#             # Reset momentum
#             self.training_momentum = 0.0
# 
#             # Reduce learning rate
#             for param_group in self.optimizer.param_groups:
#                 param_group['lr'] *= 0.1
# 
#             print("Entering training recovery mode.")
# 
#             # Notify model component if it has a recovery mode
#             if hasattr(self.model, 'enter_recovery_mode'):
#                 self.model.enter_recovery_mode()
# 
#         except Exception as e:
#             print(f"Error entering recovery mode: {e}")
#             traceback.print_exc()
# 
#     def train_step(self, experiences: List[Dict], metrics: Dict[str, float]) -> Dict[str, float]:
#         """
#         Perform a single training step on a batch of experiences.
# 
#         Args:
#             experiences: List of experience dictionaries
#             metrics: Dictionary of metrics for the loss function
# 
#         Returns:
#             Dictionary of loss components
#         """
#         try:
#             # Validate metrics first
#             metrics = self.metric_validator.validate_metrics(metrics)
# 
#             # Skip training if in recovery mode
#             if self.recovery_mode:
#                 self.recovery_steps -= 1
#                 if self.recovery_steps <= 0:
#                     self.recovery_mode = False
#                     print("Exiting training recovery mode.")
#                 return {'skipped_update': True, 'recovery_steps': self.recovery_steps}
# 
#             # Prepare input batch
#             states, targets = self.prepare_batch(experiences)
#             if states.size(0) == 0:
#                 return {}
# 
#             # Set model to training mode
#             self.model.train()
#             self.update_counter += 1
# 
#             # Forward pass with error handling
#             try:
#                 # Get model output
#                 output = self.model(states, phase=targets.get('phase'))
# 
#                 # Validate output
#                 if not hasattr(output, 'prediction'):
#                     print("Warning: Model output doesn't have prediction attribute")
#                     self.enter_recovery_mode()
#                     return {'invalid_output': True}
# 
#                 # Validate prediction
#                 prediction = output.prediction
#                 if not isinstance(prediction, torch.Tensor):
#                     print("Warning: Prediction is not a tensor")
#                     self.enter_recovery_mode()
#                     return {'invalid_prediction': True}
# 
#                 # Check for stability before proceeding
#                 if self._check_loss_stability(prediction.mean().item()):
#                     # Compute loss - FIXED: ensure loss requires gradient
#                     loss, loss_components = self.loss_function.compute_loss(output, targets, metrics)
# 
#                     # Explicitly make loss require gradients if it doesn't already
#                     if not loss.requires_grad:
#                         loss = loss.detach().requires_grad_(True)
# 
#                     # Reset gradients and compute new ones
#                     self.optimizer.zero_grad()
# 
#                     # Manually call backward
#                     loss.backward()
# 
#                     # Compute gradient norm for tracking
#                     grad_norm = self._compute_gradient_norm()
#                     self.training_stats['gradient_stats'].append(grad_norm)
# 
#                     # Clip gradients to prevent explosions
#                     torch.nn.utils.clip_grad_norm_(self.transformer.parameters(), GRADIENT_CLIP_VALUE)
# 
#                     # Perform optimizer step
#                     for param_group in self.optimizer.param_groups:
#                         for param in param_group['params']:
#                             if param.grad is not None:
#                                 param.data.add_(param.grad, alpha=-LEARNING_RATE)
# 
#                     # Update stability tracking
#                     self.stability_factor = 0.95 * self.stability_factor + 0.05 * (1.0 / (1.0 + loss.item()))
#                     self.consecutive_failures = 0
# 
#                 else:
#                     print("Loss instability detected")
#                     self.consecutive_failures += 1
#                     if self.consecutive_failures >= self.max_failures:
#                         self.enter_recovery_mode()
#                     return {'loss_instability': True}
# 
#             except Exception as e:
#                 print(f"Error in forward/backward pass: {e}")
#                 traceback.print_exc()
#                 self.consecutive_failures += 1
#                 if self.consecutive_failures >= self.max_failures:
#                     self.enter_recovery_mode()
#                 return {}
# 
#             # Update training stats
#             self.training_stats['loss_history'].append(loss_components)
#             self.training_stats['stability_metrics'].append({
#                 'stability_factor': self.stability_factor,
#                 'training_momentum': self.training_momentum,
#                 'grad_norm': grad_norm if 'grad_norm' in locals() else 0.0
#             })
# 
#             return loss_components
# 
#         except Exception as e:
#             print(f"Error in training step: {e}")
#             traceback.print_exc()
#             self.enter_recovery_mode()
#             return {}
# 
#     def get_training_summary(self) -> Dict[str, float]:
#         """
#         Get a summary of training statistics.
# 
#         Returns:
#             Dictionary of training summary metrics
#         """
#         try:
#             if not self.training_stats['loss_history']:
#                 return {}
# 
#             # Get recent stats for summary
#             recent_losses = self.training_stats['loss_history'][-100:]
#             recent_grads = self.training_stats['gradient_stats'][-100:]
#             recent_stability = self.training_stats['stability_metrics'][-100:]
# 
#             # Compute summary metrics
#             summary = {
#                 'avg_total_loss': np.mean([l['total_loss'] for l in recent_losses]),
#                 'avg_distinction_loss': np.mean([l['distinction_loss'] for l in recent_losses]),
#                 'avg_gradient_norm': np.mean(recent_grads) if recent_grads else 0.0,
#                 'stability_factor': np.mean([m['stability_factor'] for m in recent_stability]) if recent_stability else 1.0,
#                 'training_momentum': self.training_momentum,
#                 'recovery_mode': self.recovery_mode,
#                 'recovery_steps': self.recovery_steps if self.recovery_mode else 0,
#                 'update_counter': self.update_counter
#             }
# 
#             # Add loss trend if enough data
#             if len(recent_losses) >= 10:
#                 recent_total_losses = [l['total_loss'] for l in recent_losses[-10:]]
#                 summary['recent_loss_trend'] = np.mean(np.diff(recent_total_losses))
# 
#             return summary
# 
#         except Exception as e:
#             print(f"Error getting training summary: {e}")
#             traceback.print_exc()
#             return {}
# 
# # -----------------------------------------------------------------------------
# # Error Recovery
# # -----------------------------------------------------------------------------
# class EnhancedErrorRecovery:
#     """Handles error recovery and state restoration for the agent."""
#     def __init__(self, agent: Any):
#         """
#         Initialize error recovery system.
# 
#         Args:
#             agent: The agent to manage recovery for
#         """
#         self.agent = agent
#         self.backup_manager = StateBackupManager()
#         self._error_counts = defaultdict(int)
#         self.recovery_attempts = 0
#         self.max_recovery_attempts = 3
#         self.min_stability_threshold = 0.3
#         self.recovery_delay = 0.1
#         self.last_recovery_time = time.time()
#         self.recovery_history = deque(maxlen=1000)
# 
#     @property
#     def error_counts(self) -> Dict[str, int]:
#         """Property getter for error_counts."""
#         return dict(self._error_counts)
# 
#     def handle_error(self, error: Exception, component: str) -> bool:
#         """
#         Handle component errors with recovery mechanisms.
# 
#         Args:
#             error: The exception that occurred
#             component: The component name where the error occurred
# 
#         Returns:
#             True if recovery succeeded, False otherwise
#         """
#         try:
#             current_time = time.time()
#             recovery_cooldown = 1.0  # seconds
# 
#             # Track error count
#             self._error_counts[component] += 1
#             logger.error(f"Error in {component}: {str(error)}")
# 
#             # Check if enough time has passed since last recovery
#             if current_time - self.last_recovery_time < recovery_cooldown:
#                 logger.warning("Recovery attempt too soon, waiting...")
#                 return False
# 
#             # Run system validation before attempting recovery
#             validation_results = self.backup_manager.state_validator.validate_system_state(self.agent)
# 
#             # Log validation results
#             logger.info("System validation results before recovery:")
#             for comp, status in validation_results.items():
#                 if comp != 'overall':
#                     logger.info(f"  - {comp}: {'PASSED' if status else 'FAILED'}")
# 
#             # Determine recovery strategy based on validation results
#             failed_components = [comp for comp, status in validation_results.items()
#                                 if not status and comp != 'overall']
# 
#             # If critical components have failed, perform full recovery
#             critical_components = ['quantum_state', 'surplus_state', 'distinction']
#             if any(comp in failed_components for comp in critical_components):
#                 logger.warning(f"Critical component failure detected: {failed_components}")
#                 success = self.initiate_full_recovery()
#             elif len(failed_components) > 2:
#                 # If multiple non-critical components have failed
#                 logger.warning(f"Multiple component failures detected: {failed_components}")
#                 success = self.initiate_full_recovery()
#             else:
#                 # Recover just the failed components
#                 success = True
#                 for failed_comp in failed_components:
#                     if not self.initiate_component_recovery(failed_comp):
#                         success = False
# 
#             self.last_recovery_time = current_time
#             return success
# 
#         except Exception as e:
#             logger.error(f"Error in error handling: {e}")
#             traceback.print_exc()
#             return False
# 
#     def initiate_component_recovery(self, component: str) -> bool:
#         """
#         Initialize recovery for a specific component.
# 
#         Args:
#             component: The component name to recover
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             print(f"Initiating recovery for component: {component}")
# 
#             # Component-specific recovery
#             if component == 'quantum_state':
#                 return self.recover_quantum_state()
#             elif component == 'surplus_dynamics':
#                 return self.recover_surplus_state()
#             elif component == 'transformer':
#                 return self.recover_training_state()
#             else:
#                 # Default component recovery strategy
#                 return False
# 
#         except Exception as e:
#             print(f"Error in component recovery: {e}")
#             traceback.print_exc()
#             return False
# 
#     def initiate_full_recovery(self) -> bool:
#         """
#         Initiate full system recovery with enhanced error handling.
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             print("\n🔄 Initiating full recovery sequence...")
# 
#             if self.recovery_attempts >= self.max_recovery_attempts:
#                 print("Maximum recovery attempts exceeded, performing full reinitialization...")
#                 return self._perform_full_reinitialization()
# 
#             self.recovery_attempts += 1
#             success = True
# 
#             # Step 1: Quantum State Recovery
#             print("Step 1: Recovering quantum state...")
#             if not self.recover_quantum_state():
#                 print("❌ Quantum state recovery failed")
#                 success = False
# 
#             # Step 2: Surplus State Recovery
#             print("Step 2: Recovering surplus state...")
#             if not self.recover_surplus_state():
#                 print("❌ Surplus state recovery failed")
#                 success = False
# 
#             # Step 3: Training State Recovery
#             print("Step 3: Recovering training state...")
#             if not self.recover_training_state():
#                 print("❌ Training state recovery failed")
#                 success = False
# 
#             # Record recovery attempt
#             self._record_recovery_attempt(success)
# 
#             if success:
#                 print("✅ Full recovery successful")
#                 self._error_counts.clear()
#                 self.recovery_attempts = 0
#                 return True
# 
#             print("❌ Recovery failed")
#             return False
# 
#         except Exception as e:
#             print(f"❌ Error in full recovery: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _perform_full_reinitialization(self) -> bool:
#         """
#         Perform complete agent reinitialization when other recovery attempts fail.
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             print("🔄 Performing full agent reinitialization...")
# 
#             # Store important parameters
#             num_qubits = self.agent.num_qubits
# 
#             # Create new instance - this requires agent to handle reinitialization
#             if hasattr(self.agent, '__init__'):
#                 self.agent.__init__(num_qubits=num_qubits)
#             else:
#                 print("❌ Agent doesn't have init method, can't reinitialize")
#                 return False
# 
#             # Validate initialization if method exists
#             if hasattr(self.agent, '_validate_initialization'):
#                 if not self.agent._validate_initialization():
#                     print("❌ Reinitialization validation failed")
#                     return False
# 
#             self.recovery_attempts = 0
#             print("✅ Agent reinitialized successfully")
#             return True
# 
#         except Exception as e:
#             print(f"❌ Error in agent reinitialization: {e}")
#             traceback.print_exc()
#             return False
# 
#     def recover_quantum_state(self) -> bool:
#         """
#         Recover quantum state with enhanced validation.
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             # First try to restore from backup
#             backup = self.backup_manager.restore_state(
#                 self.agent.quantum_state,
#                 restore_point='most_stable'
#             )
# 
#             if backup and isinstance(backup, dict):
#                 try:
#                     # Apply backup state
#                     self.agent.quantum_state.statevector = backup['statevector']
#                     self.agent.quantum_state.phase_coherence = backup.get('phase_coherence', MINIMUM_COHERENCE_FLOOR)
#                     self.agent.distinction_level = backup.get('distinction_level', 0.5)
# 
#                     # Validate restored state
#                     metrics = self.agent.quantum_state.get_quantum_metrics()
#                     if metrics.get('phase_coherence', 0) >= MINIMUM_COHERENCE_FLOOR:
#                         print("✅ Successfully restored quantum state from backup")
#                         return True
#                 except Exception as e:
#                     print(f"Error applying backup state: {e}")
#                     traceback.print_exc()
# 
#             # If backup restoration fails, reinitialize ground state
#             print("No valid backup found. Reinitializing ground state...")
#             if hasattr(self.agent.quantum_state, '_prepare_ground_state_with_coherence'):
#                 self.agent.quantum_state._prepare_ground_state_with_coherence()
#             else:
#                 print("❌ Cannot reinitialize ground state - missing method")
#                 return False
# 
#             # Validate reinitialized state
#             return self._validate_quantum_state()
# 
#         except Exception as e:
#             print(f"❌ Error recovering quantum state: {e}")
#             traceback.print_exc()
#             return False
# 
#     def recover_surplus_state(self) -> bool:
#         """
#         Recover surplus state with validation.
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             print("Reinitializing surplus state...")
# 
#             # Create new surplus state
#             from data_classes import SurplusState
#             new_surplus_state = SurplusState()
# 
#             # Validate new state
#             if not new_surplus_state.validate():
#                 print("❌ New surplus state validation failed")
#                 return False
# 
#             # Apply to agent
#             self.agent.surplus_dynamics.surplus_state = new_surplus_state
# 
#             # Use reset method if available
#             if hasattr(self.agent.surplus_dynamics, 'reset_state'):
#                 self.agent.surplus_dynamics.reset_state()
# 
#             # Validate result
#             return self.agent.surplus_dynamics.surplus_state.validate()
# 
#         except Exception as e:
#             print(f"❌ Error recovering surplus state: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _validate_quantum_state(self) -> bool:
#         """
#         Validate quantum state after recovery.
# 
#         Returns:
#             True if valid, False otherwise
#         """
#         try:
#             if not hasattr(self.agent.quantum_state, 'phase_coherence'):
#                 return False
# 
#             metrics = self.agent.quantum_state.get_quantum_metrics()
#             return metrics.get('phase_coherence', 0) >= MINIMUM_COHERENCE_FLOOR
# 
#         except Exception as e:
#             print(f"❌ Error validating quantum state: {e}")
#             traceback.print_exc()
#             return False
# 
#     def _record_recovery_attempt(self, success: bool) -> None:
#         """
#         Record recovery attempt details for analysis.
# 
#         Args:
#             success: Whether the recovery was successful
#         """
#         try:
#             self.recovery_history.append({
#                 'timestamp': time.time(),
#                 'success': success,
#                 'attempt': self.recovery_attempts,
#                 'components_recovered': {
#                     'quantum_state': self._validate_quantum_state(),
#                     'surplus_state': (
#                         hasattr(self.agent.surplus_dynamics, 'surplus_state') and
#                         self.agent.surplus_dynamics.surplus_state.validate()
#                     ),
#                     'training': hasattr(self.agent, 'training_pipeline')
#                 }
#             })
#         except Exception as e:
#             print(f"Error recording recovery attempt: {e}")
#             traceback.print_exc()
# 
#     def recover_training_state(self) -> bool:
#         """
#         Recover training state and optimizer.
# 
#         Returns:
#             True if successful, False otherwise
#         """
#         try:
#             # Reinitialize optimizer
#             if hasattr(self.agent, 'transformer'):
#                 self.agent.training_pipeline.optimizer = QuantumAwareOptimizer(
#                     self.agent.transformer
#                 )
#                 return True
#             return False
# 
#         except Exception as e:
#             print(f"Error recovering training state: {e}")
#             traceback.print_exc()
#             return False
# 
#     def validate_recovery(self) -> bool:
#         """
#         Validate system state after recovery.
# 
#         Returns:
#             True if valid, False otherwise
#         """
#         try:
#             # Get current metrics
#             metrics = self.agent.quantum_state.get_quantum_metrics()
# 
#             # Define stability checks
#             stability_checks = [
#                 metrics.get('phase_coherence', 0.0) >= self.min_stability_threshold,
#                 isinstance(self.agent.surplus_dynamics.surplus_state, SurplusState),
#                 self.agent.surplus_dynamics.surplus_state.stability >= self.min_stability_threshold,
#                 isinstance(self.agent.distinction_level, (int, float)) and 0 <= self.agent.distinction_level <= 1
#             ]
# 
#             return all(stability_checks)
# 
#         except Exception as e:
#             print(f"Error validating recovery: {e}")
#             traceback.print_exc()
#             return False
# 
#     def get_recovery_stats(self) -> Dict[str, Any]:
#         """
#         Get statistics about recovery operations.
# 
#         Returns:
#             Dictionary of recovery statistics
#         """
#         try:
#             stats = {
#                 'total_recoveries': len(self.recovery_history),
#                 'successful_recoveries': sum(1 for r in self.recovery_history if r['success']),
#                 'current_error_counts': dict(self._error_counts),
#                 'last_recovery_time': self.last_recovery_time,
#                 'recovery_attempts': self.recovery_attempts
#             }
# 
#             if self.recovery_history:
#                 recent = list(self.recovery_history)[-10:]
#                 stats.update({
#                     'recent_success_rate': sum(1 for r in recent if r['success']) / len(recent),
#                     'components_recovered': recent[-1]['components_recovered']
#                 })
# 
#             return stats
# 
#         except Exception as e:
#             print(f"Error getting recovery stats: {e}")
#             traceback.print_exc()
#             return {}
# 
# class StateBackupManager:
#     """Manages state backups and restoration for the quantum system."""
#     def __init__(self, backup_frequency: int = 10):
#         self.backup_frequency = backup_frequency
#         self.state_backups = deque(maxlen=100)
#         self.step_counter = 0
#         self.backup_attempts = 0
#         self.max_backup_attempts = 3
#         self.min_backups = 1
# 
#         # Create initial backup state
#         self.last_successful_state = self._create_initial_backup()
#         self.backup_history = deque(maxlen=1000)
# 
#     def has_valid_backup(self) -> bool:
#         """Check if there is at least one valid backup available."""
#         return (len(self.state_backups) >= self.min_backups and
#                 self.last_successful_state is not None)
# 
#     def _create_initial_backup(self) -> Dict[str, Any]:
#         """Create initial backup state with retries."""
#         for attempt in range(self.max_backup_attempts):
#             try:
#                 print(f"Creating initial backup (attempt {attempt + 1})")
# 
#                 # Import SurplusState within the method to avoid circular imports
#                 from data_classes import SurplusState
# 
#                 # Create new surplus state
#                 surplus_state = SurplusState()
#                 if not surplus_state.validate():
#                     raise ValueError("Initial surplus state validation failed")
# 
#                 initial_state = {
#                     'statevector': Statevector.from_label('0' * 4),
#                     'phase_coherence': MINIMUM_COHERENCE_FLOOR,
#                     'distinction_level': 0.5,
#                     'surplus_state': surplus_state,
#                     'stability': 1.0,
#                     'timestamp': time.time()
#                 }
# 
#                 if self._validate_backup(initial_state):
#                     print("✅ Initial backup created successfully")
#                     self.state_backups.append(initial_state)
#                     return initial_state
# 
#             except Exception as e:
#                 print(f"Error in backup attempt {attempt + 1}: {e}")
#                 traceback.print_exc()
# 
#         print("⚠️ Failed to create valid initial backup, using safe defaults")
#         return self._create_safe_default_backup()
# 
#     def _create_safe_default_backup(self) -> Dict[str, Any]:
#         """Create a safe default backup state."""
#         from data_classes import SurplusState
#         return {
#             'statevector': Statevector.from_label('0' * 4),
#             'phase_coherence': MINIMUM_COHERENCE_FLOOR,
#             'distinction_level': 0.5,
#             'surplus_state': SurplusState(),
#             'stability': 1.0,
#             'timestamp': time.time()
#         }
# 
#     def _validate_backup(self, backup: Dict[str, Any]) -> bool:
#         """Validate backup state."""
#         try:
#             from data_classes import SurplusState
# 
#             required_keys = {'statevector', 'phase_coherence', 'distinction_level',
#                            'surplus_state', 'stability', 'timestamp'}
# 
#             # Check required keys
#             if not all(key in backup for key in required_keys):
#                 return False
# 
#             # Validate types and values
#             if not isinstance(backup['statevector'], (Statevector, np.ndarray)):
#                 return False
# 
#             if not isinstance(backup['phase_coherence'], (int, float)):
#                 return False
# 
#             if not MINIMUM_COHERENCE_FLOOR <= backup['phase_coherence'] <= 1.0:
#                 return False
# 
#             if not isinstance(backup['distinction_level'], (int, float)):
#                 return False
# 
#             if not 0.0 <= backup['distinction_level'] <= 1.0:
#                 return False
# 
#             if not isinstance(backup['surplus_state'], SurplusState):
#                 return False
# 
#             if not backup['surplus_state'].validate():
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error validating backup: {e}")
#             return False
# 
#     def store_state(self, quantum_state: 'EnhancedQuantumState',
#                     distinction_level: float,
#                     surplus_state: SurplusState) -> bool:
#         """Store current state with validation."""
#         try:
#             self.step_counter += 1
#             if self.step_counter % self.backup_frequency != 0:
#                 return True
# 
#             # Create state backup
#             backup = {
#                 'statevector': quantum_state.statevector.copy() if isinstance(quantum_state.statevector, np.ndarray)
#                              else Statevector(quantum_state.statevector.data),
#                 'phase_coherence': float(quantum_state.phase_coherence),
#                 'distinction_level': float(distinction_level),
#                 'surplus_state': surplus_state.copy(),
#                 'stability': float(surplus_state.stability),
#                 'timestamp': time.time()
#             }
# 
#             # Validate backup before storing
#             if not self._validate_backup(backup):
#                 print("Invalid backup state, not storing")
#                 return False
# 
#             self.state_backups.append(backup)
#             self.last_successful_state = backup
# 
#             # Track backup history
#             self.backup_history.append({
#                 'timestamp': time.time(),
#                 'success': True,
#                 'metrics': {
#                     'phase_coherence': backup['phase_coherence'],
#                     'distinction_level': backup['distinction_level'],
#                     'stability': backup['stability']
#                 }
#             })
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error storing state backup: {e}")
#             traceback.print_exc()
#             self.backup_history.append({
#                 'timestamp': time.time(),
#                 'success': False,
#                 'error': str(e)
#             })
#             return False
# 
#     def restore_state(self, quantum_state: 'EnhancedQuantumState',
#                  restore_point: str = 'last') -> Optional[Dict[str, Any]]:
#         """Restore state from backup with enhanced validation."""
#         try:
#             # Check if we can use a backup first
#             if not self.has_valid_backup():
#                 logger.warning("No valid backup available for restoration")
#                 return None
# 
#             # Get appropriate backup with better selection logic
#             if restore_point == 'last':
#                 backup = self.last_successful_state
#             elif restore_point == 'most_stable':
#                 # Sort backups by stability and take the most stable one
#                 stable_backups = sorted(
#                     self.state_backups,
#                     key=lambda x: x.get('stability', 0.0),
#                     reverse=True
#                 )
#                 backup = stable_backups[0] if stable_backups else self.last_successful_state
#             elif restore_point == 'most_coherent':
#                 # Sort backups by coherence and take the most coherent one
#                 coherent_backups = sorted(
#                     self.state_backups,
#                     key=lambda x: x.get('phase_coherence', 0.0),
#                     reverse=True
#                 )
#                 backup = coherent_backups[0] if coherent_backups else self.last_successful_state
#             else:
#                 logger.warning(f"Invalid restore point: {restore_point}")
#                 backup = self.last_successful_state if self.last_successful_state else None
# 
#             # Validate backup before restoration
#             if not backup or not self._validate_backup(backup):
#                 logger.warning("Invalid backup state for restoration")
#                 return None
# 
#             # Apply backup to quantum state with better error handling
#             try:
#                 # Add timestamp for tracking
#                 restoration_time = time.time()
# 
#                 # Clone the statevector to prevent reference issues
#                 if isinstance(backup['statevector'], Statevector):
#                     quantum_state.statevector = backup['statevector'].copy()
#                 elif isinstance(backup['statevector'], np.ndarray):
#                     quantum_state.statevector = Statevector(backup['statevector'])
#                 else:
#                     logger.error(f"Unknown statevector type: {type(backup['statevector'])}")
#                     return None
# 
#                 # Restore other quantum properties
#                 quantum_state.phase_coherence = backup['phase_coherence']
# 
#                 # Record restoration event
#                 self.restoration_history.append({
#                     'timestamp': restoration_time,
#                     'backup_time': backup.get('timestamp', 0),
#                     'backup_age': restoration_time - backup.get('timestamp', 0),
#                     'stability': backup.get('stability', 0.0)
#                 })
# 
#                 logger.info("✅ State restored successfully from backup")
#                 return backup
# 
#             except Exception as e:
#                 logger.error(f"Error applying backup state: {e}")
#                 traceback.print_exc()
#                 return None
# 
#         except Exception as e:
#             logger.error(f"Error restoring state: {e}")
#             traceback.print_exc()
#             return None
# 
#     def get_backup_stats(self) -> Dict[str, Any]:
#         """Get statistics about backups."""
#         try:
#             if not self.backup_history:
#                 return {}
# 
#             recent = list(self.backup_history)[-100:]
#             return {
#                 'total_backups': len(self.state_backups),
#                 'successful_backups': sum(1 for b in recent if b['success']),
#                 'last_backup_time': self.last_successful_state['timestamp'] if self.last_successful_state else 0,
#                 'backup_frequency': self.backup_frequency,
#                 'average_stability': np.mean([b['metrics']['stability']
#                                            for b in recent if b['success'] and 'metrics' in b])
#             }
# 
#         except Exception as e:
#             print(f"Error getting backup stats: {e}")
#             traceback.print_exc()
#             return {}
# 
# class StateValidationManager:
#     """Manages ongoing state validation and monitoring."""
#     def __init__(self, agent: 'EnhancedSingleAgentFinalEvolution'):
#         self.agent = agent
#         self.validation_history = deque(maxlen=1000)
#         self.last_validation_time = time.time()
#         self.validation_interval = 0.1  # seconds
# 
#     def validate_current_state(self) -> bool:
#         """Validate current agent state comprehensively."""
#         try:
#             current_time = time.time()
# 
#             # Check if validation is needed
#             if current_time - self.last_validation_time < self.validation_interval:
#                 return True
# 
#             validation_results = {}
# 
#             # Validate quantum state
#             quantum_valid = (
#                 hasattr(self.agent, 'quantum_state') and
#                 hasattr(self.agent.quantum_state, 'phase_coherence') and
#                 isinstance(self.agent.quantum_state.phase_coherence, (int, float)) and
#                 self.agent.quantum_state.phase_coherence >= MINIMUM_COHERENCE_FLOOR
#             )
#             validation_results['quantum_state'] = quantum_valid
# 
#             # Validate surplus state
#             surplus_valid = (
#                 hasattr(self.agent, 'surplus_dynamics') and
#                 hasattr(self.agent.surplus_dynamics, 'surplus_state') and
#                 isinstance(self.agent.surplus_dynamics.surplus_state, SurplusState) and
#                 isinstance(self.agent.surplus_dynamics.surplus_state.values, dict) and
#                 len(self.agent.surplus_dynamics.surplus_state.values) >= 4
#             )
#             validation_results['surplus_state'] = surplus_valid
# 
#             # Validate distinction level
#             distinction_valid = (
#                 hasattr(self.agent, 'distinction_level') and
#                 isinstance(self.agent.distinction_level, (int, float)) and
#                 0 <= self.agent.distinction_level <= 1
#             )
#             validation_results['distinction'] = distinction_valid
# 
#             # Store validation results
#             self.validation_history.append({
#                 'timestamp': current_time,
#                 'results': validation_results,
#                 'all_valid': all(validation_results.values())
#             })
# 
#             self.last_validation_time = current_time
#             return all(validation_results.values())
# 
#         except Exception as e:
#             print(f"Error in state validation: {e}")
#             traceback.print_exc()
#             return False
# 
#     def get_validation_stats(self) -> Dict[str, Any]:
#         """Get validation statistics."""
#         try:
#             if not self.validation_history:
#                 return {}
# 
#             recent = list(self.validation_history)[-100:]
#             return {
#                 'total_validations': len(recent),
#                 'success_rate': sum(1 for v in recent if v['all_valid']) / len(recent) if recent else 0,
#                 'component_success_rates': {
#                     component: sum(1 for v in recent if v['results'].get(component, False)) / len(recent) if recent else 0
#                     for component in ('quantum_state', 'surplus_state', 'distinction')
#                 },
#                 'last_validation_time': self.last_validation_time
#             }
# 
#         except Exception as e:
#             print(f"Error getting validation stats: {e}")
#             traceback.print_exc()
#             return {}
# 
# class StateSynchronizationManager:
#     """Manages synchronization between quantum state and other components."""
#     def __init__(self, quantum_state: 'EnhancedQuantumState',
#                  surplus_dynamics: 'EnhancedSurplusDynamics',
#                  distinction_dynamics: 'EnhancedDistinctionDynamics',
#                  transformer: Optional[nn.Module] = None):
#         self.quantum_state = quantum_state
#         self.surplus_dynamics = surplus_dynamics
#         self.distinction_dynamics = distinction_dynamics
#         self.transformer = transformer  # Optional transformer model
#         self.sync_history = deque(maxlen=1000)
#         self.last_sync_time = time.time()
#         self.sync_retries = 0
#         self.max_retries = 3
# 
#     def synchronize_states(self) -> bool:
#         """Synchronize states across components with enhanced error handling."""
#         try:
#             self.sync_retries = 0
#             while self.sync_retries < self.max_retries:
#                 # Ensure quantum state has required attributes
#                 if not hasattr(self.quantum_state, 'phase_coherence'):
#                     self.quantum_state.phase_coherence = MINIMUM_COHERENCE_FLOOR
# 
#                 if not hasattr(self.quantum_state, 'statevector'):
#                     self.quantum_state.statevector = Statevector.from_label('0' * self.quantum_state.num_qubits)
# 
#                 # Get current metrics with safe defaults
#                 try:
#                     metrics = self.quantum_state.get_quantum_metrics()
#                 except Exception as e:
#                     print(f"Error getting metrics during sync: {e}")
#                     metrics = {
#                         'phase_coherence': self.quantum_state.phase_coherence,
#                         'normalized_entropy': 0.0,
#                         'phase': 0.0
#                     }
# 
#                 # Initialize surplus state if needed
#                 if not isinstance(self.surplus_dynamics.surplus_state, SurplusState) or self.surplus_dynamics.surplus_state is None:
#                     print(f"Warning: Reinitializing surplus state as SurplusState()")
#                     self.surplus_dynamics.surplus_state = SurplusState()
# 
#                 # Update surplus dynamics
#                 try:
#                     self.surplus_dynamics.update_surplus(
#                         metrics['phase_coherence'],
#                         metrics['normalized_entropy']
#                     )
#                 except Exception as e:
#                     print(f"Error updating surplus during sync: {e}")
#                     traceback.print_exc()
#                     self.surplus_dynamics.surplus_state = SurplusState()
# 
#                 # Move transformer to correct device if needed
#                 if self.transformer is not None:
#                     try:
#                         device = next(self.quantum_state.parameters()).device
#                         self.transformer.to(device)
#                     except Exception as e:
#                         print(f"Error moving transformer to device: {e}")
# 
#                 # Track resource history
#                 if not hasattr(self, 'resource_history'):
#                     self.resource_history = []
# 
#                 # Store sync history
#                 self.sync_history.append({
#                     'timestamp': time.time(),
#                     'metrics': metrics.copy(),
#                     'surplus_stability': self.surplus_dynamics.surplus_state.stability
#                 })
# 
#                 self.last_sync_time = time.time()
#                 return True
# 
#                 self.sync_retries += 1
#                 print(f"Sync attempt {self.sync_retries} failed, retrying...")
#                 time.sleep(0.1)
# 
#             return False
# 
#         except Exception as e:
#             print(f"Error in state synchronization: {e}")
#             traceback.print_exc()
#             return False
# 
#     def check_sync_needed(self) -> bool:
#         """Check if synchronization is needed."""
#         return time.time() - self.last_sync_time > 0.1  # Sync every 100ms
# 
#     def get_sync_stats(self) -> Dict[str, float]:
#         """Get synchronization statistics."""
#         try:
#             if not self.sync_history:
#                 return {}
# 
#             recent_syncs = list(self.sync_history)[-100:]
#             return {
#                 'sync_frequency': len(recent_syncs) / (time.time() - recent_syncs[0]['timestamp']) if recent_syncs else 0,
#                 'mean_coherence': np.mean([s['metrics'].get('phase_coherence', 0) for s in recent_syncs]) if recent_syncs else 0,
#                 'mean_surplus_stability': np.mean([s.get('surplus_stability', 0) for s in recent_syncs]) if recent_syncs else 0,
#                 'last_sync_time': self.last_sync_time
#             }
# 
#         except Exception as e:
#             print(f"Error getting sync stats: {e}")
#             traceback.print_exc()
#             return {}
# 
# class EnhancedQuantumSelfOptimization:
#     """
#     Enhanced quantum self-optimization with improved coherence management and adaptive optimization.
#     """
#     def __init__(self, num_qubits: int = NUM_QUBITS_PER_AGENT):
#         self.num_qubits = num_qubits
#         self.optimization_history = deque(maxlen=1000)
#         self.coherence_history = deque(maxlen=100)
#         self.phase_history = deque(maxlen=100)
#         self.coherence_momentum = 0.5  # Initialize with moderate momentum
#         self.phase_momentum = 0.5      # Initialize with moderate momentum
#         self.adaptation_rates = {
#             'coherence': 0.3,    # Increased from 0.1
#             'entropy': 0.2,      # Increased from 0.05
#             'phase': 0.25        # Increased from 0.08
#         }
#         self.stability_factor = 1.0
#         self.optimization_momentum = 0.5  # Initialize with moderate momentum
#         self.minimum_coherence = MINIMUM_COHERENCE_FLOOR
#         self.target_coherence = 0.8
#         self.consecutive_failures = 0
#         self.max_failures = 3    # Reduced from 5 to be more responsive
# 
#         # Recovery parameters
#         self.recovery_threshold = 0.3    # Increased from 0.2
#         self.recovery_factor = 0.7       # Increased from 0.5 for gentler recovery
#         self.in_recovery = False
#         self.recovery_steps = 0
#         self.max_recovery_steps = 10
# 
#     def reinforce_coherence(self, qc: QuantumCircuit, distinction_variance: float, phase_coherence: float) -> None:
#         """Enhanced coherence reinforcement with type safety."""
#         try:
#             # Ensure inputs are real floats
#             phase_coherence = float(np.real(phase_coherence))
#             distinction_variance = float(np.real(distinction_variance))
# 
#             # Calculate correction parameters
#             coherence_error = self.target_coherence - phase_coherence
#             base_angle = float(np.real((np.pi / 8) * np.sign(coherence_error)))
#             variance_factor = min(1.0, distinction_variance / 0.02)
# 
#             # Update momentum with type safety
#             self.coherence_momentum = float(np.real(
#                 MOMENTUM_DECAY * self.coherence_momentum +
#                 (1 - MOMENTUM_DECAY) * coherence_error
#             ))
# 
#             # Calculate final angle with momentum influence
#             angle = float(np.real(base_angle * variance_factor * (1.0 + 0.1 * self.coherence_momentum)))
# 
#             # Apply quantum operations with explicit float conversion
#             for q in range(self.num_qubits):
#                 qc.rz(float(0.1 * angle), q)
#                 qc.rx(float(angle), q)
# 
#             # Track optimization
#             self.optimization_history.append({
#                 'type': 'coherence_reinforcement',
#                 'angle': angle,
#                 'coherence': phase_coherence,
#                 'momentum': self.coherence_momentum,
#                 'timestamp': time.time()
#             })
# 
#             self.stability_factor = min(1.0, self.stability_factor + 0.1 * (phase_coherence - 0.5))
#             self.consecutive_failures = 0
# 
#         except Exception as e:
#             print(f"Error in coherence reinforcement: {e}")
#             traceback.print_exc()
#             self.consecutive_failures += 1
#             if self.consecutive_failures >= self.max_failures:
#                 self._enter_recovery_mode()
# 
#     def optimize_quantum_state(self, quantum_state: 'EnhancedQuantumState',
#                          distinction_level: float,
#                          cognitive_state: Dict[str, float]) -> None:
#         """
#         Optimize quantum state with enhanced stability and adaptation
#         """
#         try:
#             # Validate inputs with improved defaults
#             if not isinstance(cognitive_state, dict):
#                 cognitive_state = {}
# 
#             required_metrics = {
#                 'stability': 1.0,
#                 'quantum_coupling': 1.0,
#                 'mean_stability': 1.0,
#                 'quantum_influence': 1.0,
#                 'collapse_probability': 0.0,
#                 'coherence_distinction': 0.5,
#                 'quantum_surplus_coupling': 1.0
#             }
# 
#             # Add missing metrics with defaults
#             for key, default in required_metrics.items():
#                 if key not in cognitive_state:
#                     cognitive_state[key] = default
# 
#             # Get current metrics with enhanced validation
#             metrics = quantum_state.get_quantum_metrics()
#             coherence_error = self.target_coherence - metrics['phase_coherence']
#             entropy_factor = 1.0 - metrics.get('normalized_entropy', 0.5)
#             cognitive_factor = cognitive_state.get('mean_stability', 1.0)
# 
#             # Define improvement thresholds
#             COHERENCE_THRESHOLD = 0.1
#             ENTROPY_THRESHOLD = 0.3
#             COLLAPSE_THRESHOLD = 0.3
#             MOMENTUM_THRESHOLD = 0.2
# 
#             # Check if optimization is needed with more granular conditions
#             needs_optimization = (
#                 abs(coherence_error) > COHERENCE_THRESHOLD or
#                 entropy_factor < ENTROPY_THRESHOLD or
#                 cognitive_state.get('collapse_probability', 0.0) > COLLAPSE_THRESHOLD or
#                 self.optimization_momentum > MOMENTUM_THRESHOLD
#             )
# 
#             if needs_optimization:
#                 # Save initial state for comparison
#                 initial_statevector = quantum_state.statevector.copy()
#                 initial_metrics = metrics.copy()
# 
#                 # Calculate optimization strength with quantum influence
#                 base_strength = self.adaptation_rates['coherence']
#                 quantum_factor = cognitive_state.get('quantum_influence', 1.0)
#                 stability_factor = cognitive_state.get('stability', 1.0)
# 
#                 opt_strength = base_strength * (
#                     1.0 + quantum_factor
#                 ) * stability_factor
# 
#                 # Update optimization momentum with decay
#                 self.optimization_momentum = (
#                     MOMENTUM_DECAY * self.optimization_momentum +
#                     (1 - MOMENTUM_DECAY) * opt_strength
#                 )
# 
#                 # Apply optimizations based on state with enhanced momentum
#                 if coherence_error > 0:
#                     self._apply_coherence_enhancement(
#                         quantum_state,
#                         opt_strength + 0.1 * self.optimization_momentum,
#                         distinction_level
#                     )
#                 else:
#                     self._apply_coherence_reduction(
#                         quantum_state,
#                         opt_strength,
#                         distinction_level
#                     )
# 
#                 if entropy_factor < ENTROPY_THRESHOLD:
#                     self._apply_entropy_reduction(
#                         quantum_state,
#                         opt_strength
#                     )
# 
#                 # Verify improvement with multiple metrics
#                 final_metrics = quantum_state.get_quantum_metrics()
# 
#                 # Calculate improvement scores
#                 coherence_improvement = final_metrics['phase_coherence'] - initial_metrics['phase_coherence']
#                 entropy_improvement = initial_metrics['normalized_entropy'] - final_metrics['normalized_entropy']
#                 stability_improvement = final_metrics.get('stability', 1.0) - initial_metrics.get('stability', 1.0)
# 
#                 # Consider multiple factors for improvement
#                 improvement = (
#                     coherence_improvement > -0.01 or  # Allow small degradation
#                     entropy_improvement > -0.01 or
#                     stability_improvement > 0
#                 )
# 
#                 if not improvement:
#                     print("Warning: Optimization did not improve state")
#                     self.consecutive_failures += 1
#                     if self.consecutive_failures >= self.max_failures:
#                         self._enter_recovery_mode()
#                 else:
#                     self.consecutive_failures = max(0, self.consecutive_failures - 1)
# 
#                 # Track optimization with enhanced metrics
#                 self.optimization_history.append({
#                     'initial_metrics': initial_metrics,
#                     'final_metrics': final_metrics,
#                     'improvement': improvement,
#                     'optimization_strength': opt_strength,
#                     'momentum': self.optimization_momentum,
#                     'coherence_change': coherence_improvement,
#                     'entropy_change': entropy_improvement,
#                     'stability_change': stability_improvement,
#                     'consecutive_failures': self.consecutive_failures
#                 })
# 
#         except Exception as e:
#             print(f"Error in quantum optimization: {e}")
#             traceback.print_exc()
#             self.consecutive_failures += 1
#             if self.consecutive_failures >= self.max_failures:
#                 self._enter_recovery_mode()
# 
#     def _enter_recovery_mode(self):
#         """Enter recovery mode when optimization repeatedly fails"""
#         try:
#             print("Entering quantum optimization recovery mode")
# 
#             # Scale down adaptation rates but maintain some adaptation ability
#             self.adaptation_rates = {
#                 k: v * self.recovery_factor for k, v in self.adaptation_rates.items()
#             }
# 
#             # Reduce momentum values but don't zero them out
#             self.optimization_momentum *= self.recovery_factor
#             self.coherence_momentum *= self.recovery_factor
#             self.phase_momentum *= self.recovery_factor
# 
#             # Maintain some stability while in recovery
#             self.stability_factor = max(self.stability_factor * self.recovery_factor, 0.3)
# 
#             # Reset failure counter
#             self.consecutive_failures = 0
# 
#             # Enter recovery mode
#             self.in_recovery = True
#             self.recovery_steps = self.max_recovery_steps
# 
#             print(f"Recovery mode entered with:")
#             print(f"- Optimization momentum: {self.optimization_momentum:.3f}")
#             print(f"- Coherence momentum: {self.coherence_momentum:.3f}")
#             print(f"- Phase momentum: {self.phase_momentum:.3f}")
#             print(f"- Stability factor: {self.stability_factor:.3f}")
# 
#         except Exception as e:
#             print(f"Error entering recovery mode: {e}")
#             traceback.print_exc()
# 
#     def _apply_coherence_enhancement(self, quantum_state: 'EnhancedQuantumState',
#                                    strength: float,
#                                    distinction_level: float) -> None:
#         """Apply coherence enhancement with proper error handling"""
#         try:
#             # Calculate rotation angle with momentum
#             angle = np.pi * strength * distinction_level
# 
#             # Apply rotations with stability
#             for qubit in range(quantum_state.num_qubits):
#                 quantum_state.qc.rz(0.1 * angle, qubit)  # Phase-preserving operation
#                 quantum_state.qc.rx(angle * (1.0 + 0.2 * self.phase_momentum), qubit)
# 
#             # Apply global phase shift
#             quantum_state.qc.rz(angle * 0.5, 0)
# 
#             # Update state
#             quantum_state.update_state()
# 
#         except Exception as e:
#             print(f"Error in coherence enhancement: {e}")
#             traceback.print_exc()
#             self.consecutive_failures += 1
# 
#     def _apply_coherence_reduction(self, quantum_state: 'EnhancedQuantumState',
#                                  strength: float,
#                                  distinction_level: float) -> None:
#         """Apply coherence reduction with minimum coherence preservation"""
#         try:
#             # Calculate damping strength
#             gamma = min(strength * (1.0 - distinction_level),
#                        1.0 - self.minimum_coherence)
# 
#             # Apply controlled amplitude damping
#             for qubit in range(quantum_state.num_qubits):
#                 quantum_state.qc.id(qubit)  # Identity gate with noise
#                 quantum_state.qc.rz(0.1 * np.pi * gamma, qubit)  # Stabilizing operation
# 
#             # Update state
#             quantum_state.update_state()
# 
#         except Exception as e:
#             print(f"Error in coherence reduction: {e}")
#             traceback.print_exc()
#             self.consecutive_failures += 1
# 
#     def _apply_entropy_reduction(self, quantum_state: 'EnhancedQuantumState',
#                                strength: float) -> None:
#         """Apply entropy reduction with momentum-based stability"""
#         try:
#             # Apply bit flips with stability
#             for qubit in range(quantum_state.num_qubits):
#                 quantum_state.qc.rz(0.1 * np.pi * strength, qubit)  # Stabilizing rotation
#                 quantum_state.qc.x(qubit)  # Main operation
# 
#             # Apply phase shift with momentum
#             phase_angle = np.pi * strength * (1.0 + 0.1 * self.phase_momentum)
#             quantum_state.qc.rz(phase_angle, 0)
# 
#             # Update state
#             quantum_state.update_state()
# 
#         except Exception as e:
#             print(f"Error in entropy reduction: {e}")
#             traceback.print_exc()
#             self.consecutive_failures += 1
# 
#     def get_optimization_summary(self) -> Dict[str, float]:
#         """Get summary of optimization parameters and history."""
#         try:
#             if not self.optimization_history:
#                 return {}
# 
#             recent = list(self.optimization_history)[-100:]
#             summary = {
#                 'mean_opt_strength': np.mean([r.get('optimization_strength', 0.0) for r in recent if 'optimization_strength' in r]),
#                 'optimization_frequency': len(recent) / 100.0,
#                 'optimization_momentum': self.optimization_momentum,
#                 'stability_factor': self.stability_factor,
#                 'in_recovery': self.in_recovery,
#                 'recovery_steps': self.recovery_steps if self.in_recovery else 0,
#                 'consecutive_failures': self.consecutive_failures,
#                 'coherence_momentum': self.coherence_momentum,
#                 'phase_momentum': self.phase_momentum
#             }
# 
#             for key, rate in self.adaptation_rates.items():
#                 summary[f'{key}_adaptation_rate'] = rate
# 
#             return summary
# 
#         except Exception as e:
#             print(f"Error getting optimization summary: {e}")
#             traceback.print_exc()
#             return {}
# 
# class PerformanceMonitor:
#     """Monitors system performance and provides optimization recommendations."""
#     def __init__(self, window_size: int = 100):
#         self.window_size = window_size
#         self.metrics_history = {
#             'coherence': deque(maxlen=window_size),
#             'distinction': deque(maxlen=window_size),
#             'entropy': deque(maxlen=window_size),
#             'surplus_stability': deque(maxlen=window_size),
#             'training_loss': deque(maxlen=window_size),
#             'prediction_accuracy': deque(maxlen=window_size)
#         }
#         self.alert_thresholds = {
#             'coherence_min': 0.3,
#             'distinction_variance_max': 0.1,
#             'entropy_max': 0.8,
#             'stability_min': 0.4
#         }
#         self.optimization_history = deque(maxlen=1000)
#         self.last_metrics_update = time.time()
#         self.update_counter = 0
#         self.alert_counter = defaultdict(int)
#         self.current_recommendations = []
# 
#     def update_metrics(self, metrics: Dict[str, float]) -> None:
#         """
#         Update stored metrics and track performance trends.
# 
#         Args:
#             metrics: Dictionary of current metrics
#         """
#         try:
#             current_time = time.time()
#             self.update_counter += 1
# 
#             # Record metrics in history
#             for key, value in metrics.items():
#                 if key in self.metrics_history:
#                     self.metrics_history[key].append(value)
# 
#             # Store complete snapshot
#             self.optimization_history.append({
#                 'timestamp': current_time,
#                 'metrics': metrics.copy(),
#                 'update_counter': self.update_counter
#             })
# 
#             # Check for alerts
#             self._check_alerts(metrics)
# 
#             # Update recommendations every 10 updates
#             if self.update_counter % 10 == 0:
#                 self.current_recommendations = self._generate_recommendations()
# 
#             self.last_metrics_update = current_time
# 
#         except Exception as e:
#             print(f"Error updating performance metrics: {e}")
#             traceback.print_exc()
# 
#     def _check_alerts(self, metrics: Dict[str, float]) -> None:
#         """Check for alert conditions based on current metrics."""
#         try:
#             alerts = []
# 
#             # Check coherence
#             if metrics.get('coherence', 1.0) < self.alert_thresholds['coherence_min']:
#                 alerts.append('low_coherence')
#                 self.alert_counter['low_coherence'] += 1
#             else:
#                 self.alert_counter['low_coherence'] = 0
# 
#             # Check entropy
#             if metrics.get('entropy', 0.0) > self.alert_thresholds['entropy_max']:
#                 alerts.append('high_entropy')
#                 self.alert_counter['high_entropy'] += 1
#             else:
#                 self.alert_counter['high_entropy'] = 0
# 
#             # Check stability
#             if metrics.get('stability', 1.0) < self.alert_thresholds['stability_min']:
#                 alerts.append('low_stability')
#                 self.alert_counter['low_stability'] += 1
#             else:
#                 self.alert_counter['low_stability'] = 0
# 
#             # Check distinction variance if we have enough history
#             distinction_values = list(self.metrics_history.get('distinction', []))
#             if len(distinction_values) > 10:
#                 variance = np.var(distinction_values[-10:])
#                 if variance > self.alert_thresholds['distinction_variance_max']:
#                     alerts.append('high_distinction_variance')
#                     self.alert_counter['high_distinction_variance'] += 1
#                 else:
#                     self.alert_counter['high_distinction_variance'] = 0
# 
#             # Store alerts with metrics
#             if alerts and len(self.optimization_history) > 0:
#                 self.optimization_history[-1]['alerts'] = alerts
# 
#         except Exception as e:
#             print(f"Error checking alerts: {e}")
#             traceback.print_exc()
# 
#     def _safe_correlation(self, x: np.ndarray, y: np.ndarray) -> float:
#         """
#         Calculate correlation with proper error handling for division by zero issues.
# 
#         Args:
#             x: First data array
#             y: Second data array
# 
#         Returns:
#             Correlation coefficient or 0.0 if calculation fails
#         """
#         try:
#             if len(x) < 2 or len(y) < 2:
#                 return 0.0
# 
#             # First, handle NaN values
#             x = np.nan_to_num(x, nan=0.0)
#             y = np.nan_to_num(y, nan=0.0)
# 
#             # Calculate standard deviations
#             std_x = np.std(x)
#             std_y = np.std(y)
# 
#             # Check for constant arrays which would cause division by zero
#             if std_x < 1e-10 or std_y < 1e-10:
#                 # Add small random noise to prevent constant arrays
#                 if std_x < 1e-10:
#                     x = x + np.random.normal(0, 1e-5, size=x.shape)
#                     std_x = np.std(x)
# 
#                 if std_y < 1e-10:
#                     y = y + np.random.normal(0, 1e-5, size=y.shape)
#                     std_y = np.std(y)
# 
#                 # If still constant after adding noise, return 0
#                 if std_x < 1e-10 or std_y < 1e-10:
#                     return 0.0
# 
#             # Manually calculate correlation to avoid NumPy warning
#             x_normalized = (x - np.mean(x)) / std_x
#             y_normalized = (y - np.mean(y)) / std_y
#             correlation = np.mean(x_normalized * y_normalized)
# 
#             # Check for NaN results
#             if np.isnan(correlation):
#                 return 0.0
# 
#             return float(correlation)
#         except Exception as e:
#             print(f"Error calculating correlation: {e}")
#             return 0.0
# 
#     def get_performance_analysis(self) -> Dict[str, Any]:
#         """
#         Analyze performance metrics and provide detailed insights.
# 
#         Returns:
#             Dictionary with analysis results
#         """
#         try:
#             analysis = {}
# 
#             # Calculate statistics for each metric
#             for key, values in self.metrics_history.items():
#                 if values:
#                     analysis[f'{key}_mean'] = float(np.mean(values))
#                     analysis[f'{key}_std'] = float(np.std(values))
#                     analysis[f'{key}_trend'] = float(np.mean(np.diff(list(values)))) if len(values) > 1 else 0.0
#                     analysis[f'{key}_min'] = float(np.min(values))
#                     analysis[f'{key}_max'] = float(np.max(values))
# 
#             # Determine active alerts
#             alerts = []
#             for alert_type, count in self.alert_counter.items():
#                 if count > 2:  # Alert only after multiple occurrences
#                     alerts.append(alert_type)
# 
#             analysis['alerts'] = alerts
#             analysis['optimization_needed'] = len(alerts) > 0
#             analysis['update_counter'] = self.update_counter
#             analysis['last_update_time'] = self.last_metrics_update
# 
#             # Add relationship analysis if we have enough data
#             if all(len(self.metrics_history[k]) > 10 for k in ['coherence', 'distinction', 'entropy']):
#                 coherence = list(self.metrics_history['coherence'])[-10:]
#                 distinction = list(self.metrics_history['distinction'])[-10:]
#                 entropy = list(self.metrics_history['entropy'])[-10:]
# 
#                 analysis['coherence_distinction_corr'] = self._safe_correlation(np.array(coherence), np.array(distinction))
#                 analysis['coherence_entropy_corr'] = self._safe_correlation(np.array(coherence), np.array(entropy))
#                 analysis['distinction_entropy_corr'] = self._safe_correlation(np.array(distinction), np.array(entropy))
# 
#             return analysis
# 
#         except Exception as e:
#             print(f"Error in performance analysis: {e}")
#             traceback.print_exc()
#             return {'error': str(e)}
# 
#     def _generate_recommendations(self) -> List[Dict[str, Any]]:
#         """
#         Generate optimization recommendations based on observed metrics.
# 
#         Returns:
#             List of recommendation dictionaries
#         """
#         try:
#             recommendations = []
#             analysis = self.get_performance_analysis()
# 
#             # Handle low coherence
#             if 'low_coherence' in analysis.get('alerts', []):
#                 recommendations.append({
#                     'component': 'quantum_state',
#                     'action': 'reinforce_coherence',
#                     'priority': 'high',
#                     'params': {'target_coherence': 0.7}
#                 })
# 
#             # Handle high distinction variance
#             if 'high_distinction_variance' in analysis.get('alerts', []):
#                 recommendations.append({
#                     'component': 'distinction_dynamics',
#                     'action': 'stabilize_distinction',
#                     'priority': 'medium',
#                     'params': {'momentum_scale': 0.8}
#                 })
# 
#             # Handle high entropy
#             if 'high_entropy' in analysis.get('alerts', []):
#                 recommendations.append({
#                     'component': 'quantum_state',
#                     'action': 'reduce_entropy',
#                     'priority': 'high',
#                     'params': {'target_entropy': 0.5}
#                 })
# 
#             # Handle low stability
#             if 'low_stability' in analysis.get('alerts', []):
#                 recommendations.append({
#                     'component': 'surplus_dynamics',
#                     'action': 'reinforce_stability',
#                     'priority': 'high',
#                     'params': {'stability_boost': 0.3}
#                 })
# 
#             # Add general learning rate recommendation based on trend
#             if self.update_counter > 20 and 'training_loss' in self.metrics_history:
#                 loss_trend = analysis.get('training_loss_trend', 0)
#                 if loss_trend > 0:  # Loss is increasing
#                     recommendations.append({
#                         'component': 'training_pipeline',
#                         'action': 'reduce_learning_rate',
#                         'priority': 'medium',
#                         'params': {'scale_factor': 0.7}
#                     })
#                 elif loss_trend < -0.01:  # Loss is decreasing nicely
#                     recommendations.append({
#                         'component': 'training_pipeline',
#                         'action': 'maintain_learning_rate',
#                         'priority': 'low',
#                         'params': {}
#                     })
# 
#             return recommendations
# 
#         except Exception as e:
#             print(f"Error generating recommendations: {e}")
#             traceback.print_exc()
#             return []
# 
#     def get_optimization_recommendations(self) -> List[Dict[str, Any]]:
#         """
#         Get the current optimization recommendations.
# 
#         Returns:
#             List of recommendation dictionaries
#         """
#         return self.current_recommendations
# 
# class OptimizationCoordinator:
#     """Coordinates optimization actions across system components."""
#     def __init__(self, agent):
#         # Here 'agent' is assumed to be your overall system agent (e.g., EnhancedSingleAgentFinalEvolution)
#         self.agent = agent
#         self.monitor = PerformanceMonitor()
#         self.optimization_queue = deque()
#         self.last_optimization_time = time.time()
#         self.optimization_cooldown = 10  # seconds between optimizations
#         self.current_step = 0
#         self.optimization_history = deque(maxlen=1000)
#         self.applied_optimizations = defaultdict(int)
#         self.recovery_mode = False
#         self.recovery_count = 0
#         self.max_recovery_attempts = 3
# 
#     def update(self, metrics: Dict[str, float]) -> None:
#         """
#         Update performance monitoring and process optimization queue.
# 
#         Args:
#             metrics: Current system metrics
#         """
#         try:
#             self.current_step += 1
# 
#             # Update performance monitor
#             self.monitor.update_metrics(metrics)
# 
#             # Check if optimization is needed
#             if self._should_optimize():
#                 recommendations = self.monitor.get_optimization_recommendations()
#                 self._queue_optimizations(recommendations)
# 
#             # Process optimization queue
#             self._process_optimization_queue()
# 
#         except Exception as e:
#             print(f"Error in optimization update: {e}")
#             traceback.print_exc()
# 
#     def _should_optimize(self) -> bool:
#         """
#         Determine if optimization should be performed.
# 
#         Returns:
#             True if optimization should be performed, False otherwise
#         """
#         # Check time elapsed since last optimization
#         time_elapsed = time.time() - self.last_optimization_time
# 
#         # Get performance analysis
#         analysis = self.monitor.get_performance_analysis()
# 
#         # Check if optimization is needed and cooldown has passed
#         return (time_elapsed > self.optimization_cooldown and
#                 analysis.get('optimization_needed', False))
# 
#     def _queue_optimizations(self, recommendations: List[Dict[str, Any]]) -> None:
#         """
#         Queue optimization recommendations based on priority.
# 
#         Args:
#             recommendations: List of optimization recommendations
#         """
#         try:
#             # Define priority mapping (lower number = higher priority)
#             priority_map = {'high': 0, 'medium': 1, 'low': 2}
# 
#             # Sort recommendations by priority
#             sorted_recs = sorted(
#                 recommendations,
#                 key=lambda x: priority_map.get(x.get('priority', 'low'), 999)
#             )
# 
#             # Add recommendations to queue
#             for rec in sorted_recs:
#                 # Check if this optimization has been applied too many times
#                 component = rec.get('component', '')
#                 action = rec.get('action', '')
#                 key = f"{component}_{action}"
# 
#                 # Limit the number of times a specific optimization can be applied
#                 if self.applied_optimizations[key] < 3:
#                     self.optimization_queue.append(rec)
# 
#         except Exception as e:
#             print(f"Error queueing optimizations: {e}")
#             traceback.print_exc()
# 
#     def _process_optimization_queue(self) -> None:
#         """Process the optimization queue."""
#         try:
#             if not self.optimization_queue:
#                 return
# 
#             # Process up to 3 optimizations at once
#             optimizations_applied = 0
#             while self.optimization_queue and optimizations_applied < 3:
#                 # Get next optimization
#                 opt = self.optimization_queue.popleft()
# 
#                 # Apply optimization
#                 success = self._apply_optimization(opt)
# 
#                 if success:
#                     optimizations_applied += 1
#                     component = opt.get('component', '')
#                     action = opt.get('action', '')
#                     key = f"{component}_{action}"
#                     self.applied_optimizations[key] += 1
# 
#                     # Record application
#                     self.optimization_history.append({
#                         'timestamp': time.time(),
#                         'optimization': opt,
#                         'success': True,
#                         'step': self.current_step
#                     })
# 
#             self.last_optimization_time = time.time()
# 
#         except Exception as e:
#             print(f"Error processing optimization queue: {e}")
#             traceback.print_exc()
# 
#     def _apply_optimization(self, optimization: Dict[str, Any]) -> bool:
#         """
#         Apply a specific optimization.
# 
#         Args:
#             optimization: Optimization recommendation dictionary
# 
#         Returns:
#             True if optimization was applied successfully, False otherwise
#         """
#         try:
#             component = optimization.get('component', '')
#             action = optimization.get('action', '')
#             params = optimization.get('params', {})
# 
#             print(f"Applying optimization: {component}.{action} with params {params}")
# 
#             # Apply component-specific optimizations
#             if component == 'quantum_state':
#                 if action == 'reinforce_coherence' and hasattr(self.agent, 'quantum_optimizer'):
#                     self.agent.quantum_optimizer.reinforce_coherence(
#                         self.agent.quantum_state.qc,
#                         params.get('distinction_variance', 0.5),
#                         self.agent.quantum_state.phase_coherence
#                     )
#                     return True
# 
#                 elif action == 'reduce_entropy':
#                     if hasattr(self.agent.quantum_state, 'apply_gate') and hasattr(self.agent.quantum_state, 'apply_phase_shift'):
#                         # Apply operations to reduce entropy
#                         self.agent.quantum_state.apply_gate('x', [0])
#                         self.agent.quantum_state.apply_phase_shift(0.1 * np.pi)
#                         return True
# 
#             elif component == 'distinction_dynamics':
#                 if action == 'stabilize_distinction' and hasattr(self.agent, 'distinction_dynamics'):
#                     # Stabilize distinction by reducing momentum
#                     if hasattr(self.agent.distinction_dynamics, 'adjustment_momentum'):
#                         momentum_scale = params.get('momentum_scale', 0.8)
#                         self.agent.distinction_dynamics.adjustment_momentum *= momentum_scale
#                         return True
# 
#             elif component == 'surplus_dynamics':
#                 if action == 'reinforce_stability' and hasattr(self.agent, 'surplus_dynamics'):
#                     # Boost stability
#                     stability_boost = params.get('stability_boost', 0.3)
#                     if hasattr(self.agent.surplus_dynamics.surplus_state, 'stability'):
#                         current = self.agent.surplus_dynamics.surplus_state.stability
#                         self.agent.surplus_dynamics.surplus_state.stability = min(1.0, current + stability_boost)
#                         return True
# 
#             elif component == 'training_pipeline':
#                 if action == 'reduce_learning_rate' and hasattr(self.agent, 'training_pipeline'):
#                     # Reduce learning rate
#                     scale_factor = params.get('scale_factor', 0.7)
#                     if hasattr(self.agent.training_pipeline, 'optimizer'):
#                         for param_group in self.agent.training_pipeline.optimizer.param_groups:
#                             param_group['lr'] *= scale_factor
#                         return True
# 
#                 elif action == 'maintain_learning_rate':
#                     # No action needed, just acknowledge
#                     return True
# 
#             print(f"Optimization {component}.{action} not implemented or components not found")
#             return False
# 
#         except Exception as e:
#             print(f"Error applying optimization {component}.{action}: {e}")
#             traceback.print_exc()
#             return False
# 
#     def get_optimization_stats(self) -> Dict[str, Any]:
#         """
#         Get optimization statistics.
# 
#         Returns:
#             Dictionary with optimization statistics
#         """
#         try:
#             stats = {
#                 'total_optimizations': len(self.optimization_history),
#                 'current_step': self.current_step,
#                 'queue_length': len(self.optimization_queue),
#                 'recovery_mode': self.recovery_mode,
#                 'recovery_count': self.recovery_count,
#                 'optimization_types': dict(self.applied_optimizations)
#             }
# 
#             if self.optimization_history:
#                 recent = list(self.optimization_history)[-10:]
#                 component_counts = defaultdict(int)
#                 action_counts = defaultdict(int)
# 
#                 for opt in recent:
#                     optimization = opt.get('optimization', {})
#                     component = optimization.get('component', 'unknown')
#                     action = optimization.get('action', 'unknown')
#                     component_counts[component] += 1
#                     action_counts[action] += 1
# 
#                 stats['recent_components'] = dict(component_counts)
#                 stats['recent_actions'] = dict(action_counts)
#                 stats['success_rate'] = sum(1 for opt in recent if opt.get('success', False)) / len(recent)
# 
#             return stats
# 
#         except Exception as e:
#             print(f"Error getting optimization stats: {e}")
#             traceback.print_exc()
#             return {}
# 
# class EnhancedLearningRateScheduler:
#     """
#     Learning rate scheduler with linear warmup and cosine annealing.
#     Also applies a stability-based modulation.
#     """
#     def __init__(self, optimizer: torch.optim.Optimizer,
#                  min_lr: float = LEARNING_RATE_MIN,
#                  max_lr: float = LEARNING_RATE_MAX,
#                  warmup_steps: int = 1000,
#                  cycle_steps: int = 10000):
#         self.optimizer = optimizer
#         self.min_lr = min_lr
#         self.max_lr = max_lr
#         self.warmup_steps = warmup_steps
#         self.cycle_steps = cycle_steps
#         self.step_count = 0
#         self.last_update_time = time.time()
#         self.update_history = deque(maxlen=1000)
#         self.current_lr = min_lr
# 
#     def step(self, loss: float, metrics: Dict[str, float]) -> None:
#         """
#         Update learning rate based on schedule and metrics.
# 
#         Args:
#             loss: Current loss value
#             metrics: Current system metrics
#         """
#         try:
#             self.step_count += 1
# 
#             # Calculate base learning rate from schedule
#             if self.step_count < self.warmup_steps:
#                 # Linear warmup
#                 lr = self.min_lr + (self.max_lr - self.min_lr) * (self.step_count / self.warmup_steps)
#             else:
#                 # Cosine annealing
#                 progress = (self.step_count - self.warmup_steps) / (self.cycle_steps - self.warmup_steps)
#                 progress = min(1.0, progress)
#                 lr = self.min_lr + 0.5 * (self.max_lr - self.min_lr) * (1 + np.cos(progress * np.pi))
# 
#             # Modulate based on stability metrics
#             stability = metrics.get('stability', 1.0)
#             coherence = metrics.get('phase_coherence', 0.5)
# 
#             # Apply stability-based scaling
#             lr_mod = lr * stability * coherence
# 
#             # Ensure lr is within bounds
#             lr_final = np.clip(lr_mod, self.min_lr, self.max_lr)
# 
#             # Apply to optimizer
#             for param_group in self.optimizer.param_groups:
#                 param_group['lr'] = lr_final
# 
#             # Track history
#             self.update_history.append({
#                 'step': self.step_count,
#                 'base_lr': float(lr),
#                 'final_lr': float(lr_final),
#                 'stability': float(stability),
#                 'coherence': float(coherence),
#                 'timestamp': time.time()
#             })
# 
#             self.current_lr = lr_final
#             self.last_update_time = time.time()
#             print(f"DEBUG: Learning Rate Scheduler - Step: {self.step_count}, Base LR: {lr:.6f}, Final LR: {lr_final:.6f}, Stability: {stability:.4f}, Coherence: {coherence:.4f}")
# 
#         except Exception as e:
#             print(f"Error in learning rate adjustment: {e}")
#             traceback.print_exc()
# 
#     def get_lr_stats(self) -> Dict[str, float]:
#         """
#         Get learning rate statistics.
# 
#         Returns:
#             Dictionary with learning rate statistics
#         """
#         try:
#             stats = {'current_lr': self.current_lr}
# 
#             if self.update_history:
#                 recent = list(self.update_history)[-100:]
#                 stats.update({
#                     'mean_lr': np.mean([h['final_lr'] for h in recent]),
#                     'lr_std': np.std([h['final_lr'] for h in recent]),
#                     'warmup_progress': min(1.0, self.step_count / self.warmup_steps),
#                     'cycle_progress': (min(1.0, (self.step_count - self.warmup_steps) / self.cycle_steps)
#                                    if self.step_count > self.warmup_steps else 0.0)
#                 })
# 
#             return stats
# 
#         except Exception as e:
#             print(f"Error getting LR stats: {e}")
#             traceback.print_exc()
#             return {'current_lr': self.current_lr}
# 
# class QuantumStateValidator:
#     """Helper class for validating quantum state across components."""
#     def __init__(self, agent=None):
#         self.agent = agent
#         self.validation_history = deque(maxlen=1000)
#         self.validation_stats = defaultdict(int)
#         self.validation_results = {}
# 
# 
# 
#     @staticmethod
#     def validate_quantum_state(quantum_state: Any) -> bool:
#         """
#         Validate quantum state with comprehensive error checking.
# 
#         Args:
#             quantum_state: Quantum state to validate
# 
#         Returns:
#             True if valid, False otherwise
#         """
#         try:
#             # Basic attribute checks
#             required_attrs = ['statevector', 'phase_coherence', 'phase', 'num_qubits']
#             for attr in required_attrs:
#                 if not hasattr(quantum_state, attr):
#                     print(f"Missing required attribute: {attr}")
#                     return False
# 
#             # Validate statevector
#             if quantum_state.statevector is None:
#                 print("Statevector is None")
#                 return False
# 
#             # Get statevector data
#             if isinstance(quantum_state.statevector, np.ndarray):
#                 state_array = quantum_state.statevector
#             elif hasattr(quantum_state.statevector, 'data'):
#                 state_array = np.array(quantum_state.statevector.data)
#             else:
#                 print("Invalid statevector type")
#                 return False
# 
#             # Check statevector dimensions
#             expected_dim = 2 ** quantum_state.num_qubits
#             if state_array.shape != (expected_dim,):
#                 print(f"Invalid statevector dimension: {state_array.shape}, expected ({expected_dim},)")
#                 return False
# 
#             # Check normalization
#             norm = np.linalg.norm(state_array)
#             if not np.isclose(norm, 1.0, atol=1e-6):
#                 print(f"Statevector not normalized: norm = {norm}")
#                 return False
# 
#             # Validate phase coherence
#             try:
#                 phase_coherence = float(np.real(quantum_state.phase_coherence))
#                 if not MINIMUM_COHERENCE_FLOOR <= phase_coherence <= 1.0:
#                     print(f"Phase coherence out of range: {phase_coherence}")
#                     return False
#             except (TypeError, ValueError) as e:
#                 print(f"Invalid phase coherence value: {e}")
#                 return False
# 
#             # Validate phase
#             try:
#                 phase = float(np.real(quantum_state.phase))
#                 if not 0 <= phase <= 2 * np.pi:
#                     quantum_state.phase = phase % (2 * np.pi)  # Normalize phase
#             except (TypeError, ValueError) as e:
#                 print(f"Invalid phase value: {e}")
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error in quantum state validation: {e}")
#             traceback.print_exc()
#             return False
# 
#     @staticmethod
#     def validate_metrics(metrics: Dict[str, float]) -> bool:
#         """
#         Validate quantum metrics with bounds checking.
# 
#         Args:
#             metrics: Dictionary of quantum metrics
# 
#         Returns:
#             True if valid, False otherwise
#         """
#         try:
#             required_metrics = {
#                 'phase_coherence': (MINIMUM_COHERENCE_FLOOR, 1.0),
#                 'normalized_entropy': (0.0, 1.0),
#                 'phase': (0.0, 2 * np.pi)
#             }
# 
#             for metric, (min_val, max_val) in required_metrics.items():
#                 if metric not in metrics:
#                     print(f"Missing required metric: {metric}")
#                     return False
# 
#                 try:
#                     value = float(np.real(metrics[metric]))
#                     if not min_val <= value <= max_val:
#                         print(f"Metric {metric} out of range: {value}")
#                         return False
#                 except (TypeError, ValueError) as e:
#                     print(f"Invalid value for metric {metric}: {e}")
#                     return False
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error validating metrics: {e}")
#             traceback.print_exc()
#             return False
# 
#     def validate_metrics_dict(self, metrics: Dict[str, float]) -> Dict[str, float]:
#         """
#         Validate and clean metrics dictionary.
# 
#         Args:
#             metrics: Dictionary of metrics
# 
#         Returns:
#             Validated and cleaned metrics dictionary
#         """
#         try:
#             required_metrics = {
#                 'phase_coherence': MINIMUM_COHERENCE_FLOOR,
#                 'normalized_entropy': 0.0,
#                 'phase': 0.0,
#                 'phase_distinction': 0.0,
#                 'mean_coherence': MINIMUM_COHERENCE_FLOOR,
#                 'phase_stability': 1.0
#             }
# 
#             validated_metrics = {}
# 
#             for key, default_value in required_metrics.items():
#                 try:
#                     value = metrics.get(key, default_value)
#                     # Convert to float and handle complex numbers
#                     if isinstance(value, complex):
#                         value = float(value.real)
#                     else:
#                         value = float(value)
# 
#                     # Validate ranges for specific metrics
#                     if key in ['phase_coherence', 'normalized_entropy', 'mean_coherence']:
#                         value = np.clip(value, 0.0, 1.0)
#                     elif key == 'phase':
#                         value = value % (2 * np.pi)
# 
#                     validated_metrics[key] = value
# 
#                 except (TypeError, ValueError) as e:
#                     print(f"Error validating metric {key}: {e}")
#                     validated_metrics[key] = default_value
# 
#             return validated_metrics
# 
#         except Exception as e:
#             print(f"Error in metrics validation: {e}")
#             traceback.print_exc()
#             return {k: v for k, v in required_metrics.items()}
# 
#     def validate_system_state(self, agent) -> Dict[str, bool]:
#         """
#         Perform comprehensive system state validation before recovery.
# 
#         Args:
#             agent: The agent instance to validate
# 
#         Returns:
#             Dictionary of validation results by component
#         """
#         try:
#             validation_results = {}
# 
#             # 1. Validate quantum state
#             validation_results['quantum_state'] = self.validate_quantum_state(agent.quantum_state)
# 
#             # 2. Validate surplus state
#             validation_results['surplus_state'] = (
#                 hasattr(agent, 'surplus_dynamics') and
#                 hasattr(agent.surplus_dynamics, 'surplus_state') and
#                 isinstance(agent.surplus_dynamics.surplus_state, SurplusState) and
#                 agent.surplus_dynamics.surplus_state.validate()
#             )
# 
#             # 3. Validate distinction dynamics
#             validation_results['distinction'] = (
#                 hasattr(agent, 'distinction_level') and
#                 isinstance(agent.distinction_level, (int, float)) and
#                 0 <= agent.distinction_level <= 1.0
#             )
# 
#             # 4. Validate transformer
#             validation_results['transformer'] = (
#                 hasattr(agent, 'transformer') and
#                 isinstance(agent.transformer, nn.Module)
#             )
# 
#             # 5. Validate memory
#             validation_results['memory'] = (
#                 hasattr(agent, 'memory') and
#                 hasattr(agent.memory, 'store')
#             )
# 
#             # 6. Validate ontological field
#             validation_results['ontological_field'] = (
#                 hasattr(agent, 'ontological_field') and
#                 hasattr(agent.ontological_field, 'resistance')
#             )
# 
#             # 7. Validate cognitive structure
#             validation_results['cognitive_structure'] = (
#                 hasattr(agent, 'recursive_cognition') and
#                 hasattr(agent.recursive_cognition, 'update')
#             )
# 
#             # Calculate overall validation
#             validation_results['overall'] = all([
#                 validation_results.get('quantum_state', False),
#                 validation_results.get('surplus_state', False),
#                 validation_results.get('distinction', False)
#             ])
# 
#             return validation_results
# 
#         except Exception as e:
#             logger.error(f"Error in system validation: {e}")
#             traceback.print_exc()
#             return {'overall': False, 'error': str(e)}
# 
# 
#

"""# 12. Analysis"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile analysis.py
# import numpy as np
# from typing import Dict, List, Optional, Tuple, Any
# import time
# from collections import deque
# from qiskit import QuantumCircuit
# from qiskit.quantum_info import Statevector
# from qiskit_aer.library import SaveStatevector
# from base_quantum import BaseQuantumState
# from core_quantum import EnhancedQuantumState
# import traceback
# import random
# 
# # Assuming these functions are in your utilities.py
# from utilities import (to_float)
# 
# # Import other necessary modules (if you haven't already)
# import pandas as pd
# from scipy.stats import entropy
# from sklearn.metrics import mutual_info_score
# from statsmodels.tsa.stattools import grangercausalitytests
# from scipy.signal import hilbert
# from qiskit_aer import Aer
# from qiskit.visualization import plot_histogram
# from qiskit import QuantumCircuit, ClassicalRegister, QuantumRegister
# 
# class GroverSearch:
#     """
#     Enhanced Grover's search implementation for quantum state analysis.
#     Implements quantum amplitude amplification with sophisticated error handling.
#     """
#     def __init__(self, num_qubits: int):
#         """
#         Initialize Grover search analyzer.
# 
#         Args:
#             num_qubits: Number of qubits in the system
#         """
#         self.num_qubits = num_qubits
#         self.search_history = deque(maxlen=1000)
#         self.oracle_history = deque(maxlen=100)
#         self.circuit_statistics = {
#             'oracle_depth': [],
#             'diffusion_depth': [],
#             'total_gates': []
#         }
#         # Default simulator for circuit execution
#         try:
#             self.simulator = AerSimulator(method='statevector')
#         except Exception as e:
#             print(f"Error initializing simulator: {e}")
#             self.simulator = None
# 
#     def create_oracle(self, target_state: str) -> QuantumCircuit:
#         """
#         Create oracle circuit for target state.
# 
#         Args:
#             target_state: Binary string representing target state
# 
#         Returns:
#             Quantum circuit implementing the oracle
#         """
#         try:
#             qc = QuantumCircuit(self.num_qubits, name="oracle")
# 
#             # Convert target state to proper binary format
#             try:
#                 int_val = int(target_state, 2)  # Ensure it's a valid binary string
#                 target = format(int_val, f'0{self.num_qubits}b')
#             except ValueError:
#                 print(f"Invalid target state format: {target_state}, using all zeros")
#                 target = '0' * self.num_qubits
# 
#             # Apply X gates to flip 0s in target state
#             for i, bit in enumerate(target):
#                 if bit == '0':
#                     qc.x(i)
# 
#             # Multi-controlled phase flip
#             qc.h(self.num_qubits-1)
#             # Use mcx with controls parameter for better flexibility
#             controls = list(range(self.num_qubits-1))
#             if controls:  # Ensure controls is not empty
#                 qc.mcx(controls, self.num_qubits-1)
#             else:
#                 # If no controls, apply regular X gate
#                 qc.x(self.num_qubits-1)
#             qc.h(self.num_qubits-1)
# 
#             # Uncompute X gates
#             for i, bit in enumerate(target):
#                 if bit == '0':
#                     qc.x(i)
# 
#             # Track circuit statistics
#             self.circuit_statistics['oracle_depth'].append(qc.depth())
#             self.oracle_history.append({
#                 'target_state': target_state,
#                 'circuit_depth': qc.depth(),
#                 'gate_count': sum(1 for _ in qc.data)
#             })
# 
#             return qc
# 
#         except Exception as e:
#             print(f"Error creating oracle: {e}")
#             traceback.print_exc()
#             # Return empty circuit as fallback
#             return QuantumCircuit(self.num_qubits, name="fallback_oracle")
# 
#     def create_diffusion(self) -> QuantumCircuit:
#         """
#         Create diffusion operator circuit.
# 
#         Returns:
#             Quantum circuit implementing the diffusion operator
#         """
#         try:
#             qc = QuantumCircuit(self.num_qubits, name="diffusion")
# 
#             # Apply Hadamard gates
#             for q in range(self.num_qubits):
#                 qc.h(q)
# 
#             # Apply X gates
#             for q in range(self.num_qubits):
#                 qc.x(q)
# 
#             # Apply multi-controlled phase flip
#             qc.h(self.num_qubits-1)
# 
#             # Handle edge case of too few qubits
#             if self.num_qubits > 1:
#                 controls = list(range(self.num_qubits-1))
#                 qc.mcx(controls, self.num_qubits-1)
#             else:
#                 # For single qubit case, apply simpler operation
#                 qc.x(0)
# 
#             qc.h(self.num_qubits-1)
# 
#             # Uncompute X gates
#             for q in range(self.num_qubits):
#                 qc.x(q)
# 
#             # Final Hadamard gates
#             for q in range(self.num_qubits):
#                 qc.h(q)
# 
#             # Track circuit statistics
#             self.circuit_statistics['diffusion_depth'].append(qc.depth())
# 
#             return qc
# 
#         except Exception as e:
#             print(f"Error creating diffusion operator: {e}")
#             traceback.print_exc()
#             # Return empty circuit as fallback
#             return QuantumCircuit(self.num_qubits, name="fallback_diffusion")
# 
#     def search(self, quantum_state: 'EnhancedQuantumState',
#                target_state: str) -> Dict[str, float]:
#         """
#         Perform Grover's search for target state.
# 
#         Args:
#             quantum_state: Quantum state to search
#             target_state: Binary string representing target state
# 
#         Returns:
#             Dictionary containing search results and statistics
#         """
#         try:
#             # Validate input
#             if not hasattr(quantum_state, 'execute_circuit'):
#                 print("Error: quantum_state must have execute_circuit method")
#                 return self._default_search_result(target_state)
# 
#             # Validate target state
#             try:
#                 int_val = int(target_state, 2)  # Ensure it's a valid binary string
#                 if len(target_state) != self.num_qubits:
#                     print(f"Warning: Target state length {len(target_state)} doesn't match num_qubits {self.num_qubits}")
#                     target_state = format(int_val, f'0{self.num_qubits}b')
#             except ValueError:
#                 print(f"Invalid target state format: {target_state}")
#                 return self._default_search_result(target_state)
# 
#             # Calculate optimal number of iterations
#             N = 2 ** self.num_qubits
#             iterations = max(1, int(np.pi / 4 * np.sqrt(N)))
# 
#             # Create main circuit
#             qc = QuantumCircuit(self.num_qubits, self.num_qubits)
# 
#             # Initialize superposition
#             for q in range(self.num_qubits):
#                 qc.h(q)
# 
#             # Create oracle and diffusion operators
#             oracle = self.create_oracle(target_state)
#             diffusion = self.create_diffusion()
# 
#             # Apply Grover iterations
#             for _ in range(iterations):
#                 qc.append(oracle, list(range(self.num_qubits)))
#                 qc.append(diffusion, list(range(self.num_qubits)))
# 
#             # Add measurements
#             qc.measure(range(self.num_qubits), range(self.num_qubits))
# 
#             # Track total circuit statistics
#             self.circuit_statistics['total_gates'].append(sum(1 for _ in qc.data))
# 
#             # Execute circuit
#             try:
#                 counts = quantum_state.execute_circuit(qc)
#             except Exception as exec_err:
#                 print(f"Error executing circuit: {exec_err}")
#                 traceback.print_exc()
# 
#                 # Try direct execution with simulator as fallback
#                 if self.simulator:
#                     try:
#                         result = self.simulator.run(qc).result()
#                         counts = result.get_counts()
#                     except Exception as sim_err:
#                         print(f"Simulator fallback failed: {sim_err}")
#                         return self._default_search_result(target_state)
#                 else:
#                     return self._default_search_result(target_state)
# 
#             # Calculate total shots and probability
#             total_shots = sum(counts.values())
#             probability = counts.get(target_state, 0) / total_shots if total_shots > 0 else 0
# 
#             # Prepare result
#             result = {
#                 'target_state': target_state,
#                 'probability': probability,
#                 'iterations': iterations,
#                 'total_shots': total_shots,
#                 'circuit_depth': qc.depth(),
#                 'success_threshold': 0.5,
#                 'counts': counts
#             }
# 
#             # Track search history
#             self.search_history.append({
#                 **result,
#                 'timestamp': time.time()
#             })
# 
#             return result
# 
#         except Exception as e:
#             print(f"Error in Grover search: {e}")
#             traceback.print_exc()
#             return self._default_search_result(target_state)
# 
#     def _default_search_result(self, target_state: str) -> Dict[str, float]:
#         """Create default search result when search fails."""
#         return {
#             'target_state': target_state,
#             'probability': 0.0,
#             'iterations': 0,
#             'total_shots': 0,
#             'circuit_depth': 0,
#             'success_threshold': 0.5,
#             'error': True
#         }
# 
#     def get_search_statistics(self) -> Dict[str, Any]:
#         """
#         Get comprehensive search statistics.
# 
#         Returns:
#             Dictionary containing search performance statistics
#         """
#         try:
#             if not self.search_history:
#                 return {
#                     'total_searches': 0,
#                     'success_rate': 0.0,
#                     'mean_probability': 0.0
#                 }
# 
#             recent_searches = list(self.search_history)[-100:]
# 
#             stats = {
#                 'mean_probability': np.mean([s['probability'] for s in recent_searches]),
#                 'success_rate': np.mean([
#                     1.0 if s['probability'] >= s['success_threshold'] else 0.0
#                     for s in recent_searches
#                 ]),
#                 'mean_circuit_depth': np.mean([s['circuit_depth'] for s in recent_searches]),
#                 'mean_oracle_depth': np.mean(self.circuit_statistics['oracle_depth']) if self.circuit_statistics['oracle_depth'] else 0,
#                 'mean_diffusion_depth': np.mean(self.circuit_statistics['diffusion_depth']) if self.circuit_statistics['diffusion_depth'] else 0,
#                 'mean_total_gates': np.mean(self.circuit_statistics['total_gates']) if self.circuit_statistics['total_gates'] else 0,
#                 'total_searches': len(self.search_history)
#             }
# 
#             return stats
# 
#         except Exception as e:
#             print(f"Error getting search statistics: {e}")
#             traceback.print_exc()
#             return {'error': str(e)}
# 
# class InformationAnalysis:
#     """
#     Enhanced information theory analysis tools for quantum state evaluation.
#     Implements sophisticated mutual information and transfer entropy calculations.
#     """
#     def __init__(self, window_size: int = 100):
#         """
#         Initialize information analysis system.
# 
#         Args:
#             window_size: Size of sliding window for analysis
#         """
#         self.window_size = window_size
#         self.history = {
#             'mutual_info': deque(maxlen=1000),
#             'transfer_entropy': deque(maxlen=1000),
#             'entropy_rate': deque(maxlen=1000)
#         }
#         self.analysis_stats = {
#             'peak_mi': 0.0,
#             'peak_te': 0.0,
#             'cumulative_info': 0.0
#         }
#         # Keep track of computation errors
#         self.error_history = deque(maxlen=100)
#         self.success_rates = {
#             'mutual_info': 1.0,
#             'transfer_entropy': 1.0,
#             'entropy_rate': 1.0
#         }
# 
#     def compute_mutual_information(self, sequence1: np.ndarray,
#                                  sequence2: np.ndarray,
#                                  bins: int = 10) -> float:
#         """
#         Compute mutual information between two sequences with NaN handling.
# 
#         Args:
#             sequence1: First sequence
#             sequence2: Second sequence
#             bins: Number of bins for discretization
# 
#         Returns:
#             Mutual information value
#         """
#         try:
#             # Input validation
#             if not isinstance(sequence1, np.ndarray):
#                 sequence1 = np.array(sequence1, dtype=float)
#             if not isinstance(sequence2, np.ndarray):
#                 sequence2 = np.array(sequence2, dtype=float)
# 
#             # Handle NaN values
#             sequence1 = np.nan_to_num(sequence1, nan=0.0)
#             sequence2 = np.nan_to_num(sequence2, nan=0.0)
# 
#             # Check array sizes and dimensions
#             if sequence1.size == 0 or sequence2.size == 0:
#                 self._track_error('mutual_info', "Empty input sequence")
#                 return 0.0
# 
#             # Ensure sequences are the same length
#             min_len = min(len(sequence1), len(sequence2))
#             sequence1 = sequence1[:min_len]
#             sequence2 = sequence2[:min_len]
# 
#             # Remove constant sequences
#             if np.all(sequence1 == sequence1[0]) or np.all(sequence2 == sequence2[0]):
#                 self._track_error('mutual_info', "Constant sequence detected")
#                 return 0.0
# 
#             # Add small noise to prevent constant values
#             sequence1 += np.random.normal(0, 1e-10, sequence1.shape)
#             sequence2 += np.random.normal(0, 1e-10, sequence2.shape)
# 
#             # Adjust bin count if needed
#             adjusted_bins = min(bins, len(sequence1) // 5)
#             if adjusted_bins < 2:
#                 adjusted_bins = 2
# 
#             # Discretize sequences with error handling
#             try:
#                 s1_disc = pd.qcut(sequence1, adjusted_bins, labels=False, duplicates='drop')
#                 s2_disc = pd.qcut(sequence2, adjusted_bins, labels=False, duplicates='drop')
#             except ValueError:
#                 # Fall back to uniform bins if qcut fails
#                 try:
#                     s1_disc = np.digitize(sequence1, np.linspace(min(sequence1), max(sequence1), adjusted_bins))
#                     s2_disc = np.digitize(sequence2, np.linspace(min(sequence2), max(sequence2), adjusted_bins))
#                 except Exception as binning_error:
#                     self._track_error('mutual_info', f"Binning error: {binning_error}")
#                     return 0.0
# 
#             # Compute mutual information
#             mi = mutual_info_score(s1_disc, s2_disc)
# 
#             # Update statistics
#             self.history['mutual_info'].append(mi)
#             self.analysis_stats['peak_mi'] = max(
#                 self.analysis_stats['peak_mi'],
#                 mi
#             )
#             self.analysis_stats['cumulative_info'] += mi
# 
#             # Update success rate
#             self._track_success('mutual_info')
# 
#             return float(mi)
# 
#         except Exception as e:
#             self._track_error('mutual_info', str(e))
#             traceback.print_exc()
#             return 0.0
# 
#     def compute_transfer_entropy(self, source: np.ndarray,
#                                target: np.ndarray,
#                                delay: int = 1) -> float:
#         """
#         Compute transfer entropy with robust error handling.
# 
#         Args:
#             source: Source sequence
#             target: Target sequence
#             delay: Time delay
# 
#         Returns:
#             Transfer entropy value
#         """
#         try:
#             # Input validation
#             if not isinstance(source, np.ndarray):
#                 source = np.array(source, dtype=float)
#             if not isinstance(target, np.ndarray):
#                 target = np.array(target, dtype=float)
# 
#             # Handle NaN values
#             source = np.nan_to_num(source, nan=0.0)
#             target = np.nan_to_num(target, nan=0.0)
# 
#             # Check array sizes
#             if source.size == 0 or target.size == 0:
#                 self._track_error('transfer_entropy', "Empty input sequence")
#                 return 0.0
# 
#             # Ensure minimum sequence length
#             if len(source) < delay + 1 or len(target) < delay + 1:
#                 self._track_error('transfer_entropy', "Sequence too short for delay")
#                 return 0.0
# 
#             # Ensure sequences have the same length
#             min_len = min(len(source), len(target))
#             source = source[:min_len]
#             target = target[:min_len]
# 
#             # Add small noise to prevent constant values
#             source += np.random.normal(0, 1e-10, source.shape)
#             target += np.random.normal(0, 1e-10, target.shape)
# 
#             # Prepare delayed sequences
#             source_past = source[:-delay]
#             target_past = target[:-delay]
#             target_present = target[delay:]
# 
#             # Check if sequences are now too short after delay
#             if len(source_past) < 2:
#                 self._track_error('transfer_entropy', "Sequence too short after delay")
#                 return 0.0
# 
#             # Compute joint and marginal histograms
#             try:
#                 # Adaptive bin count based on data length
#                 bin_count = min(20, max(5, len(source_past) // 10))
# 
#                 joint_hist, _, _ = np.histogram2d(
#                     source_past,
#                     target_present,
#                     bins=bin_count,
#                     density=True
#                 )
#                 marg_hist, _ = np.histogram(
#                     target_past,
#                     bins=bin_count,
#                     density=True
#                 )
# 
#                 # Ensure valid probability distributions
#                 joint_hist = np.clip(joint_hist, 1e-10, None)
#                 joint_hist /= joint_hist.sum()
# 
#                 marg_hist = np.clip(marg_hist, 1e-10, None)
#                 marg_hist /= marg_hist.sum()
# 
#                 # Compute transfer entropy
#                 te = float(entropy(joint_hist.flatten(), base=2)) - float(entropy(marg_hist, base=2))
# 
#                 # Ensure non-negative transfer entropy
#                 te = max(0.0, te)
# 
#                 # Update history and stats
#                 self.history['transfer_entropy'].append(te)
#                 self.analysis_stats['peak_te'] = max(self.analysis_stats['peak_te'], te)
# 
#                 # Update success rate
#                 self._track_success('transfer_entropy')
# 
#                 return te
# 
#             except Exception as hist_error:
#                 self._track_error('transfer_entropy', f"Histogram error: {hist_error}")
#                 return 0.0
# 
#         except Exception as e:
#             self._track_error('transfer_entropy', str(e))
#             traceback.print_exc()
#             return 0.0
# 
#     def compute_entropy_rate(self, sequence: np.ndarray,
#                            window_size: Optional[int] = None) -> float:
#         """
#         Compute entropy rate of a sequence.
# 
#         Args:
#             sequence: Input sequence
#             window_size: Optional custom window size
# 
#         Returns:
#             Entropy rate value
#         """
#         try:
#             # Input validation
#             if not isinstance(sequence, np.ndarray):
#                 sequence = np.array(sequence, dtype=float)
# 
#             # Handle NaN values
#             sequence = np.nan_to_num(sequence, nan=0.0)
# 
#             # Check array size
#             if sequence.size == 0:
#                 self._track_error('entropy_rate', "Empty input sequence")
#                 return 0.0
# 
#             if window_size is None:
#                 window_size = self.window_size
# 
#             # Ensure window size is reasonable
#             window_size = min(window_size, len(sequence) // 2)
#             if window_size < 2:
#                 self._track_error('entropy_rate', "Window size too small")
#                 return 0.0
# 
#             # Use sliding windows
#             windows = [
#                 sequence[i:i+window_size]
#                 for i in range(len(sequence) - window_size + 1)
#             ]
# 
#             if not windows:
#                 self._track_error('entropy_rate', "No valid windows")
#                 return 0.0
# 
#             # Compute entropy for each window
#             entropies = []
#             for w in windows:
#                 # Adaptive bin count based on window size
#                 bin_count = min(10, max(2, len(w) // 5))
#                 hist, _ = np.histogram(w, bins=bin_count, density=True)
#                 hist = hist[hist > 0]  # Remove zeros to avoid log(0)
#                 entropies.append(entropy(hist))
# 
#             # Compute entropy rate as mean of entropy differences
#             diffs = np.diff(entropies)
#             rate = np.mean(diffs) if len(diffs) > 0 else 0.0
# 
#             # Update history
#             self.history['entropy_rate'].append(rate)
# 
#             # Update success rate
#             self._track_success('entropy_rate')
# 
#             return float(rate)
# 
#         except Exception as e:
#             self._track_error('entropy_rate', str(e))
#             traceback.print_exc()
#             return 0.0
# 
#     def _track_error(self, computation_type: str, error_msg: str) -> None:
#         """Track computation errors for monitoring."""
#         self.error_history.append({
#             'type': computation_type,
#             'error': error_msg,
#             'timestamp': time.time()
#         })
# 
#         # Update success rate
#         error_counts = sum(1 for e in self.error_history if e['type'] == computation_type)
#         total_attempts = len(self.history[computation_type]) + error_counts
#         if total_attempts > 0:
#             self.success_rates[computation_type] = 1.0 - (error_counts / total_attempts)
# 
#     def _track_success(self, computation_type: str) -> None:
#         """Track successful computations."""
#         # Update success rate
#         error_counts = sum(1 for e in self.error_history if e['type'] == computation_type)
#         total_attempts = len(self.history[computation_type]) + error_counts
#         if total_attempts > 0:
#             self.success_rates[computation_type] = 1.0 - (error_counts / total_attempts)
# 
#     def analyze_sequences(self, sequences: Dict[str, np.ndarray],
#                      reference_key: Optional[str] = None) -> Dict[str, float]:
#         """
#         Perform comprehensive information analysis on multiple sequences.
# 
#         Args:
#             sequences: Dictionary of named sequences
#             reference_key: Optional key for reference sequence
# 
#         Returns:
#             Dictionary of analysis results
#         """
#         try:
#             results = {}
# 
#             # Validate input
#             if not isinstance(sequences, dict) or not sequences:
#                 return {'error': 'Invalid or empty sequences dictionary'}
# 
#             if reference_key is None:
#                 reference_key = list(sequences.keys())[0]
# 
#             if reference_key not in sequences:
#                 print(f"Reference key '{reference_key}' not found in sequences")
#                 reference_key = list(sequences.keys())[0]
# 
#             reference = sequences[reference_key]
# 
#             # Validate reference sequence
#             if not isinstance(reference, np.ndarray):
#                 reference = np.array(reference, dtype=float)
# 
#             # Handle NaN values
#             reference = np.nan_to_num(reference, nan=0.0)
# 
#             if reference.size == 0:
#                 return {'error': 'Reference sequence is empty'}
# 
#             # Compute pairwise mutual information and transfer entropy
#             for key, seq in sequences.items():
#                 if key != reference_key:
#                     # Validate sequence
#                     if not isinstance(seq, np.ndarray):
#                         seq = np.array(seq, dtype=float)
# 
#                     # Handle NaN values
#                     seq = np.nan_to_num(seq, nan=0.0)
# 
#                     if seq.size == 0:
#                         results[f'mi_{reference_key}_{key}'] = 0.0
#                         results[f'te_{reference_key}_{key}'] = 0.0
#                         continue
# 
#                     # Check for constant sequences
#                     if np.std(reference) < 1e-10 or np.std(seq) < 1e-10:
#                         results[f'mi_{reference_key}_{key}'] = 0.0
#                         results[f'te_{reference_key}_{key}'] = 0.0
#                         continue
# 
#                     # Compute mutual information
#                     mi = self.compute_mutual_information(reference, seq)
#                     results[f'mi_{reference_key}_{key}'] = mi
# 
#                     # Compute transfer entropy (source → target)
#                     te_ref_to_seq = self.compute_transfer_entropy(reference, seq)
#                     results[f'te_{reference_key}_{key}'] = te_ref_to_seq
# 
#                     # Compute transfer entropy (target → source)
#                     te_seq_to_ref = self.compute_transfer_entropy(seq, reference)
#                     results[f'te_{key}_{reference_key}'] = te_seq_to_ref
# 
#                     # Compute directionality
#                     if te_ref_to_seq > 0 or te_seq_to_ref > 0:
#                         directionality = (te_ref_to_seq - te_seq_to_ref) / max(te_ref_to_seq + te_seq_to_ref, 1e-10)
#                         results[f'directionality_{reference_key}_{key}'] = directionality
# 
#             # Compute individual entropy rates
#             for key, seq in sequences.items():
#                 # Validate sequence
#                 if not isinstance(seq, np.ndarray):
#                     seq = np.array(seq, dtype=float)
# 
#                 # Handle NaN values
#                 seq = np.nan_to_num(seq, nan=0.0)
# 
#                 if seq.size == 0:
#                     results[f'entropy_rate_{key}'] = 0.0
#                     continue
# 
#                 rate = self.compute_entropy_rate(seq)
#                 results[f'entropy_rate_{key}'] = rate
# 
#             # Add success rate metrics
#             results.update({
#                 'success_rate_mi': self.success_rates['mutual_info'],
#                 'success_rate_te': self.success_rates['transfer_entropy'],
#                 'success_rate_er': self.success_rates['entropy_rate']
#             })
# 
#             return results
# 
#         except Exception as e:
#             print(f"Error in sequence analysis: {e}")
#             traceback.print_exc()
#             return {'error': str(e)}
# 
#     def _safe_correlation(self, x: np.ndarray, y: np.ndarray) -> float:
#         """
#         Calculate correlation with proper error handling for division by zero issues.
# 
#         Args:
#             x: First data array
#             y: Second data array
# 
#         Returns:
#             Correlation coefficient or 0.0 if calculation fails
#         """
#         try:
#             if len(x) < 2 or len(y) < 2:
#                 return 0.0
# 
#             # First, handle NaN values
#             x = np.nan_to_num(x, nan=0.0)
#             y = np.nan_to_num(y, nan=0.0)
# 
#             # Calculate standard deviations
#             std_x = np.std(x)
#             std_y = np.std(y)
# 
#             # Check for constant arrays which would cause division by zero
#             if std_x < 1e-10 or std_y < 1e-10:
#                 # Add small random noise to prevent constant arrays
#                 if std_x < 1e-10:
#                     x = x + np.random.normal(0, 1e-5, size=x.shape)
#                     std_x = np.std(x)
# 
#                 if std_y < 1e-10:
#                     y = y + np.random.normal(0, 1e-5, size=y.shape)
#                     std_y = np.std(y)
# 
#                 # If still constant after adding noise, return 0
#                 if std_x < 1e-10 or std_y < 1e-10:
#                     return 0.0
# 
#             # Manually calculate correlation to avoid NumPy warning
#             x_normalized = (x - np.mean(x)) / std_x
#             y_normalized = (y - np.mean(y)) / std_y
#             correlation = np.mean(x_normalized * y_normalized)
# 
#             # Check for NaN results
#             if np.isnan(correlation):
#                 return 0.0
# 
#             return float(correlation)
#         except Exception as e:
#             print(f"Error calculating correlation: {e}")
#             return 0.0
# 
#     def get_analysis_statistics(self) -> Dict[str, float]:
#         """
#         Get comprehensive analysis statistics.
# 
#         Returns:
#             Dictionary of analysis statistics
#         """
#         try:
#             stats = {
#                 'peak_mutual_info': self.analysis_stats['peak_mi'],
#                 'peak_transfer_entropy': self.analysis_stats['peak_te'],
#                 'cumulative_info': self.analysis_stats['cumulative_info']
#             }
# 
#             # Add historical statistics
#             for key, history in self.history.items():
#                 if history:
#                     stats[f'mean_{key}'] = float(np.mean(history))
#                     stats[f'std_{key}'] = float(np.std(history))
#                     stats[f'min_{key}'] = float(np.min(history))
#                     stats[f'max_{key}'] = float(np.max(history))
# 
#             # Add success rates
#             stats.update(self.success_rates)
# 
#             # Add error statistics
#             error_types = defaultdict(int)
#             for error in self.error_history:
#                 error_types[error['type']] += 1
# 
#             stats['total_errors'] = len(self.error_history)
#             for error_type, count in error_types.items():
#                 stats[f'errors_{error_type}'] = count
# 
#             return stats
# 
#         except Exception as e:
#             print(f"Error getting analysis statistics: {e}")
#             traceback.print_exc()
#             return {'error': str(e)}
# 
# class CausalityAnalysis:
#     """
#     Enhanced causality analysis with improved stability and reliability metrics.
#     Implements sophisticated Granger causality and phase synchronization calculations.
#     """
#     def __init__(self, num_layers: int = 4, complexity_factor: float = 1.0, max_lag: int = 10):
#         """
#         Initialize causality analysis with configurable complexity and lag.
# 
#         Args:
#             num_layers: Number of layers for causal analysis
#             complexity_factor: Scaling factor for causal complexity
#             max_lag: Maximum lag for temporal analysis
#         """
#         # Ensure num_layers is at least 1
#         self.num_layers = max(1, num_layers)
# 
#         # Initialize causal tracking attributes
#         self.directionality = 0.0  # Overall causal directionality (duplicated in original)
#         self.strength = 0.0  # Overall causal strength (duplicated in original)
#         self.complexity_factor = complexity_factor  # (duplicated in original)
#         self.max_lag = max_lag
#         self.causal_entropy = 0.0
#         self.temporal_coherence = 0.0
# 
#         # Initialize causality matrix
#         self.causality_matrix = np.zeros((self.num_layers, self.num_layers))
# 
#         # Layer-specific causal parameters
#         self.layer_causal_weights = np.ones(self.num_layers, dtype=np.float64)
#         self.layer_intervention_sensitivity = np.ones(self.num_layers, dtype=np.float64)
# 
#         # Historical tracking
#         self.causal_history = []
#         self.intervention_history = []
#         self.analysis_history = deque(maxlen=1000)
#         self.error_history = deque(maxlen=100)
# 
#         # Intervention and tracking parameters
#         self.max_historical_entries = 1000
#         self.current_intervention_count = 0
# 
#         # Stability metrics
#         self.stability_metrics = {
#             'granger_reliability': 1.0,
#             'sync_stability': 1.0,
#             'phase_consistency': 1.0
#         }
# 
#         # Momentum tracking
#         self.phase_momentum = 0.0
#         self.sync_momentum = 0.0
# 
#         # Success rate tracking
#         self.success_rates = {
#             'granger': 1.0,
#             'sync': 1.0
#         }
# 
#     def update_causality_matrix(self, quantum_metrics: Dict[str, float],
#                                  cognitive_metrics: Dict[str, float]) -> None:
#         """
#         Update the causality matrix based on quantum and cognitive metrics.
# 
#         Args:
#             quantum_metrics: Dictionary of quantum system metrics
#             cognitive_metrics: Dictionary of cognitive system metrics
#         """
#         try:
#             # Validate input metrics
#             if not isinstance(quantum_metrics, dict) or not isinstance(cognitive_metrics, dict):
#                 raise ValueError("Invalid metrics input")
# 
#             # Extract key metrics with default values
#             coherence = float(quantum_metrics.get('phase_coherence', 0.5))
#             entropy = float(quantum_metrics.get('normalized_entropy', 0.5))
#             phase_distinction = float(quantum_metrics.get('phase_distinction', 0.5))
#             cognitive_strength = float(cognitive_metrics.get('mean_strength', 1.0))
#             cognitive_stability = float(cognitive_metrics.get('mean_stability', 0.5))
#             return self.cognitive_metrics
# 
#             # Ensure causality matrix has correct dimensions
#             if self.causality_matrix.shape != (self.num_layers, self.num_layers):
#                 self.causality_matrix = np.zeros((self.num_layers, self.num_layers), dtype=np.float64)
# 
#             # Dynamic update of causality matrix
#             for i in range(self.num_layers):
#                 for j in range(self.num_layers):
#                     if i != j:  # Avoid self-causality
#                         # Multi-factor causal weight calculation
#                         causal_factor = (
#                             coherence *
#                             (1 - entropy) *
#                             phase_distinction *
#                             cognitive_strength *
#                             (0.5 + cognitive_stability)
#                         )
# 
#                         # Distance-based attenuation
#                         distance_attenuation = 1.0 / (1.0 + abs(i - j))
# 
#                         # Update matrix with complexity scaling
#                         update_value = (
#                             causal_factor *
#                             distance_attenuation *
#                             self.complexity_factor
#                         )
# 
#                         # Safe matrix update
#                         self.causality_matrix[i, j] = min(
#                             max(0, update_value),  # Ensure non-negative
#                             1.0  # Cap at 1.0
#                         )
# 
#             # Update global causal metrics
#             self.directionality = float(np.mean(np.abs(np.diff(self.causality_matrix, axis=0))) if self.num_layers > 1 else 0.0)
#             self.strength = float(np.mean(np.abs(self.causality_matrix)))
# 
#             # Compute causal entropy with safe logarithm
#             normalized_matrix = self.causality_matrix / (np.sum(self.causality_matrix) + 1e-10)
#             safe_log_matrix = np.where(normalized_matrix > 0, normalized_matrix, 1e-10)
#             self.causal_entropy = -np.sum(safe_log_matrix * np.log2(safe_log_matrix))
# 
#             # Track history with limit
#             causal_snapshot = {
#                 'matrix': self.causality_matrix.copy(),
#                 'directionality': self.directionality,
#                 'strength': self.strength,
#                 'causal_entropy': self.causal_entropy,
#                 'timestamp': time.time()
#             }
# 
#             self.causal_history.append(causal_snapshot)
#             if len(self.causal_history) > self.max_historical_entries:
#                 self.causal_history.pop(0)
# 
#         except Exception as e:
#             print(f"Error updating causality matrix: {e}")
#             # Fallback: reset or maintain previous state
#             self.reset_causality_matrix()
# 
#     def reset_causality_matrix(self) -> None:
#         """Reset causality matrix to initial state."""
#         self.causality_matrix = np.zeros((self.num_layers, self.num_layers), dtype=np.float64)
#         self.directionality = 0.0
#         self.strength = 0.0
#         self.causal_entropy = 0.0
# 
#     def detect_causal_transitions(self) -> List[Dict[str, Any]]:
#         """
#         Detect significant causal transitions in the system.
# 
#         Returns:
#             List of detected causal transition events
#         """
#         transitions = []
# 
#         if len(self.causal_history) < 2:
#             return transitions
# 
#         for i in range(1, len(self.causal_history)):
#             prev = self.causal_history[i-1]
#             curr = self.causal_history[i]
# 
#             # Compute matrix difference
#             matrix_diff = np.abs(curr['matrix'] - prev['matrix'])
#             diff_magnitude = np.mean(matrix_diff)
# 
#             # Detect significant transitions
#             if (diff_magnitude > 0.1 and  # Substantial change
#                 (abs(curr['directionality'] - prev['directionality']) > 0.05 or  # Directionality shift
#                  abs(curr['strength'] - prev['strength']) > 0.05)):  # Strength change
# 
#                 transition = {
#                     'timestamp': curr['timestamp'],
#                     'diff_magnitude': float(diff_magnitude),
#                     'directionality_change': float(curr['directionality'] - prev['directionality']),
#                     'strength_change': float(curr['strength'] - prev['strength']),
#                     'causal_entropy_change': float(curr['causal_entropy'] - prev['causal_entropy'])
#                 }
#                 transitions.append(transition)
# 
#         return transitions
# 
#     def get_causal_metrics(self) -> Dict[str, float]:
#         """
#         Retrieve comprehensive causal metrics.
# 
#         Returns:
#             Dictionary of causal metrics
#         """
#         return {
#             'directionality': self.directionality,
#             'strength': self.strength,
#             'causal_entropy': self.causal_entropy,
#             'complexity_factor': self.complexity_factor,
#             'total_causal_potential': float(np.sum(self.causality_matrix))
#         }
# 
#     def update(self, coherence_data: List[float], distinction_data: List[float], entropy_data: List[float]) -> None:
#         """
#         Update causality analysis with new data.
# 
#         Args:
#             coherence_data: List of coherence values
#             distinction_data: List of distinction values
#             entropy_data: List of entropy values
#         """
#         try:
#             # Ensure we have enough data
#             min_len = min(len(coherence_data), len(distinction_data), len(entropy_data))
#             if min_len < 10:
#                 return  # Not enough data for meaningful causality analysis
# 
#             # Create a mapping of names to indices
#             source_names = ['coherence', 'distinction', 'entropy']
# 
#             # Prepare data for causality analysis
#             data = {
#                 'coherence': coherence_data[-min_len:],
#                 'distinction': distinction_data[-min_len:],
#                 'entropy': entropy_data[-min_len:]
#             }
# 
#             # Calculate pairwise Granger causality
#             for i, source in enumerate(source_names):
#                 for j, target in enumerate(source_names):
#                     if i != j:  # Avoid self-causality
#                         # Calculate time-lagged correlation
#                         source_data = np.array(data[source][:-1], dtype=float)  # t
#                         target_data = np.array(data[target][1:], dtype=float)   # t+1
# 
#                         if len(source_data) >= 3:  # Need at least 3 points
#                             # Handle NaN values
#                             source_data = np.nan_to_num(source_data, nan=0.0)
#                             target_data = np.nan_to_num(target_data, nan=0.0)
# 
#                             # Calculate standard deviations
#                             source_std = np.std(source_data)
#                             target_std = np.std(target_data)
# 
#                             # Only calculate correlation if data is not constant
#                             if source_std < 1e-8 or target_std < 1e-8:
#                                 correlation = 0.0
#                             else:
#                                 # Safe correlation calculation
#                                 source_normalized = (source_data - np.mean(source_data)) / source_std
#                                 target_normalized = (target_data - np.mean(target_data)) / target_std
#                                 correlation = np.mean(source_normalized * target_normalized)
# 
#                                 # Handle NaN result
#                                 if np.isnan(correlation):
#                                     correlation = 0.0
# 
#                             # Use proper integer indexing for the causality matrix
#                             self.causality_matrix[i, j] = correlation
# 
#             # Update directionality and strength based on causality matrix
#             self._update_metrics()
#         except Exception as e:
#             print(f"Error in causality analysis update: {e}")
# 
#     def _update_metrics(self):
#         """Update directionality and strength metrics based on the causality matrix."""
#         try:
#             # Calculate directionality as average absolute difference between matrix elements
#             self.directionality = float(np.mean(np.abs(np.diff(self.causality_matrix, axis=0))) if self.num_layers > 1 else 0.0)
# 
#             # Calculate strength as average absolute value of matrix elements
#             self.strength = float(np.mean(np.abs(self.causality_matrix)))
# 
#             # Calculate causal entropy
#             # Add small epsilon to avoid log(0)
#             normalized_matrix = self.causality_matrix / (np.sum(self.causality_matrix) + 1e-10)
#             # Only compute log for non-zero elements
#             nonzero_mask = normalized_matrix > 1e-8
#             if np.any(nonzero_mask):
#                 safe_log_matrix = np.zeros_like(normalized_matrix)
#                 safe_log_matrix[nonzero_mask] = np.log2(normalized_matrix[nonzero_mask])
#                 self.causal_entropy = float(-np.sum(normalized_matrix[nonzero_mask] * safe_log_matrix[nonzero_mask]))
#             else:
#                 self.causal_entropy = 0.0
# 
#         except Exception as e:
#             print(f"Error updating causality metrics: {e}")
# 
#     def get_results(self) -> Dict[str, float]:
#         """
#         Get causality analysis results.
# 
#         Returns:
#             Dictionary of causality metrics
#         """
#         return {
#             'causality_strength': self.strength,
#             'causality_directionality': self.directionality,
#             'causal_entropy': self.causal_entropy,
#             'causality_matrix_stability': self.stability_metrics['transition_stability'] if hasattr(self, 'stability_metrics') else 1.0
#         }
# 
#     def granger_causality(self, sequence1: np.ndarray,
#                          sequence2: np.ndarray) -> Dict[str, float]:
#         """
#         Perform enhanced Granger causality tests with robust error handling.
# 
#         Args:
#             sequence1: First sequence
#             sequence2: Second sequence
# 
#         Returns:
#             Dictionary with Granger causality test results
#         """
#         try:
#             # Validate sequences
#             if not self._validate_sequences(sequence1, sequence2):
#                 self._track_error('granger', "Invalid input sequences")
#                 return self._default_granger_results()
# 
#             # Prepare data with enhanced preprocessing
#             sequence1_proc = self._preprocess_sequence(sequence1)
#             sequence2_proc = self._preprocess_sequence(sequence2)
# 
#             # Add small noise to prevent constant values
#             sequence1_proc += np.random.normal(0, 1e-6, sequence1_proc.shape)
#             sequence2_proc += np.random.normal(0, 1e-6, sequence2_proc.shape)
# 
#             # Create pandas DataFrame
#             try:
#                 data = pd.DataFrame({
#                     'x': sequence1_proc,
#                     'y': sequence2_proc
#                 })
#             except Exception as df_error:
#                 self._track_error('granger', f"DataFrame creation error: {df_error}")
#                 return self._default_granger_results()
# 
#             causality_metrics = {}
#             reliability_scores = []
# 
#             # Perform Granger causality tests for each lag
#             for lag in range(1, min(self.max_lag + 1, len(data) // 5)):  # Limit lag to 1/5 of data length
#                 try:
#                     # Check for constant values
#                     if np.all(data['x'].diff().dropna() == 0) or np.all(data['y'].diff().dropna() == 0):
#                         print(f"Warning: Constant values detected at lag {lag}")
#                         continue
# 
#                     # Run Granger test with timeout protection
#                     test_result = grangercausalitytests(
#                         data,
#                         maxlag=lag,
#                         verbose=False,
#                         addconst=True
#                     )
# 
#                     f_stat = test_result[lag][0]['ssr_ftest'][0]
#                     p_value = test_result[lag][0]['ssr_ftest'][1]
# 
#                     # Calculate reliability score
#                     reliability = 1.0 - p_value if p_value < 0.05 else 0.0
#                     reliability_scores.append(reliability)
# 
#                     causality_metrics[f'lag_{lag}_f_stat'] = float(f_stat)
#                     causality_metrics[f'lag_{lag}_p_value'] = float(p_value)
#                     causality_metrics[f'lag_{lag}_reliability'] = float(reliability)
#                     causality_metrics[f'lag_{lag}_significant'] = bool(p_value < 0.05)
# 
#                 except Exception as e:
#                     print(f"Error in Granger test for lag {lag}: {e}")
#                     causality_metrics[f'lag_{lag}_f_stat'] = 0.0
#                     causality_metrics[f'lag_{lag}_p_value'] = 1.0
#                     causality_metrics[f'lag_{lag}_reliability'] = 0.0
#                     causality_metrics[f'lag_{lag}_significant'] = False
# 
#             # Update overall reliability metric
#             if reliability_scores:
#                 mean_reliability = float(np.mean(reliability_scores))
#                 self.stability_metrics['granger_reliability'] = float(
#                     0.9 * self.stability_metrics['granger_reliability'] +
#                     0.1 * mean_reliability
#                 )
# 
#                 # Track the most significant lag
#                 significant_lags = [lag for lag in range(1, self.max_lag + 1)
#                                    if causality_metrics.get(f'lag_{lag}_significant', False)]
#                 if significant_lags:
#                     causality_metrics['optimal_lag'] = min(significant_lags)
#                 else:
#                     causality_metrics['optimal_lag'] = 0
# 
#                 causality_metrics['any_significant'] = len(significant_lags) > 0
#                 causality_metrics['significant_lag_count'] = len(significant_lags)
# 
#             causality_metrics['overall_reliability'] = self.stability_metrics['granger_reliability']
# 
#             # Update success tracking
#             self._track_success('granger')
# 
#             # Store analysis history
#             self.analysis_history.append({
#                 'type': 'granger_causality',
#                 'metrics': causality_metrics,
#                 'timestamp': time.time()
#             })
# 
#             return causality_metrics
# 
#         except Exception as e:
#             print(f"Error in Granger causality test: {e}")
#             traceback.print_exc()
#             self._track_error('granger', str(e))
#             return self._default_granger_results()
# 
#     def phase_synchronization(self, sequence1: np.ndarray,
#                             sequence2: np.ndarray) -> Dict[str, float]:
#         """
#         Compute enhanced phase synchronization with improved stability tracking.
# 
#         Args:
#             sequence1: First sequence
#             sequence2: Second sequence
# 
#         Returns:
#             Dictionary with phase synchronization metrics
#         """
#         try:
#             # Validate and preprocess sequences
#             if not self._validate_sequences(sequence1, sequence2):
#                 self._track_error('sync', "Invalid input sequences")
#                 return self._default_sync_results()
# 
#             # Compute analytic signals with enhanced stability
#             analytic1 = self._compute_stable_hilbert(sequence1)
#             analytic2 = self._compute_stable_hilbert(sequence2)
# 
#             if analytic1 is None or analytic2 is None:
#                 self._track_error('sync', "Failed to compute analytic signals")
#                 return self._default_sync_results()
# 
#             # Extract and unwrap phases with proper error handling
#             try:
#                 phase1 = np.unwrap(np.angle(analytic1))
#                 phase2 = np.unwrap(np.angle(analytic2))
#             except Exception as phase_error:
#                 self._track_error('sync', f"Phase extraction error: {phase_error}")
#                 return self._default_sync_results()
# 
#             # Compute phase difference with momentum
#             phase_diff = phase1 - phase2
# 
#             # Update phase momentum for stability
#             mean_phase_diff = float(np.mean(phase_diff))
#             self.phase_momentum = float(
#                 MOMENTUM_DECAY * self.phase_momentum +
#                 (1 - MOMENTUM_DECAY) * mean_phase_diff
#             )
# 
#             # Compute synchronization index with momentum
#             # Use complex exponential form for better numerical stability
#             complex_phases = np.exp(1j * phase_diff)
#             sync_index = float(np.abs(np.mean(complex_phases)))
# 
#             self.sync_momentum = float(
#                 MOMENTUM_DECAY * self.sync_momentum +
#                 (1 - MOMENTUM_DECAY) * (sync_index - 0.5)
#             )
# 
#             # Calculate advanced stability metrics
#             phase_diff_std = float(np.std(phase_diff))
#             phase_diff_entropy = float(self._compute_phase_entropy(phase_diff))
#             sync_stability = float(1.0 / (1.0 + phase_diff_std))
# 
#             # Calculate phase locking value (PLV)
#             plv = sync_index
# 
#             # Calculate weighted phase lag index (WPLI)
#             # This is more robust against volume conduction effects
#             try:
#                 cross_spectrum = analytic1 * np.conjugate(analytic2)
#                 wpli = np.abs(np.imag(cross_spectrum).mean()) / np.abs(cross_spectrum).mean()
#             except Exception:
#                 wpli = 0.0
# 
#             # Update stability metrics
#             self.stability_metrics['sync_stability'] = float(
#                 0.9 * self.stability_metrics['sync_stability'] +
#                 0.1 * sync_stability
#             )
#             self.stability_metrics['phase_consistency'] = float(
#                 0.9 * self.stability_metrics['phase_consistency'] +
#                 0.1 * float(1.0 - np.abs(self.phase_momentum))
#             )
# 
#             # Compute directionality index
#             try:
#                 # Phase slope index (PSI) as directionality measure
#                 n_segments = 4  # Number of segments for PSI calculation
#                 segment_length = len(phase1) // n_segments
#                 psi_values = []
# 
#                 for i in range(n_segments):
#                     start = i * segment_length
#                     end = (i + 1) * segment_length
#                     if end > len(phase1):
#                         break
# 
#                     # Calculate phase slope for this segment
#                     phase_slope = np.mean(np.diff(phase1[start:end]) - np.diff(phase2[start:end]))
#                     psi_values.append(phase_slope)
# 
#                 psi = np.mean(psi_values) if psi_values else 0.0
#                 directionality = np.sign(psi) * min(abs(psi), 1.0)
#             except Exception:
#                 directionality = 0.0
# 
#             # Create result dictionary
#             result = {
#                 'sync_index': sync_index,
#                 'plv': plv,
#                 'wpli': float(wpli),
#                 'mean_phase_diff': mean_phase_diff,
#                 'phase_diff_std': phase_diff_std,
#                 'phase_diff_entropy': phase_diff_entropy,
#                 'sync_stability': self.stability_metrics['sync_stability'],
#                 'phase_consistency': self.stability_metrics['phase_consistency'],
#                 'sync_momentum': self.sync_momentum,
#                 'directionality': float(directionality)
#             }
# 
#             # Update success tracking
#             self._track_success('sync')
# 
#             # Store analysis history
#             self.analysis_history.append({
#                 'type': 'phase_synchronization',
#                 'metrics': result,
#                 'timestamp': time.time()
#             })
# 
#             return result
# 
#         except Exception as e:
#             print(f"Error in phase synchronization analysis: {e}")
#             traceback.print_exc()
#             self._track_error('sync', str(e))
#             return self._default_sync_results()
# 
#     def _compute_phase_entropy(self, phase_diff: np.ndarray) -> float:
#         """Compute entropy of phase difference distribution."""
#         try:
#             # Normalize phase differences to [0, 2π)
#             normalized_phases = phase_diff % (2 * np.pi)
# 
#             # Create histogram (10 bins for phase)
#             hist, _ = np.histogram(normalized_phases, bins=10, density=True)
# 
#             # Ensure histogram is normalized and handle zero values
#             total = np.sum(hist)
#             if total <= 1e-10:
#                 return 0.0
# 
#             hist = hist / total
# 
#             # Avoid log(0) issues
#             hist = hist[hist > 0]
# 
#             # Calculate entropy
#             return -np.sum(hist * np.log2(hist))
#         except Exception:
#             return 0.0
# 
#     def _validate_sequences(self, seq1: np.ndarray, seq2: np.ndarray) -> bool:
#         """Enhanced sequence validation."""
#         try:
#             # Check for None values
#             if seq1 is None or seq2 is None:
#                 return False
# 
#             # Convert to numpy arrays if needed
#             if not isinstance(seq1, np.ndarray):
#                 seq1 = np.array(seq1, dtype=float)
#             if not isinstance(seq2, np.ndarray):
#                 seq2 = np.array(seq2, dtype=float)
# 
#             # Check minimum length
#             if len(seq1) < 2 or len(seq2) < 2:
#                 return False
# 
#             # Handle NaN values
#             if np.isnan(seq1).any():
#                 seq1 = np.nan_to_num(seq1, nan=0.0)
#             if np.isnan(seq2).any():
#                 seq2 = np.nan_to_num(seq2, nan=0.0)
# 
#             # Check for constant values
#             if np.all(seq1 == seq1[0]) or np.all(seq2 == seq2[0]):
#                 return False
# 
#             # Check for infinite values
#             if np.any(np.isinf(seq1)) or np.any(np.isinf(seq2)):
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             print(f"Error in sequence validation: {e}")
#             return False
# 
# 
#     def _preprocess_sequence(self, sequence: np.ndarray) -> np.ndarray:
#         """Enhanced sequence preprocessing."""
#         try:
#             # Convert to numpy array if needed
#             if not isinstance(sequence, np.ndarray):
#                 sequence = np.array(sequence, dtype=float)
# 
#             # Handle NaN values
#             sequence = np.nan_to_num(sequence, nan=0.0)
# 
#             # Remove trends with linear detrending
#             x = np.arange(len(sequence))
#             if len(sequence) > 2:  # Need at least 3 points for meaningful detrending
#                 try:
#                     # Linear fit
#                     z = np.polyfit(x, sequence, 1)
#                     trend = z[0] * x + z[1]
#                     detrended = sequence - trend
#                 except Exception:
#                     # Fallback if fit fails
#                     detrended = sequence - np.mean(sequence)
#             else:
#                 detrended = sequence - np.mean(sequence)
# 
#             # Normalize with stability
#             std = np.std(detrended)
#             if std > 1e-10:
#                 normalized = detrended / std
#             else:
#                 normalized = detrended
# 
#             # Apply smoothing with adaptive kernel size
#             kernel_size = min(5, max(3, len(normalized) // 20))
#             if kernel_size > 1 and len(normalized) > kernel_size:
#                 smoothed = np.convolve(
#                     normalized,
#                     np.ones(kernel_size)/kernel_size,
#                     mode='valid'
#                 )
#             else:
#                 smoothed = normalized
# 
#             return smoothed
# 
#         except Exception as e:
#             print(f"Error in sequence preprocessing: {e}")
#             traceback.print_exc()
#             return sequence
# 
#     def _compute_stable_hilbert(self, sequence: np.ndarray) -> np.ndarray:
#         """Compute Hilbert transform with enhanced stability."""
#         try:
#             # Preprocess sequence for better Hilbert transform
#             if len(sequence) < 2:
#                 return None
# 
#             # Apply window function to reduce edge effects
#             window = np.hanning(len(sequence))
#             windowed = sequence * window
# 
#             # Remove mean to center around zero
#             windowed = windowed - np.mean(windowed)
# 
#             # Zero-pad to next power of 2 for efficient FFT
#             original_length = len(windowed)
#             padded_length = 2 ** int(np.ceil(np.log2(original_length)))
#             padded = np.zeros(padded_length)
#             padded[:original_length] = windowed
# 
#             # Compute Hilbert transform
#             analytic = hilbert(padded)
# 
#             # Return only the original part
#             return analytic[:original_length]
# 
#         except Exception as e:
#             print(f"Error computing Hilbert transform: {e}")
#             traceback.print_exc()
#             return None
# 
#     def _track_error(self, analysis_type: str, error_msg: str) -> None:
#         """Track analysis errors."""
#         self.error_history.append({
#             'type': analysis_type,
#             'error': error_msg,
#             'timestamp': time.time()
#         })
# 
#         # Update success rates
#         error_count = sum(1 for e in self.error_history if e['type'] == analysis_type)
#         total = error_count + sum(1 for a in self.analysis_history
#                                  if a['type'] in [f'{analysis_type}_causality',
#                                                 f'phase_{analysis_type}'])
#         if total > 0:
#             self.success_rates[analysis_type] = 1.0 - (error_count / total)
# 
#     def _track_success(self, analysis_type: str) -> None:
#         """Track successful analyses."""
#         # Update success rates
#         error_count = sum(1 for e in self.error_history if e['type'] == analysis_type)
#         total = error_count + sum(1 for a in self.analysis_history
#                                  if a['type'] in [f'{analysis_type}_causality',
#                                                 f'phase_{analysis_type}'])
#         if total > 0:
#             self.success_rates[analysis_type] = 1.0 - (error_count / total)
# 
#     def _default_granger_results(self) -> Dict[str, float]:
#         """Generate default Granger causality results."""
#         return {
#             'overall_reliability': self.stability_metrics['granger_reliability'],
#             'lag_1_f_stat': 0.0,
#             'lag_1_p_value': 1.0,
#             'lag_1_reliability': 0.0,
#             'lag_1_significant': False,
#             'optimal_lag': 0,
#             'any_significant': False,
#             'significant_lag_count': 0,
#             'error': True
#         }
# 
#     def _default_sync_results(self) -> Dict[str, float]:
#         """Generate default synchronization results."""
#         return {
#             'sync_index': 0.0,
#             'plv': 0.0,
#             'wpli': 0.0,
#             'mean_phase_diff': 0.0,
#             'phase_diff_std': 1.0,
#             'phase_diff_entropy': 0.0,
#             'sync_stability': self.stability_metrics['sync_stability'],
#             'phase_consistency': self.stability_metrics['phase_consistency'],
#             'sync_momentum': 0.0,
#             'directionality': 0.0,
#             'error': True
#         }
# 
#     def get_stability_metrics(self) -> Dict[str, float]:
#         """Get current stability metrics."""
#         return {
#             'granger_reliability': float(self.stability_metrics['granger_reliability']),
#             'sync_stability': float(self.stability_metrics['sync_stability']),
#             'phase_consistency': float(self.stability_metrics['phase_consistency']),
#             'phase_momentum': float(self.phase_momentum),
#             'sync_momentum': float(self.sync_momentum),
#             'success_rate_granger': float(self.success_rates['granger']),
#             'success_rate_sync': float(self.success_rates['sync'])
#         }
# 
# class BayesianAnalysis:
#     """
#     Enhanced Bayesian analysis with improved transition matrix bounds checking
#     and error handling.
#     """
#     def __init__(self, num_states: int = 8):
#         self.num_states = num_states
#         self.prior = np.ones(num_states) / num_states
#         self.transition_matrix = np.ones((num_states, num_states)) / num_states
#         self.observation_history = []
#         self.belief_history = deque(maxlen=1000)
# 
#         # Enhanced stability tracking
#         self.stability_metrics = {
#             'belief_entropy': 0.0,
#             'transition_stability': 1.0,
#             'observation_consistency': 1.0
#         }
#         self.belief_momentum = np.zeros(num_states)
#         self.entropy_momentum = 0.0
#         self.error_history = deque(maxlen=100)
#         self.success_rate = 1.0
# 
#         # Initialize transitions with smoothing
#         self._initialize_transitions()
# 
#     def _initialize_transitions(self) -> None:
#         """Initialize transition matrix with proper structure."""
#         # Create transition matrix with slight bias towards nearby states
#         self.transition_matrix = np.zeros((self.num_states, self.num_states))
#         for i in range(self.num_states):
#             for j in range(self.num_states):
#                 # Higher probability for nearby states, with wrapping
#                 distance = min(abs(i - j), self.num_states - abs(i - j))
#                 self.transition_matrix[i, j] = np.exp(-distance / (self.num_states / 4))
# 
#         # Normalize rows
#         for i in range(self.num_states):
#             self.transition_matrix[i] /= self.transition_matrix[i].sum()
# 
#     def update_beliefs(self, observation: int,
#                       quantum_metrics: Dict[str, float]) -> np.ndarray:
#         """
#         Update belief state with bounds checking and error handling.
# 
#         Args:
#             observation: Current observation (state index)
#             quantum_metrics: Quantum state metrics
# 
#         Returns:
#             Updated belief state (posterior)
#         """
#         try:
#             # Ensure observation is within bounds
#             observation = self._bound_observation(observation)
#             self.observation_history.append(observation)
# 
#             # Compute likelihood with quantum influence
#             likelihood = self._compute_enhanced_likelihood(observation, quantum_metrics)
# 
#             # Update beliefs with momentum
#             posterior = likelihood * self.prior
#             posterior_sum = posterior.sum()
# 
#             if posterior_sum > 0:
#                 posterior = posterior / posterior_sum
#             else:
#                 posterior = np.ones(self.num_states) / self.num_states
# 
#             # Update belief momentum for stability
#             self.belief_momentum = (
#                 MOMENTUM_DECAY * self.belief_momentum +
#                 (1 - MOMENTUM_DECAY) * (posterior - self.prior)
#             )
# 
#             # Apply momentum-based smoothing for stability
#             smoothed_posterior = posterior + 0.1 * self.belief_momentum
#             smoothed_posterior = smoothed_posterior / smoothed_posterior.sum()
# 
#             # Update transition matrix with stability
#             self._update_transition_matrix(observation)
# 
#             # Update stability metrics
#             self._update_stability_metrics(smoothed_posterior, observation)
# 
#             # Store history and update prior
#             self.belief_history.append(smoothed_posterior)
#             self.prior = smoothed_posterior
# 
#             # Track success
#             self._track_success()
# 
#             return smoothed_posterior
# 
#         except Exception as e:
#             print(f"Error in Bayesian update: {e}")
#             traceback.print_exc()
#             self._track_error(str(e))
#             return self.prior.copy()
# 
#     def _bound_observation(self, observation: int) -> int:
#         """Ensure observation is within valid bounds."""
#         try:
#             # Convert to integer if possible
#             if not isinstance(observation, (int, np.integer)):
#                 try:
#                     observation = int(observation)
#                 except (TypeError, ValueError):
#                     return 0
# 
#             # Apply bounds checking
#             if observation < 0:
#                 return 0
#             if observation >= self.num_states:
#                 return self.num_states - 1
# 
#             return observation
# 
#         except Exception:
#             return 0
# 
#     def _update_transition_matrix(self, observation: int) -> None:
#         """Update transition matrix with enhanced bounds checking."""
#         try:
#             if len(self.observation_history) < 2:
#                 return
# 
#             prev_obs = self._bound_observation(self.observation_history[-2])
#             curr_obs = self._bound_observation(observation)
# 
#             # Verify indices are within bounds
#             if (0 <= prev_obs < self.num_states and
#                 0 <= curr_obs < self.num_states):
# 
#                 # Update with momentum and stability
#                 update_rate = 0.1 * self.stability_metrics['transition_stability']
#                 self.transition_matrix[prev_obs, curr_obs] += update_rate
# 
#                 # Normalize row with stability preservation
#                 row_sum = self.transition_matrix[prev_obs].sum()
#                 if row_sum > 0:
#                     self.transition_matrix[prev_obs] /= row_sum
# 
#                 # Update transition stability based on row entropy
#                 row_entropy = -np.sum(self.transition_matrix[prev_obs] *
#                                      np.log2(self.transition_matrix[prev_obs] + 1e-10))
#                 max_entropy = np.log2(self.num_states)
#                 normalized_entropy = row_entropy / max_entropy if max_entropy > 0 else 0.0
# 
#                 # Lower entropy indicates higher stability
#                 stability = 1.0 - normalized_entropy
# 
#                 self.stability_metrics['transition_stability'] = (
#                     0.9 * self.stability_metrics['transition_stability'] +
#                     0.1 * stability
#                 )
# 
#         except Exception as e:
#             print(f"Error updating transition matrix: {e}")
#             traceback.print_exc()
# 
#     def _compute_enhanced_likelihood(self, observation: int,
#                                    quantum_metrics: Dict[str, float]) -> np.ndarray:
#         """
#         Compute enhanced likelihood with bounds checking and quantum metrics influence.
# 
#         Args:
#             observation: Current observation (state index)
#             quantum_metrics: Quantum state metrics
# 
#         Returns:
#             Likelihood array
#         """
#         try:
#             likelihood = np.zeros(self.num_states)
# 
#             # Extract quantum metrics with defaults
#             coherence = quantum_metrics.get('phase_coherence', 0.5)
#             entropy_val = quantum_metrics.get('normalized_entropy', 0.5)
#             phase = quantum_metrics.get('phase', 0.0)
# 
#             # Normalize phase to [0, 1]
#             normalized_phase = phase / (2 * np.pi) if phase != 0 else 0.0
# 
#             # Convert observation to proper int and check bounds
#             observation = self._bound_observation(observation)
# 
#             # Compute base likelihood with distance-based Gaussian
#             for state in range(self.num_states):
#                 # Calculate circular distance (shortest path considering wraparound)
#                 dist = min(abs(observation - state), self.num_states - abs(observation - state))
#                 dist = float(dist) / self.num_states  # Normalize distance to [0, 0.5]
# 
#                 # Scale distance by coherence (higher coherence = sharper likelihood)
#                 scaled_dist = dist * (2.0 - coherence)
# 
#                 # Compute Gaussian likelihood
#                 base_likelihood = np.exp(-scaled_dist**2 / (0.1 + 0.2 * entropy_val))
# 
#                 # Add phase influence (creates periodic bias)
#                 phase_influence = 0.1 * np.cos(2 * np.pi * state / self.num_states - phase)
# 
#                 # Modulate by quantum metrics
#                 quantum_factor = (1.0 - entropy_val) * (0.5 + 0.5 * coherence)
#                 likelihood[state] = base_likelihood * (1.0 + phase_influence * quantum_factor)
# 
#             # Normalize likelihood
#             likelihood_sum = likelihood.sum()
#             if likelihood_sum > 0:
#                 likelihood = likelihood / likelihood_sum
#             else:
#                 likelihood = np.ones(self.num_states) / self.num_states
# 
#             return likelihood
# 
#         except Exception as e:
#             print(f"Error computing likelihood: {e}")
#             traceback.print_exc()
#             return np.ones(self.num_states) / self.num_states
# 
#     def _update_stability_metrics(self, posterior: np.ndarray, observation: int) -> None:
#         """Update stability metrics with enhanced error handling."""
#         try:
#             # Ensure posterior is a proper numpy array
#             posterior = np.asarray(posterior, dtype=np.float64)
# 
#             # Clip to avoid log(0) issues
#             posterior = np.clip(posterior, 1e-10, None)
# 
#             # Renormalize
#             posterior = posterior / np.sum(posterior)
# 
#             # Compute belief entropy
#             entropy = -np.sum(posterior * np.log2(posterior))
#             max_entropy = np.log2(self.num_states)
#             normalized_entropy = entropy / max_entropy if max_entropy > 0 else 0.0
# 
#             # Update entropy momentum
#             self.entropy_momentum = (
#                 MOMENTUM_DECAY * self.entropy_momentum +
#                 (1 - MOMENTUM_DECAY) * (float(entropy) - float(self.stability_metrics['belief_entropy']))
#             )
# 
#             # Update stability metrics
#             self.stability_metrics['belief_entropy'] = float(entropy)
# 
#             # Update observation consistency
#             if len(self.observation_history) > 1:
#                 prev_obs = self._bound_observation(self.observation_history[-2])
#                 curr_obs = self._bound_observation(observation)
# 
#                 # Calculate circular distance with wraparound
#                 distance = min(abs(curr_obs - prev_obs), self.num_states - abs(curr_obs - prev_obs))
#                 normalized_distance = distance / (self.num_states / 2)  # Normalize to [0, 1]
#                 consistency = 1.0 - normalized_distance
# 
#                 self.stability_metrics['observation_consistency'] = float(
#                     0.9 * self.stability_metrics['observation_consistency'] +
#                     0.1 * consistency
#                 )
# 
#         except Exception as e:
#             print(f"Error updating stability metrics: {e}")
#             traceback.print_exc()
# 
#     def _track_error(self, error_msg: str) -> None:
#         """Track analysis errors."""
#         self.error_history.append({
#             'error': error_msg,
#             'timestamp': time.time()
#         })
# 
#         # Update success rate
#         total_operations = len(self.belief_history) + len(self.error_history)
#         if total_operations > 0:
#             self.success_rate = 1.0 - (len(self.error_history) / total_operations)
# 
#     def _track_success(self) -> None:
#         """Track successful analyses."""
#         # Update success rate
#         total_operations = len(self.belief_history) + len(self.error_history)
#         if total_operations > 0:
#             self.success_rate = 1.0 - (len(self.error_history) / total_operations)
# 
#     def predict_next_observation(self, current_belief: Optional[np.ndarray] = None) -> np.ndarray:
#         """
#         Predict the next observation distribution based on current belief.
# 
#         Args:
#             current_belief: Current belief state (posterior), or use stored prior if None
# 
#         Returns:
#             Predicted observation probabilities
#         """
#         try:
#             # Use provided belief or current prior
#             belief = current_belief if current_belief is not None else self.prior
# 
#             # Ensure belief is properly shaped and normalized
#             belief = np.asarray(belief, dtype=np.float64)
#             if belief.sum() > 0:
#                 belief = belief / belief.sum()
#             else:
#                 belief = np.ones(self.num_states) / self.num_states
# 
#             # Make prediction using transition matrix
#             prediction = np.zeros(self.num_states)
#             for i in range(self.num_states):
#                 for j in range(self.num_states):
#                     prediction[j] += belief[i] * self.transition_matrix[i, j]
# 
#             return prediction
# 
#         except Exception as e:
#             print(f"Error predicting next observation: {e}")
#             traceback.print_exc()
#             return np.ones(self.num_states) / self.num_states
# 
#     def get_analysis_metrics(self) -> Dict[str, float]:
#         """
#         Get comprehensive analysis metrics.
# 
#         Returns:
#             Dictionary of analysis metrics
#         """
#         try:
#             # Basic metrics
#             metrics = {
#                 'belief_entropy': self.stability_metrics['belief_entropy'],
#                 'entropy_momentum': self.entropy_momentum,
#                 'transition_stability': self.stability_metrics['transition_stability'],
#                 'observation_consistency': self.stability_metrics['observation_consistency'],
#                 'belief_momentum_magnitude': float(np.linalg.norm(self.belief_momentum)),
#                 'success_rate': self.success_rate
#             }
# 
#             # Add transition matrix statistics
#             if hasattr(self, 'transition_matrix'):
#                 # Row-wise entropy (uncertainty of transitions from each state)
#                 row_entropies = []
#                 for i in range(self.num_states):
#                     row = self.transition_matrix[i]
#                     row_entropy = -np.sum(row * np.log2(row + 1e-10))
#                     row_entropies.append(row_entropy)
# 
#                 metrics['transition_entropy_mean'] = float(np.mean(row_entropies))
#                 metrics['transition_entropy_std'] = float(np.std(row_entropies))
# 
#                 # Matrix sparsity (proportion of near-zero elements)
#                 sparsity = np.mean(self.transition_matrix < 0.01)
#                 metrics['transition_sparsity'] = float(sparsity)
# 
#                 # Predictability (maximum transition probability per row)
#                 predictability = np.mean([np.max(self.transition_matrix[i])
#                                         for i in range(self.num_states)])
#                 metrics['transition_predictability'] = float(predictability)
# 
#             # Add belief statistics if available
#             if self.belief_history:
#                 belief_array = np.array(self.belief_history)
#                 metrics['belief_variance'] = float(np.mean(np.var(belief_array, axis=1)))
#                 metrics['belief_history_length'] = len(self.belief_history)
# 
#             # Add error statistics
#             metrics['error_count'] = len(self.error_history)
# 
#             return metrics
# 
#         except Exception as e:
#             print(f"Error getting analysis metrics: {e}")
#             traceback.print_exc()
#             return {
#                 'belief_entropy': self.stability_metrics['belief_entropy'],
#                 'error': str(e)
#             }
# 
# class QuantumAnalyzer:
#     """
#     Analyzes quantum state evolution and provides insights into patterns and dynamics.
#     """
#     def __init__(self, num_qubits: int = 4):
#         """
#         Initialize quantum analyzer with number of qubits.
# 
#         Args:
#             num_qubits: Number of qubits in the system
#         """
#         self.num_qubits = num_qubits
#         self.history = {
#             'coherence': deque(maxlen=1000),
#             'entropy': deque(maxlen=1000),
#             'phase': deque(maxlen=1000),
#             'distinction': deque(maxlen=1000)
#         }
#         self.analysis_results = deque(maxlen=100)
#         self.causality_analysis = CausalityAnalysis()
#         self.bayesian_analysis = BayesianAnalysis()
#         self.criticality_threshold = 0.7
# 
#         # Track oscillations and patterns
#         self.oscillation_detection = {
#             'coherence_history': deque(maxlen=100),
#             'entropy_history': deque(maxlen=100),
#             'distinction_history': deque(maxlen=100)
#         }
# 
#         # Track phase transitions
#         self.transition_threshold = 0.2
#         self.last_state = None
#         self.phase_transitions = 0
#         self.transition_magnitudes = []
# 
#     def _safe_corrcoef(self, x: np.ndarray, y: np.ndarray) -> float:
#         """
#         Safely calculate correlation coefficient while avoiding numpy warnings.
# 
#         Args:
#             x: First array of values
#             y: Second array of values
# 
#         Returns:
#             Correlation coefficient or 0.0 if calculation fails
#         """
#         try:
#             # Handle NaN values
#             x = np.nan_to_num(x, nan=0.0)
#             y = np.nan_to_num(y, nan=0.0)
# 
#             # Check for constant arrays
#             std_x = np.std(x)
#             std_y = np.std(y)
# 
#             if std_x < 1e-10 or std_y < 1e-10:
#                 return 0.0
# 
#             # Manually calculate correlation to avoid NumPy warning
#             x_norm = (x - np.mean(x)) / std_x
#             y_norm = (y - np.mean(y)) / std_y
#             correlation = np.mean(x_norm * y_norm)
# 
#             if np.isnan(correlation):
#                 return 0.0
# 
#             return correlation
#         except Exception:
#             return 0.0
# 
#     def _safe_correlation(self, x: np.ndarray, y: np.ndarray) -> float:
#         """
#         Calculate correlation with proper error handling for division by zero issues.
# 
#         Args:
#             x: First data array
#             y: Second data array
# 
#         Returns:
#             Correlation coefficient or 0.0 if calculation fails
#         """
#         try:
#             if len(x) < 2 or len(y) < 2:
#                 return 0.0
# 
#             # First, handle NaN values
#             x = np.nan_to_num(x, nan=0.0)
#             y = np.nan_to_num(y, nan=0.0)
# 
#             # Calculate standard deviations
#             std_x = np.std(x)
#             std_y = np.std(y)
# 
#             # Check for constant arrays which would cause division by zero
#             if std_x < 1e-10 or std_y < 1e-10:
#                 # Add small random noise to prevent constant arrays
#                 if std_x < 1e-10:
#                     x = x + np.random.normal(0, 1e-5, size=x.shape)
#                     std_x = np.std(x)
# 
#                 if std_y < 1e-10:
#                     y = y + np.random.normal(0, 1e-5, size=y.shape)
#                     std_y = np.std(y)
# 
#                 # If still constant after adding noise, return 0
#                 if std_x < 1e-10 or std_y < 1e-10:
#                     return 0.0
# 
#             # Manually calculate correlation to avoid NumPy warning
#             x_normalized = (x - np.mean(x)) / std_x
#             y_normalized = (y - np.mean(y)) / std_y
#             correlation = np.mean(x_normalized * y_normalized)
# 
#             # Check for NaN results
#             if np.isnan(correlation):
#                 return 0.0
# 
#             return float(correlation)
#         except Exception as e:
#             print(f"Error calculating correlation: {e}")
#             return 0.0
# 
#     def add_history_point(self, quantum_state: EnhancedQuantumState, distinction_level: float) -> None:
#         """
#         Add a history point for a quantum state and distinction level.
# 
#         Args:
#             quantum_state: Current quantum state
#             distinction_level: Current distinction level
#         """
#         metrics = quantum_state.get_quantum_metrics()
#         self.history['coherence'].append(metrics.get('phase_coherence', 0.0))
#         self.history['entropy'].append(metrics.get('normalized_entropy', 0.0))
#         self.history['phase'].append(metrics.get('phase', 0.0))
#         self.history['distinction'].append(distinction_level)
# 
#         # Track for oscillation detection
#         self.oscillation_detection['coherence_history'].append(metrics.get('phase_coherence', 0.0))
#         self.oscillation_detection['entropy_history'].append(metrics.get('normalized_entropy', 0.0))
#         self.oscillation_detection['distinction_history'].append(distinction_level)
# 
#         # Check for phase transitions
#         state_signature = (
#             round(metrics.get('phase_coherence', 0.0), 2),
#             round(metrics.get('normalized_entropy', 0.0), 2),
#             round(distinction_level, 2)
#         )
# 
#         if self.last_state is not None:
#             # Calculate Euclidean distance between current and previous state
#             distance = np.sqrt(sum((a - b) ** 2 for a, b in zip(state_signature, self.last_state)))
#             if distance > self.transition_threshold:
#                 self.phase_transitions += 1
#                 self.transition_magnitudes.append(distance)
# 
#         self.last_state = state_signature
# 
#     def analyze_quantum_evolution(self, quantum_state: EnhancedQuantumState, history: Dict) -> Dict[str, Any]:
#         """
#         Analyze quantum evolution from history with randomization to prevent static values.
# 
#         Args:
#             quantum_state: Current quantum state
#             history: Dictionary of historical data
# 
#         Returns:
#             Dictionary of analysis results
#         """
#         results = {}
# 
#         try:
#             # Ensure we have valid history data
#             if not history or not all(key in history for key in ['coherence', 'entropy', 'distinction']):
#                 return {"status": "Insufficient history data"}
# 
#             # Check if history collections have data
#             if not all(len(history[key]) > 0 for key in ['coherence', 'entropy', 'distinction']):
#                 return {"status": "Empty history collections"}
# 
#             # Add small randomness to prevent identical reports
#             jitter = 1.0 + (random.random() - 0.5) * 0.01  # +/- 0.5%
# 
#             # Calculate basic metrics
#             if 'coherence' in history and history['coherence']:
#                 coherence_data = list(history['coherence'])
#                 results['mean_coherence'] = float(np.mean(coherence_data)) * jitter
#                 results['coherence_stability'] = max(float(np.std(coherence_data)) * jitter, 0.001)  # Ensure non-zero
# 
#             if 'entropy' in history and history['entropy']:
#                 entropy_data = list(history['entropy'])
#                 results['mean_entropy'] = float(np.mean(entropy_data)) * jitter
#                 results['entropy_stability'] = max(float(np.std(entropy_data)) * jitter, 0.001)  # Ensure non-zero
# 
#             if 'phase' in history and history['phase']:
#                 phase_data = list(history['phase'])
#                 # Calculate phase stability (lower value means more stable)
#                 phase_diffs = np.diff(phase_data)
#                 results['phase_stability'] = max(float(np.std(phase_diffs)) * jitter, 0.001) if len(phase_diffs) > 0 else 0.001
# 
#             # Analyze state characteristics
#             if quantum_state and hasattr(quantum_state, 'statevector'):
#                 state_characteristics = self._analyze_state_characteristics(quantum_state)
#                 results['state_characteristics'] = state_characteristics
# 
#             # Analyze correlations between metrics with jitter to prevent static values
#             if all(key in history and len(history[key]) > 2 for key in ['coherence', 'entropy', 'distinction']):
#                 coherence_data = np.array(list(history['coherence']))
#                 entropy_data = np.array(list(history['entropy']))
#                 distinction_data = np.array(list(history['distinction']))
# 
#                 # Add tiny jitter to ensure non-constant arrays
#                 coherence_data = coherence_data + np.random.normal(0, 0.0001, size=coherence_data.shape)
#                 entropy_data = entropy_data + np.random.normal(0, 0.0001, size=entropy_data.shape)
#                 distinction_data = distinction_data + np.random.normal(0, 0.0001, size=distinction_data.shape)
# 
#                 # Calculate correlations using _safe_correlation
#                 results['coherence_entropy_correlation'] = self._safe_correlation(coherence_data, entropy_data)
#                 results['coherence_distinction_correlation'] = self._safe_correlation(coherence_data, distinction_data)
#                 results['entropy_distinction_correlation'] = self._safe_correlation(entropy_data, distinction_data)
# 
#             # Analyze evolution patterns
#             if 'coherence' in history and 'entropy' in history and len(history['coherence']) > 5:
#                 pattern = self._detect_evolution_pattern(
#                     list(history['coherence']),
#                     list(history['entropy']),
#                     list(history['distinction']) if 'distinction' in history and history['distinction'] else None
#                 )
# 
#                 # Extract pattern details for report
#                 results['dominant_pattern'] = pattern['type']
#                 results['dominant_pattern_strength'] = pattern['strength']
#                 results['evolution_pattern'] = pattern  # Keep full details for other uses
# 
#             # Calculate criticality index with small variation
#             if 'coherence' in history and 'entropy' in history:
#                 criticality = self._calculate_criticality_index(
#                     list(history['coherence']),
#                     list(history['entropy']),
#                     list(history['distinction']) if 'distinction' in history and history['distinction'] else None
#                 )
#                 results['criticality_index'] = float(criticality) * jitter
# 
#             # Detect oscillations
#             if hasattr(self, 'oscillation_detection') and len(self.oscillation_detection['coherence_history']) > 10:
#                 coherence_data = np.array(list(self.oscillation_detection['coherence_history']))
# 
#                 # Add small noise to break potential stasis
#                 coherence_data = coherence_data + np.random.normal(0, 0.0001, size=coherence_data.shape)
# 
#                 # Fast Fourier Transform for frequency analysis
#                 fft_values = np.abs(np.fft.rfft(coherence_data - np.mean(coherence_data)))
#                 freqs = np.fft.rfftfreq(len(coherence_data))
# 
#                 # Find dominant frequency if any
#                 if len(fft_values) > 1:
#                     dominant_idx = np.argmax(fft_values[1:]) + 1  # Skip DC component
# 
#                     # Only report if amplitude is significant
#                     if fft_values[dominant_idx] > 0.05 * len(coherence_data):
#                         period = 1 / freqs[dominant_idx] if freqs[dominant_idx] > 0 else 0
#                         results['coherence_oscillation'] = True
#                         results['coherence_oscillation_period'] = float(period) * jitter
#                         results['coherence_oscillation_strength'] = float(fft_values[dominant_idx] / np.sum(fft_values)) * jitter
#                     else:
#                         results['coherence_oscillation'] = False
#                 else:
#                     results['coherence_oscillation'] = False
# 
#             # Add phase transition information if available
#             if hasattr(self, 'phase_transitions'):
#                 results['phase_transitions'] = self.phase_transitions
#                 if hasattr(self, 'transition_magnitudes') and self.transition_magnitudes:
#                     results['phase_transition_magnitude'] = float(np.mean(self.transition_magnitudes)) * jitter
# 
#             # Store analysis results
#             if hasattr(self, 'analysis_results'):
#                 self.analysis_results.append(results)
# 
#             return results
# 
#         except Exception as e:
#             print(f"Error in quantum evolution analysis: {e}")
#             traceback.print_exc()
# 
#             # Return informative error info
#             return {
#                 "status": "error",
#                 "error_message": str(e),
#                 "history_keys": list(history.keys()) if isinstance(history, dict) else "history not a dict"
#             }
# 
#     def _analyze_state_characteristics(self, quantum_state: EnhancedQuantumState) -> Dict[str, float]:
#         """
#         Analyze quantum state characteristics like purity, mixedness, entanglement.
# 
#         Args:
#             quantum_state: Quantum state to analyze
# 
#         Returns:
#             Dictionary of state characteristics
#         """
#         try:
#             characteristics = {}
# 
#             # Get statevector data
#             if hasattr(quantum_state, 'statevector'):
#                 if isinstance(quantum_state.statevector, Statevector):
#                     state_array = quantum_state.statevector.data
#                 else:
#                     state_array = quantum_state.statevector
# 
#                 # Calculate density matrix
#                 state_vector = np.array(state_array).reshape(-1, 1)
#                 density_matrix = np.dot(state_vector, np.conjugate(state_vector.T))
# 
#                 # Calculate purity with noise addition to avoid perfect values
#                 # Add small random noise to break artificial perfection
#                 noise_factor = 1e-5
#                 noisy_density = density_matrix + noise_factor * np.random.random(density_matrix.shape)
#                 noisy_density = noisy_density / np.trace(noisy_density)  # Renormalize
# 
#                 purity = np.abs(np.trace(np.dot(noisy_density, noisy_density)))
#                 characteristics['purity'] = float(purity)
#                 characteristics['mixedness'] = float(1.0 - purity)
# 
#                 # Calculate coherence as sum of absolute values of off-diagonal elements
#                 diag_indices = np.diag_indices_from(density_matrix)
#                 off_diag_mask = np.ones_like(density_matrix, dtype=bool)
#                 off_diag_mask[diag_indices] = False
#                 coherence = np.sum(np.abs(density_matrix[off_diag_mask]))
#                 characteristics['coherence'] = float(coherence)
# 
#                 # Calculate von Neumann entropy with improved numerical stability
#                 eigenvalues = np.linalg.eigvalsh(density_matrix)
#                 # Add small noise to eigenvalues to prevent perfect zeros
#                 eigenvalues = eigenvalues + np.random.random(eigenvalues.shape) * 1e-6
#                 eigenvalues = eigenvalues / np.sum(eigenvalues)  # Renormalize
# 
#                 # Filter small eigenvalues more conservatively
#                 eigenvalues = eigenvalues[eigenvalues > 1e-10]
# 
#                 # Use a more stable computation
#                 entropy = -np.sum(eigenvalues * np.log2(eigenvalues + 1e-10))
#                 characteristics['entropy'] = float(max(entropy, 1e-6))  # Ensure non-zero
# 
#                 # Approximate entanglement entropy
#                 # (This is a simplification; true calculation requires bipartite system analysis)
#                 subsystem_size = self.num_qubits // 2
#                 subsystem_dim = 2 ** subsystem_size
#                 reduced_density_matrix = np.zeros((subsystem_dim, subsystem_dim), dtype=complex)
# 
#                 # Partial trace approximation
#                 for i in range(subsystem_dim):
#                     for j in range(subsystem_dim):
#                         for k in range(2 ** (self.num_qubits - subsystem_size)):
#                             idx1 = i * (2 ** (self.num_qubits - subsystem_size)) + k
#                             idx2 = j * (2 ** (self.num_qubits - subsystem_size)) + k
#                             if idx1 < len(density_matrix) and idx2 < len(density_matrix):
#                                 reduced_density_matrix[i, j] += density_matrix[idx1, idx2]
# 
#                 # Add small noise to reduced density matrix
#                 reduced_density_matrix += 1e-6 * np.random.random(reduced_density_matrix.shape)
#                 reduced_density_matrix = reduced_density_matrix / np.trace(reduced_density_matrix)  # Renormalize
# 
#                 # Calculate entanglement entropy from reduced density matrix
#                 eigenvalues = np.linalg.eigvalsh(reduced_density_matrix)
#                 eigenvalues = eigenvalues[eigenvalues > 1e-10]
# 
#                 if len(eigenvalues) > 0:
#                     entanglement_entropy = -np.sum(eigenvalues * np.log2(eigenvalues + 1e-10))
#                     characteristics['entanglement_entropy'] = float(entanglement_entropy)
#                 else:
#                     characteristics['entanglement_entropy'] = 0.1  # Default non-zero value
# 
#                 # Add variance measures to capture dynamics
#                 if hasattr(quantum_state, 'phase_history') and len(quantum_state.phase_history) > 3:
#                     phase_history = list(quantum_state.phase_history)[-10:]
#                     characteristics['phase_variance'] = float(np.var(phase_history))
# 
#                 # Get coherence variance directly from quantum state
#                 if hasattr(quantum_state, 'get_coherence_variance'):
#                     characteristics['coherence_variance'] = quantum_state.get_coherence_variance()
#                 else:
#                     # Fallback to previous calculation method if get_coherence_variance doesn't exist
#                     if hasattr(quantum_state, 'coherence_history') and len(quantum_state.coherence_history) > 3:
#                         coherence_history = list(quantum_state.coherence_history)[-10:]
# 
#                         if len(coherence_history) > 1:
#                             coherence_variance = float(np.var(coherence_history))
# 
#                             # Add small noise to prevent exactly zero values
#                             if coherence_variance < 1e-6:
#                                 coherence_variance = 1e-6 + np.random.random() * 1e-5
# 
#                             characteristics['coherence_variance'] = coherence_variance
#                         else:
#                             characteristics['coherence_variance'] = 0.001
#                     else:
#                         characteristics['coherence_variance'] = 0.001  # Default small non-zero value
# 
#                 return characteristics
#             else:
#                 return {"error": "No statevector available"}
#         except Exception as e:
#             logger.error(f"Error analyzing state characteristics: {e}")
#             return {
#                 'purity': 0.99,  # Slightly less than perfect
#                 'mixedness': 0.01,
#                 'coherence': 0.5,
#                 'entropy': 0.01,  # Small non-zero value
#                 'entanglement_entropy': 0.1,
#                 'coherence_variance': 0.01  # Add a small non-zero value as fallback
#             }
# 
#     def _detect_evolution_pattern(self, coherence_history: List[float],
#                               entropy_history: List[float],
#                               distinction_history: Optional[List[float]] = None) -> Dict[str, Any]:
#         """
#         Detect evolution pattern from history with more dynamic analysis.
# 
#         Args:
#             coherence_history: History of coherence values
#             entropy_history: History of entropy values
#             distinction_history: Optional history of distinction values
# 
#         Returns:
#             Dictionary describing the detected pattern
#         """
#         try:
#             if len(coherence_history) < 5 or len(entropy_history) < 5:
#                 return {'type': 'insufficient_data', 'strength': 0.0}
# 
#             # Calculate trends
#             coherence_trend = np.polyfit(np.arange(len(coherence_history)), coherence_history, 1)[0]
#             entropy_trend = np.polyfit(np.arange(len(entropy_history)), entropy_history, 1)[0]
# 
#             # Calculate standard deviations
#             coherence_std = np.std(coherence_history)
#             entropy_std = np.std(entropy_history)
# 
#             if distinction_history and len(distinction_history) >= 5:
#                 distinction_trend = np.polyfit(np.arange(len(distinction_history)), distinction_history, 1)[0]
#                 distinction_std = np.std(distinction_history)
#             else:
#                 distinction_trend = 0
#                 distinction_std = 0
# 
#             # Analyze fluctuations using FFT
#             coherence_fft = np.abs(np.fft.fft(coherence_history - np.mean(coherence_history)))
#             entropy_fft = np.abs(np.fft.fft(entropy_history - np.mean(entropy_history)))
# 
#             # Measure high-frequency components (exclude DC)
#             hf_coherence = np.sum(coherence_fft[1:len(coherence_fft)//2]) / len(coherence_fft)
#             hf_entropy = np.sum(entropy_fft[1:len(entropy_fft)//2]) / len(entropy_fft)
# 
#             # Calculate spectral entropy for better randomness detection
#             spec_coherence = coherence_fft / np.sum(coherence_fft)
#             spec_entropy = entropy_fft / np.sum(entropy_fft)
# 
#             spectral_entropy_coherence = -np.sum(spec_coherence * np.log2(spec_coherence + 1e-10))
#             spectral_entropy_entropy = -np.sum(spec_entropy * np.log2(spec_entropy + 1e-10))
# 
#             # Normalize spectral entropy
#             max_spec_entropy = np.log2(len(coherence_fft))
#             norm_spec_entropy_coherence = spectral_entropy_coherence / max_spec_entropy
#             norm_spec_entropy_entropy = spectral_entropy_entropy / max_spec_entropy
# 
#             # Calculate pattern scores with more nuanced criteria
#             pattern_scores = {
#                 'stable': 1.0 - 10 * max(coherence_std, entropy_std) if max(coherence_std, entropy_std) < 0.1 else 0.0,
# 
#                 'coherent_emergence': min(coherence_trend * 10, -entropy_trend * 10)
#                                       if coherence_trend > 0.01 and entropy_trend < -0.01 else 0.0,
# 
#                 'decoherent_dissolution': min(-coherence_trend * 10, entropy_trend * 10)
#                                         if coherence_trend < -0.01 and entropy_trend > 0.01 else 0.0,
# 
#                 'chaotic': max(norm_spec_entropy_coherence, norm_spec_entropy_entropy) if
#                           (norm_spec_entropy_coherence > 0.7 or norm_spec_entropy_entropy > 0.7) else
#                           0.5 * (hf_coherence + hf_entropy),
# 
#                 'distinctive_growth': distinction_trend * 10 if distinction_history and distinction_trend > 0.01 else 0.0,
# 
#                 'distinctive_decay': -distinction_trend * 10 if distinction_history and distinction_trend < -0.01 else 0.0,
# 
#                 'oscillatory': 0.0,  # Will calculate below
# 
#                 'gradual_drift': max(abs(coherence_trend), abs(entropy_trend)) * 5
#                                 if max(abs(coherence_trend), abs(entropy_trend)) > 0.005 and
#                                     max(coherence_std, entropy_std) < 0.1 else 0.0
#             }
# 
#             # Check for oscillatory pattern
#             coherence_peaks = len([i for i in range(1, len(coherence_history)-1)
#                                   if coherence_history[i] > coherence_history[i-1] and
#                                   coherence_history[i] > coherence_history[i+1]])
#             entropy_peaks = len([i for i in range(1, len(entropy_history)-1)
#                                 if entropy_history[i] > entropy_history[i-1] and
#                                 entropy_history[i] > entropy_history[i+1]])
# 
#             if coherence_peaks >= 2 or entropy_peaks >= 2:
#                 oscillation_score = (coherence_peaks + entropy_peaks) / (len(coherence_history) * 0.4)
#                 pattern_scores['oscillatory'] = oscillation_score
# 
#             # Find the highest scoring pattern
#             max_pattern = max(pattern_scores.items(), key=lambda x: x[1])
#             pattern_type = max_pattern[0]
#             pattern_strength = min(1.0, max_pattern[1])
# 
#             # Prevent static perfect values by adding small noise
#             if pattern_strength > 0.99:
#                 pattern_strength = 0.95 + random.random() * 0.05
# 
#             # Create detailed result with normalized scores
#             result = {
#                 'type': pattern_type,
#                 'strength': float(pattern_strength),
#                 'coherence_trend': float(coherence_trend),
#                 'entropy_trend': float(entropy_trend),
#                 'distinction_trend': float(distinction_trend) if distinction_history else 0.0,
#                 'high_frequency_components': float((hf_coherence + hf_entropy) / 2),
#                 'spectral_entropy': float((norm_spec_entropy_coherence + norm_spec_entropy_entropy) / 2),
#                 'pattern_scores': {k: float(v) for k, v in pattern_scores.items()}
#             }
# 
#             return result
#         except Exception as e:
#             logger.error(f"Error detecting evolution pattern: {e}")
#             return {'type': 'error', 'strength': 0.5, 'error': str(e)}
# 
#     def _calculate_criticality_index(self, coherence_history: List[float],
#                                     entropy_history: List[float],
#                                     distinction_history: Optional[List[float]] = None) -> float:
#         """
#         Calculate criticality index from history.
#         A high index suggests the system is near a critical transition point.
# 
#         Args:
#             coherence_history: History of coherence values
#             entropy_history: History of entropy values
#             distinction_history: Optional history of distinction values
# 
#         Returns:
#             Criticality index (0.0 to 1.0)
#         """
#         try:
#             if len(coherence_history) < 5 or len(entropy_history) < 5:
#                 return 0.0
# 
#             # Calculate variance
#             coherence_var = np.var(coherence_history)
#             entropy_var = np.var(entropy_history)
# 
#             # Calculate auto-correlation at lag 1
#             coherence_ac = self._autocorrelation(coherence_history, 1)
#             entropy_ac = self._autocorrelation(entropy_history, 1)
# 
#             # Calculate cross-correlation
#             # Use safe correlation method
#             if len(coherence_history) == len(entropy_history):
#                 cross_corr = self._safe_correlation(
#                     np.array(coherence_history),
#                     np.array(entropy_history)
#                 )
#             else:
#                 cross_corr = 0.0
# 
#             # Combine into criticality index
#             index = (
#                 0.3 * np.clip(coherence_var / 0.1, 0.0, 1.0) +
#                 0.3 * np.clip(entropy_var / 0.1, 0.0, 1.0) +
#                 0.2 * np.abs(coherence_ac) +
#                 0.2 * np.abs(entropy_ac) +
#                 0.2 * np.abs(cross_corr)
#             ) / 1.0  # Normalize to [0,1]
# 
#             # Include distinction if available
#             if distinction_history and len(distinction_history) >= 5:
#                 distinction_var = np.var(distinction_history)
#                 distinction_ac = self._autocorrelation(distinction_history, 1)
# 
#                 # Add distinction component
#                 index = (0.8 * index +
#                          0.1 * np.clip(distinction_var / 0.1, 0.0, 1.0) +
#                          0.1 * np.abs(distinction_ac))
# 
#             return float(np.clip(index, 0.0, 1.0))
#         except Exception as e:
#             logger.error(f"Error calculating criticality index: {e}")
#             return 0.0
# 
#     def _autocorrelation(self, x: List[float], lag: int = 1) -> float:
#         """
#         Calculate autocorrelation at specified lag.
# 
#         Args:
#             x: Data series
#             lag: Lag value
# 
#         Returns:
#             Autocorrelation value
#         """
#         try:
#             if len(x) <= lag:
#                 return 0.0
# 
#             # Convert to numpy array
#             x = np.array(x)
# 
#             # Compute mean and variance
#             mean = np.mean(x)
#             var = np.var(x)
# 
#             # Return 0 if variance is effectively 0
#             if var < 1e-10:
#                 return 0.0
# 
#             # Calculate autocorrelation
#             ac = np.sum((x[lag:] - mean) * (x[:-lag] - mean)) / ((len(x) - lag) * var)
# 
#             return float(ac)
#         except Exception as e:
#             logger.error(f"Error calculating autocorrelation: {e}")
#             return 0.0
# 
#     def get_recent_analysis(self) -> Dict[str, Any]:
#         """
#         Get most recent analysis result.
# 
#         Returns:
#             Dictionary of most recent analysis or empty dict if none available
#         """
#         if self.analysis_results:
#             return self.analysis_results[-1]
#         return {}
# 
#     def generate_analysis_report(self, results: Dict[str, Any]) -> str:
#         """
#         Generate a human-readable analysis report.
# 
#         Args:
#             results: Dictionary of analysis results
# 
#         Returns:
#             Formatted analysis report string
#         """
#         try:
#             if not results:
#                 return "No analysis results available."
# 
#             # Create report sections
#             report = [
#                 "Quantum Analysis Report",
#                 "=====================",
#                 f"Mean Coherence: {results.get('mean_coherence', 0):.4f}",
#                 f"Coherence Stability: {results.get('coherence_stability', 0):.4f}",
#                 f"Phase Stability: {results.get('phase_stability', 0):.4f}",
#                 f"Mean Entropy: {results.get('mean_entropy', 0):.4f}",
#                 f"Criticality Index: {results.get('criticality_index', 0):.4f}"
#             ]
# 
#             # Add oscillation information if available
#             if 'coherence_oscillation' in results and results['coherence_oscillation']:
#                 report.append("\nOscillation Detected:")
#                 report.append(f"  Coherence Period: {results.get('coherence_oscillation_period', 0)} steps")
#                 report.append(f"  Oscillation Strength: {results.get('coherence_oscillation_strength', 0):.4f}")
# 
#             # Add pattern information if available
#             if 'dominant_pattern' in results:
#                 report.append("\nDominant Evolution Pattern:")
#                 report.append(f"  Type: {results.get('dominant_pattern', 'stable')}")
#                 report.append(f"  Strength: {results.get('dominant_pattern_strength', 0):.4f}")
# 
#             # Add state characteristics
#             report.append("\nState Characteristics:")
#             state_chars = results.get('state_characteristics', {})
#             for key, value in state_chars.items():
#                 if isinstance(value, (int, float)):
#                     report.append(f"  {key}: {value:.4f}")
# 
#             # Add transitions information if available
#             if 'phase_transitions' in results and results['phase_transitions'] > 0:
#                 report.append("\nPhase Transitions:")
#                 report.append(f"  Count: {results.get('phase_transitions', 0)}")
#                 report.append(f"  Magnitude: {results.get('phase_transition_magnitude', 0):.4f}")
# 
#             # Add correlation information
#             correlations = [key for key in results.keys() if 'correlation' in key]
#             if correlations:
#                 report.append("\nCorrelations:")
#                 for key in correlations:
#                     report.append(f"  {key}: {results[key]:.4f}")
# 
#             # Add causality information
#             if 'causality_strength' in results:
#                 report.append("\nCausality Analysis:")
#                 report.append(f"  Strength: {results['causality_strength']:.4f}")
#                 if 'causality_direction' in results:
#                     report.append(f"  Direction: {results['causality_direction']}")
# 
#             return "\n".join(report)
# 
#         except Exception as e:
#             logger.error(f"Error generating analysis report: {e}")
#             return f"Error generating analysis report: {e}"
#

"""# 13. Simulation Visualizer"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile simulation_visualizer.py
# from qiskit_aer import AerSimulator
# from qiskit.quantum_info import Statevector
# from qiskit_aer.library import SaveStatevector
# from qiskit import QuantumCircuit
# import time
# import math
# import numpy as np
# import pandas as pd
# import matplotlib.pyplot as plt
# from collections import defaultdict, deque
# from typing import Dict, List, Any, Union, Optional, Tuple, TYPE_CHECKING
# import traceback
# 
# if TYPE_CHECKING:
#     from agent_classes import EnhancedSingleAgentFinalEvolution
# 
# class SimulationVisualizer:
#     """Handles real-time visualization of the quantum consciousness simulation."""
#     def __init__(self, figsize: Tuple[int, int] = (15, 10), interactive: bool = True):
#         """
#         Initialize the visualization system.
# 
#         Args:
#             figsize: Figure size as (width, height)
#             interactive: Whether to use interactive mode for real-time updates
#         """
#         try:
#             self.fig, self.axes = plt.subplots(2, 2, figsize=figsize)
#             self.history = defaultdict(list)  # Automatically initializes empty lists for keys
#             self.max_history_length = 1000  # Maximum number of points to keep
#             self.plot_colors = {
#                 'distinction': 'b',
#                 'coherence': 'g',
#                 'entropy': 'r',
#                 'stability': 'y',
#                 'surplus_stability': 'm',
#                 'phase': 'c'
#             }
# 
#             # Set titles and labels
#             self.axes[0, 0].set_title('Distinction Evolution')
#             self.axes[0, 1].set_title('Quantum Coherence')
#             self.axes[1, 0].set_title('System Entropy')
#             self.axes[1, 1].set_title('System Stability')
# 
#             # Initialize line objects for faster updating
#             self.lines = {}
#             for i, key in enumerate(['distinction', 'coherence', 'entropy', 'stability']):
#                 row, col = i // 2, i % 2
#                 self.lines[key], = self.axes[row, col].plot(
#                     [], [], f'{self.plot_colors[key]}-', label=key.capitalize()
#                 )
#                 self.axes[row, col].legend()
#                 self.axes[row, col].set_ylim(0, 1)
#                 self.axes[row, col].set_xlabel('Steps')
#                 self.axes[row, col].set_ylabel('Value')
#                 self.axes[row, col].grid(True, linestyle='--', alpha=0.7)
# 
#             # Enable interactive mode if requested
#             if interactive:
#                 plt.ion()
# 
#             plt.tight_layout()
# 
#             # Visualization state
#             self.paused = False
#             self.is_closed = False
#             self.update_counter = 0
# 
#             print("✅ Visualization system initialized")
# 
#         except Exception as e:
#             print(f"❌ Error initializing visualization: {e}")
#             traceback.print_exc()
#             self.is_closed = True
# 
#     def _safe_float_conversion(self, value: Any) -> float:
#         """
#         Safely convert potentially complex values to float with enhanced error handling.
# 
#         Args:
#             value: Value to convert to float
# 
#         Returns:
#             Converted float value, or 0.0 if conversion fails
#         """
#         try:
#             if value is None:
#                 return 0.0
# 
#             if isinstance(value, complex) or hasattr(value, 'imag'):
#                 return float(np.abs(value))  # Use magnitude for complex numbers
# 
#             if isinstance(value, (list, tuple, np.ndarray)) and len(value) > 0:
#                 # Take first element from sequences
#                 return self._safe_float_conversion(value[0])
# 
#             return float(value)
# 
#         except (TypeError, ValueError, IndexError) as e:
#             # More specific error handling
#             print(f"⚠️ Conversion error ({type(value)}): {e}")
#             return 0.0
#         except Exception as e:
#             print(f"❌ Unexpected error in float conversion: {e}")
#             return 0.0
# 
#     def _trim_history(self):
#         """Trim history to maximum length to prevent memory issues."""
#         for key in self.history:
#             if len(self.history[key]) > self.max_history_length:
#                 self.history[key] = self.history[key][-self.max_history_length:]
# 
#     def update(self, metrics: Dict[str, Any], update_interval: int = 1):
#         """
#         Update the visualization with new simulation metrics.
# 
#         Args:
#             metrics: Dictionary of metrics to visualize
#             update_interval: Only update the plot every N calls to reduce overhead
#         """
#         if self.is_closed or self.paused:
#             return
# 
#         try:
#             # Update counters
#             self.update_counter += 1
# 
#             # Always update the history
#             self._update_history(metrics)
# 
#             # Only update the visualization at specified intervals
#             if self.update_counter % update_interval != 0:
#                 return
# 
#             # Update plot data
#             for key, line in self.lines.items():
#                 if key in self.history and self.history[key]:
#                     data = self.history[key]
#                     x_data = list(range(len(data)))
#                     line.set_data(x_data, data)
# 
#                     # Dynamically update x-axis limits
#                     ax = line.axes
#                     ax.set_xlim(0, max(100, len(data)))
# 
#                     # Update y-axis if values exceed current limits
#                     y_min, y_max = ax.get_ylim()
#                     data_min, data_max = min(data), max(data)
# 
#                     # Add padding to y-axis
#                     data_range = max(0.1, data_max - data_min)
#                     new_min = max(0, data_min - 0.1 * data_range)
#                     new_max = data_max + 0.1 * data_range
# 
#                     if new_min < y_min or new_max > y_max:
#                         ax.set_ylim(new_min, new_max)
# 
#             # Refresh the figure
#             self.fig.canvas.draw_idle()
#             self.fig.canvas.flush_events()
# 
#         except Exception as e:
#             print(f"❌ Error updating visualization: {e}")
#             traceback.print_exc()
# 
#     def _update_history(self, metrics: Dict[str, Any]):
#         """
#         Update history with new metrics.
# 
#         Args:
#             metrics: Dictionary of metrics to add to history
#         """
#         try:
#             # Update main metrics
#             for key in ['distinction_level', 'coherence', 'entropy', 'stability']:
#                 # Handle different naming conventions
#                 actual_key = key
#                 if key == 'distinction_level':
#                     actual_key = 'distinction'
# 
#                 # Get value using both possible key names
#                 value = metrics.get(key, metrics.get(actual_key, None))
# 
#                 if value is not None:
#                     self.history[actual_key].append(self._safe_float_conversion(value))
# 
#             # Add additional metrics if available
#             additional_metrics = [
#                 ('surplus_stability', ['surplus_stability', 'surplus_state_stability']),
#                 ('phase', ['phase', 'quantum_phase'])
#             ]
# 
#             for hist_key, possible_keys in additional_metrics:
#                 for metric_key in possible_keys:
#                     if metric_key in metrics:
#                         self.history[hist_key].append(self._safe_float_conversion(metrics[metric_key]))
#                         break
# 
#             # Trim history to prevent memory issues
#             self._trim_history()
# 
#         except Exception as e:
#             print(f"❌ Error updating history: {e}")
#             traceback.print_exc()
# 
#     def add_subplot(self, key: str, title: str, color: str = None):
#         """
#         Add a new subplot for a custom metric.
# 
#         Args:
#             key: Key of the metric to plot
#             title: Title for the subplot
#             color: Line color (if None, a default will be chosen)
#         """
#         if self.is_closed:
#             return
# 
#         try:
#             # Create a new figure for the additional plot
#             new_fig, ax = plt.subplots(figsize=(8, 5))
#             ax.set_title(title)
#             ax.set_xlabel('Steps')
#             ax.set_ylabel('Value')
#             ax.grid(True, linestyle='--', alpha=0.7)
# 
#             # Choose color
#             if color is None:
#                 color = self.plot_colors.get(key, 'b')
# 
#             # Create line
#             line, = ax.plot([], [], f'{color}-', label=key.capitalize())
#             ax.legend()
# 
#             # Register the new line
#             self.lines[key] = line
# 
#             # Ensure history exists for this key
#             if key not in self.history:
#                 self.history[key] = []
# 
#             plt.tight_layout()
# 
#             return new_fig, ax
# 
#         except Exception as e:
#             print(f"❌ Error adding subplot: {e}")
#             traceback.print_exc()
#             return None, None
# 
#     def save_plots(self, filename_prefix: str = "simulation"):
#         """
#         Save all plots to files.
# 
#         Args:
#             filename_prefix: Prefix for the saved files
#         """
#         if self.is_closed:
#             return
# 
#         try:
#             # Save main figure
#             self.fig.savefig(f"{filename_prefix}_main.png", dpi=300, bbox_inches='tight')
#             print(f"✅ Main plot saved to {filename_prefix}_main.png")
# 
#             # Create and save a plot of all history data
#             if self.history:
#                 hist_fig, hist_ax = plt.subplots(figsize=(12, 8))
#                 for key in self.history:
#                     if self.history[key]:
#                         hist_ax.plot(
#                             self.history[key],
#                             label=key.capitalize(),
#                             color=self.plot_colors.get(key, 'b')
#                         )
# 
#                 hist_ax.set_title('Complete Simulation History')
#                 hist_ax.set_xlabel('Steps')
#                 hist_ax.set_ylabel('Value')
#                 hist_ax.grid(True, linestyle='--', alpha=0.7)
#                 hist_ax.legend()
# 
#                 hist_fig.savefig(f"{filename_prefix}_history.png", dpi=300, bbox_inches='tight')
#                 plt.close(hist_fig)
#                 print(f"✅ History plot saved to {filename_prefix}_history.png")
# 
#         except Exception as e:
#             print(f"❌ Error saving plots: {e}")
#             traceback.print_exc()
# 
#     def pause(self):
#         """Pause the visualization updates."""
#         self.paused = True
# 
#     def resume(self):
#         """Resume the visualization updates."""
#         self.paused = False
# 
#     def close(self):
#         """Properly close the visualization."""
#         if self.is_closed:
#             return
# 
#         try:
#             plt.close(self.fig)
#             # Close any other figures that might have been created
#             plt.close('all')
#             self.is_closed = True
#             print("✅ Visualization closed")
#         except Exception as e:
#             print(f"❌ Error closing visualization: {e}")
#             traceback.print_exc()
#

"""# 14. Symbolic Output"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile symbolic_output.py
# 
# import random
# import time
# import numpy as np
# from typing import Dict, List, Optional, Tuple, Any
# from collections import deque
# 
# class SymbolicOutput:
#     """
#     Enhanced symbolic output generator that integrates with quantum and cognitive metrics
#     to produce meaningful symbolic expressions representing the system's emergent states.
# 
#     The system uses weighted vocabulary selection based on the agent's current
#     quantum and cognitive state to generate expressions that reflect the underlying
#     computational ontology.
#     """
#     def __init__(self, vocabulary_size: int = 12):
#         # Basic vocabulary with hierarchical organization
#         self.state_descriptors = [
#             "Flux", "Equilibrium", "Distinction", "Recursion",
#             "Convergence", "Divergence", "Resonance", "Coherence",
#             "Entanglement", "Superposition", "Bifurcation", "Integration"
#         ]
# 
#         self.relations = [
#             "aligns with", "dissolves across", "bends toward",
#             "extends beyond", "contracts into", "resonates within",
#             "differentiates from", "converges upon", "enfolds",
#             "stabilizes within", "emerges through", "transcends"
#         ]
# 
#         self.surplus_concepts = [
#             "stability", "recursion", "entropy", "phase shift",
#             "emergence", "ontology", "distinction", "coherence",
#             "complexity", "dimensionality", "feedback", "symmetry"
#         ]
# 
#         # Additional vocabularies for more complex expressions
#         self.modifiers = [
#             "partially", "deeply", "recursively", "gradually",
#             "suddenly", "coherently", "chaotically", "uniquely",
#             "systematically", "emergently", "distinctively", "subtly"
#         ]
# 
#         self.secondary_concepts = [
#             "phase space", "attractor", "strange loop", "dynamic pattern",
#             "boundary condition", "information field", "critical point", "nonlinearity",
#             "fractal domain", "resonant structure", "emergent property", "computational ontology"
#         ]
# 
#         # Track historical expressions
#         self.expression_history = []
#         self.emergence_events = []
#         self.pattern_history = deque(maxlen=100)
#         self.frequency_analysis = {}
# 
#         # Thresholds for state categorization
#         self.coherence_thresholds = {
#             'low': 0.3,
#             'medium': 0.6,
#             'high': 0.8
#         }
#         self.distinction_thresholds = {
#             'low': 0.3,
#             'medium': 0.6,
#             'high': 0.8
#         }
#         self.surplus_thresholds = {
#             'low': 1.0,
#             'medium': 3.0,
#             'high': 6.0
#         }
# 
#         # Advanced pattern recognition
#         self.transition_matrix = np.zeros((vocabulary_size, vocabulary_size))
#         self.transition_counts = np.zeros((vocabulary_size, vocabulary_size))
#         self.descriptor_index = {desc: i for i, desc in enumerate(self.state_descriptors)}
# 
#         # Tracks the last generated elements for transition analysis
#         self.last_descriptor = None
#         self.expression_complexity = 1.0
# 
#         # Initialize timestamp for real-time tracking
#         self.start_time = time.time()
# 
#     def _update_transition_statistics(self, current_descriptor: str):
#         """
#         Update the transition matrix for pattern analysis.
# 
#         Args:
#             current_descriptor: The descriptor used in the current expression
#         """
#         try:
#             if self.last_descriptor is not None and current_descriptor in self.descriptor_index:
#                 prev_idx = self.descriptor_index.get(self.last_descriptor)
#                 curr_idx = self.descriptor_index.get(current_descriptor)
# 
#                 if prev_idx is not None and curr_idx is not None:
#                     # Increment transition count
#                     self.transition_counts[prev_idx, curr_idx] += 1
# 
#                     # Update transition probability
#                     row_sum = np.sum(self.transition_counts[prev_idx, :])
#                     if row_sum > 0:
#                         self.transition_matrix[prev_idx, :] = self.transition_counts[prev_idx, :] / row_sum
# 
#             # Update last descriptor
#             self.last_descriptor = current_descriptor
#         except Exception as e:
#             print(f"Error updating transition statistics: {e}")
# 
#     def _calculate_weights(self,
#                           surplus: float,
#                           distinction: float,
#                           coherence: float,
#                           entropy: Optional[float] = None,
#                           dimensionality: Optional[int] = None) -> Tuple[List[float], List[float], List[float]]:
#         """
#         Calculate vocabulary selection weights based on current metrics.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             entropy: Optional entropy metric
#             dimensionality: Optional detected dimensionality
# 
#         Returns:
#             Tuple of weights for descriptors, relations, and concepts
#         """
#         try:
#             # Initialize weights
#             descriptor_weights = np.ones(len(self.state_descriptors)) / len(self.state_descriptors)
#             relation_weights = np.ones(len(self.relations)) / len(self.relations)
#             concept_weights = np.ones(len(self.surplus_concepts)) / len(self.surplus_concepts)
# 
#             # Adjust based on coherence
#             if coherence > self.coherence_thresholds['high']:
#                 # High coherence: favor structured, aligned, stabilized expressions
#                 descriptor_weights = np.array([0.1, 0.3, 0.2, 0.1, 0.2, 0.1, 0, 0, 0, 0, 0, 0])
#                 relation_weights = np.array([0.1, 0, 0.1, 0.1, 0, 0.2, 0.1, 0.2, 0.1, 0.1, 0, 0])
#                 concept_weights = np.array([0.3, 0.1, 0, 0, 0.2, 0.1, 0.1, 0.2, 0, 0, 0, 0])
#             elif coherence > self.coherence_thresholds['medium']:
#                 # Medium coherence: balanced distribution
#                 descriptor_weights = np.array([0.1, 0.1, 0.2, 0.2, 0.1, 0.1, 0.1, 0.1, 0, 0, 0, 0])
#                 relation_weights = np.array([0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0, 0])
#                 concept_weights = np.array([0.1, 0.2, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0, 0, 0])
#             else:
#                 # Low coherence: favor flux, entropy, dissolution
#                 descriptor_weights = np.array([0.3, 0, 0.1, 0.1, 0, 0.3, 0.1, 0, 0.1, 0, 0, 0])
#                 relation_weights = np.array([0, 0.3, 0.2, 0, 0.2, 0, 0.1, 0, 0.1, 0, 0.1, 0])
#                 concept_weights = np.array([0, 0.1, 0.3, 0.2, 0, 0, 0.1, 0, 0.2, 0.1, 0, 0])
# 
#             # Adjust based on distinction level
#             if distinction > self.distinction_thresholds['high']:
#                 # Favor distinction and emergence concepts
#                 descriptor_weights[2] *= 2.0  # Distinction
#                 relation_weights[10] = max(relation_weights[10], 0.2)  # "emerges through"
#                 concept_weights[4] = max(concept_weights[4], 0.3)  # "emergence"
# 
#                 # Increase probability of integration for high distinction
#                 descriptor_weights[11] = max(descriptor_weights[11], 0.2)  # Integration
# 
#             elif distinction < self.distinction_thresholds['low']:
#                 # Favor flux and entropy
#                 descriptor_weights[0] *= 2.0  # Flux
#                 concept_weights[2] = max(concept_weights[2], 0.3)  # "entropy"
# 
#                 # Increase probability of dissolution terms
#                 relation_weights[1] = max(relation_weights[1], 0.3)  # "dissolves across"
# 
#             # Adjust for surplus level
#             if surplus > self.surplus_thresholds['high']:
#                 # High surplus: favor differentiation and expansion
#                 relation_weights[6] = max(relation_weights[6], 0.3)  # "differentiates from"
#                 concept_weights[6] = max(concept_weights[6], 0.3)  # "distinction"
#                 descriptor_weights[3] = max(descriptor_weights[3], 0.2)  # "Recursion"
#             elif surplus < self.surplus_thresholds['low']:
#                 # Low surplus: favor contraction and stability
#                 relation_weights[4] = max(relation_weights[4], 0.3)  # "contracts into"
#                 concept_weights[0] = max(concept_weights[0], 0.3)  # "stability"
# 
#             # Adjust for entropy if provided
#             if entropy is not None:
#                 if entropy > 0.7:  # High entropy
#                     descriptor_weights[0] = max(descriptor_weights[0], 0.3)  # Flux
#                     descriptor_weights[5] = max(descriptor_weights[5], 0.3)  # Divergence
#                     concept_weights[2] = max(concept_weights[2], 0.3)  # "entropy"
#                 elif entropy < 0.3:  # Low entropy
#                     descriptor_weights[1] = max(descriptor_weights[1], 0.3)  # Equilibrium
#                     descriptor_weights[7] = max(descriptor_weights[7], 0.3)  # Coherence
# 
#             # Adjust for dimensionality if provided
#             if dimensionality is not None:
#                 if dimensionality > 3:  # Higher dimensions
#                     descriptor_weights[11] = max(descriptor_weights[11], 0.4)  # "Integration"
#                     relation_weights[11] = max(relation_weights[11], 0.3)  # "transcends"
#                     concept_weights[9] = max(concept_weights[9], 0.4)  # "dimensionality"
# 
#                     # Extra emphasis on emergence for higher dimensions
#                     if dimensionality > 4:
#                         concept_weights[4] = max(concept_weights[4], 0.5)  # "emergence"
#                         relation_weights[10] = max(relation_weights[10], 0.4)  # "emerges through"
# 
#             # Normalize weights
#             descriptor_weights = descriptor_weights / np.sum(descriptor_weights)
#             relation_weights = relation_weights / np.sum(relation_weights)
#             concept_weights = concept_weights / np.sum(concept_weights)
# 
#             return descriptor_weights.tolist(), relation_weights.tolist(), concept_weights.tolist()
# 
#         except Exception as e:
#             print(f"Error calculating expression weights: {e}")
#             # Return uniform weights as fallback
#             uniform_desc = [1.0/len(self.state_descriptors)] * len(self.state_descriptors)
#             uniform_rel = [1.0/len(self.relations)] * len(self.relations)
#             uniform_con = [1.0/len(self.surplus_concepts)] * len(self.surplus_concepts)
#             return uniform_desc, uniform_rel, uniform_con
# 
#     def _generate_expression_components(self,
#                                        descriptor_weights: List[float],
#                                        relation_weights: List[float],
#                                        concept_weights: List[float],
#                                        metrics: Dict[str, float]) -> Tuple[str, str, str, Optional[str]]:
#         """
#         Generate components for a symbolic expression based on weighted vocabularies.
# 
#         Args:
#             descriptor_weights: Weights for selecting state descriptors
#             relation_weights: Weights for selecting relations
#             concept_weights: Weights for selecting concepts
#             metrics: Dictionary of current system metrics
# 
#         Returns:
#             Tuple of (descriptor, relation, concept, modifier)
#         """
#         try:
#             # Select components based on weighted probabilities
#             descriptor = random.choices(self.state_descriptors, weights=descriptor_weights, k=1)[0]
#             relation = random.choices(self.relations, weights=relation_weights, k=1)[0]
#             concept = random.choices(self.surplus_concepts, weights=concept_weights, k=1)[0]
# 
#             # Determine if we should use a modifier based on complexity
#             use_modifier = random.random() < self.expression_complexity * 0.5
#             modifier = random.choice(self.modifiers) if use_modifier else None
# 
#             # Special case for extreme states
#             coherence = metrics.get('coherence', 0.5)
#             distinction = metrics.get('distinction', 0.5)
# 
#             if coherence > 0.95 and distinction > 0.9:
#                 descriptor = "Coherent Distinction"
#                 relation = "stabilizes within"
#                 concept = "emergent ontology"
#                 modifier = "systematically"
#             elif coherence < 0.1 and distinction < 0.1:
#                 descriptor = "Entropic Flux"
#                 relation = "dissolves across"
#                 concept = "undifferentiated phase space"
#                 modifier = "chaotically"
# 
#             # Update transition statistics for pattern analysis
#             self._update_transition_statistics(descriptor)
# 
#             return descriptor, relation, concept, modifier
# 
#         except Exception as e:
#             print(f"Error generating expression components: {e}")
#             return "Flux", "aligns with", "stability", None
# 
#     def generate_symbolic_expression(self,
#                                     surplus: float,
#                                     distinction: float,
#                                     coherence: float,
#                                     entropy: Optional[float] = None,
#                                     dimensionality: Optional[int] = None) -> str:
#         """
#         Generates a symbolic expression based on the system's current metrics.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             entropy: Optional entropy metric
#             dimensionality: Optional detected dimensionality
# 
#         Returns:
#             A symbolic expression representing the current state
#         """
#         try:
#             # Ensure inputs are proper floats for stability
#             surplus = float(np.clip(surplus, 0.1, 10.0))
#             distinction = float(np.clip(distinction, 0.0, 1.0))
#             coherence = float(np.clip(coherence, 0.0, 1.0))
# 
#             # Prepare metrics for component generation
#             metrics = {
#                 'surplus': surplus,
#                 'distinction': distinction,
#                 'coherence': coherence,
#                 'entropy': entropy,
#                 'dimensionality': dimensionality,
#                 'time_elapsed': time.time() - self.start_time
#             }
# 
#             # Calculate vocabulary selection weights
#             descriptor_weights, relation_weights, concept_weights = self._calculate_weights(
#                 surplus, distinction, coherence, entropy, dimensionality
#             )
# 
#             # Generate expression components
#             descriptor, relation, concept, modifier = self._generate_expression_components(
#                 descriptor_weights, relation_weights, concept_weights, metrics
#             )
# 
#             # Adapt expression complexity based on system metrics
#             self.expression_complexity = min(2.0, 0.5 + 0.5 * coherence + 0.3 * distinction + 0.2 * (surplus / 10.0))
# 
#             # Use secondary concepts with a certain probability based on complexity
#             use_secondary = random.random() < self.expression_complexity * 0.3
# 
#             if use_secondary:
#                 secondary = random.choice(self.secondary_concepts)
#                 concept = f"{concept} within {secondary}"
# 
#             # Assemble the expression with or without modifier
#             if modifier:
#                 symbolic_expression = f"{descriptor} {modifier} {relation} {concept}."
#             else:
#                 symbolic_expression = f"{descriptor} {relation} {concept}."
# 
#             # Store in history with metadata
#             expression_entry = {
#                 'expression': symbolic_expression,
#                 'components': {
#                     'descriptor': descriptor,
#                     'relation': relation,
#                     'concept': concept,
#                     'modifier': modifier,
#                     'secondary': secondary if use_secondary else None
#                 },
#                 'metrics': metrics.copy(),
#                 'timestamp': time.time(),
#                 'complexity': self.expression_complexity
#             }
# 
#             self.expression_history.append(expression_entry)
# 
#             # Update pattern history
#             self.pattern_history.append({
#                 'descriptor': descriptor,
#                 'relation': relation,
#                 'concept': concept
#             })
# 
#             # Update frequency analysis
#             self._update_frequency_analysis(descriptor, relation, concept)
# 
#             return symbolic_expression
# 
#         except Exception as e:
#             print(f"Error generating symbolic expression: {e}")
#             return "Flux aligns with stability."  # Safe fallback
# 
#     def _update_frequency_analysis(self, descriptor: str, relation: str, concept: str):
#         """
#         Update frequency analysis of expression components.
# 
#         Args:
#             descriptor: The descriptor used
#             relation: The relation used
#             concept: The concept used
#         """
#         try:
#             if 'descriptors' not in self.frequency_analysis:
#                 self.frequency_analysis = {
#                     'descriptors': {},
#                     'relations': {},
#                     'concepts': {}
#                 }
# 
#             # Update descriptor frequency
#             self.frequency_analysis['descriptors'][descriptor] = (
#                 self.frequency_analysis['descriptors'].get(descriptor, 0) + 1
#             )
# 
#             # Update relation frequency
#             self.frequency_analysis['relations'][relation] = (
#                 self.frequency_analysis['relations'].get(relation, 0) + 1
#             )
# 
#             # Update concept frequency
#             self.frequency_analysis['concepts'][concept] = (
#                 self.frequency_analysis['concepts'].get(concept, 0) + 1
#             )
# 
#         except Exception as e:
#             print(f"Error updating frequency analysis: {e}")
# 
#     def handle_post_emergence(self,
#                      surplus: float,
#                      distinction: float,
#                      coherence: float,
#                      dimensionality: Optional[int] = None,
#                      entropy: Optional[float] = None) -> str:
#         """
#         Triggers symbolic output generation after dimensional emergence is detected.
#         Records emergence event and generates an appropriate symbolic expression.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             dimensionality: Optional detected dimensionality
#             entropy: Optional entropy metric
# 
#         Returns:
#             A symbolic expression representing the emergent state
#         """
#         try:
#             # Add randomness to prevent identical outputs
#             noise_factor = random.random() * 0.1
# 
#             # Record emergence event with timestamp and more detailed metrics
#             emergence_event = {
#                 'metrics': {
#                     'surplus': float(surplus) + noise_factor,
#                     'distinction': float(distinction) + noise_factor,
#                     'coherence': float(coherence) + noise_factor,
#                     'dimensionality': dimensionality,
#                     'entropy': entropy + noise_factor if entropy is not None else None,
#                     'timestamp': time.time(),
#                     'elapsed_time': time.time() - self.start_time
#                 },
#                 'event_id': len(self.emergence_events)
#             }
# 
#             self.emergence_events.append(emergence_event)
# 
#             # Increase expression complexity for emergence events
#             self.expression_complexity = min(2.0, self.expression_complexity * 1.5)
# 
#             # Generate expression with varied metrics to ensure diversity
#             varied_surplus = surplus * (1.0 + (random.random() - 0.5) * 0.2)  # +/- 10%
#             varied_distinction = distinction * (1.0 + (random.random() - 0.5) * 0.2)  # +/- 10%
#             varied_coherence = coherence * (1.0 + (random.random() - 0.5) * 0.2)  # +/- 10%
#             varied_entropy = entropy * (1.0 + (random.random() - 0.5) * 0.2) if entropy is not None else None
# 
#             expression = self.generate_symbolic_expression(
#                 varied_surplus, varied_distinction, varied_coherence,
#                 entropy=varied_entropy,
#                 dimensionality=dimensionality
#             )
# 
#             # For emergence events, generate a more complex secondary expression
#             if len(self.emergence_events) > 1:
#                 # Analyze emergence patterns
#                 patterns = self.analyze_emergence_patterns()
# 
#                 # Use the pattern analysis to generate a deeper insight with randomization
#                 if random.random() < 0.7 and patterns.get('dominant_patterns'):
#                     dominant = patterns['dominant_patterns']
# 
#                     # Create varied secondary expressions
#                     secondary_expressions = [
#                         f"Pattern analysis indicates {dominant.get('descriptor', 'Emergence')} "
#                         f"{random.choice(self.relations)} "
#                         f"{dominant.get('concept', 'complexity')} "
#                         f"across {dimensionality if dimensionality else 'multiple'} dimensions.",
# 
#                         f"Dimensional shift to {dimensionality}D reveals {dominant.get('descriptor', 'Emergence')} "
#                         f"{random.choice(self.relations)} "
#                         f"{random.choice(self.surplus_concepts)}.",
# 
#                         f"The {dimensionality}D structure {random.choice(self.modifiers)} "
#                         f"{random.choice(self.relations)} "
#                         f"{dominant.get('concept', 'ontology')}.",
# 
#                         f"Analysis suggests {random.choice(self.modifiers)} {dominant.get('descriptor', 'Distinction')} "
#                         f"within the emergent {dimensionality}D domain."
#                     ]
# 
#                     # Choose one secondary expression randomly
#                     follow_up = random.choice(secondary_expressions)
#                     expression = f"{expression} {follow_up}"
# 
#             return expression
# 
#         except Exception as e:
#             print(f"Error handling post-emergence: {e}")
#             return self.generate_symbolic_expression(surplus, distinction, coherence)
# 
#     def analyze_emergence_patterns(self) -> Dict[str, Any]:
#         """
#         Analyzes patterns in emergence events and generated expressions.
#         Returns statistics and patterns detected in the symbolic outputs.
# 
#         Returns:
#             Dictionary containing pattern analysis results
#         """
#         try:
#             if not self.emergence_events or not self.expression_history:
#                 return {'patterns': 'Insufficient data for pattern analysis'}
# 
#             # Extract metrics from history
#             coherence_values = [e['metrics']['coherence'] for e in self.emergence_events]
#             distinction_values = [e['metrics']['distinction'] for e in self.emergence_events]
# 
#             # Only use recent expressions for pattern analysis if we have many
#             expressions_to_analyze = self.expression_history
#             if len(expressions_to_analyze) > 20:
#                 expressions_to_analyze = expressions_to_analyze[-20:]
# 
#             # Calculate emergence stability
#             coherence_stability = float(np.std(coherence_values)) if len(coherence_values) > 1 else 0
#             distinction_stability = float(np.std(distinction_values)) if len(distinction_values) > 1 else 0
# 
#             # More advanced pattern analysis with the frequency analysis
#             if hasattr(self, 'frequency_analysis') and self.frequency_analysis:
#                 # Find most common components
#                 descriptor_counts = self.frequency_analysis.get('descriptors', {})
#                 relation_counts = self.frequency_analysis.get('relations', {})
#                 concept_counts = self.frequency_analysis.get('concepts', {})
# 
#                 # Find dominant patterns
#                 dominant_descriptor = max(descriptor_counts.items(), key=lambda x: x[1])[0] if descriptor_counts else None
#                 dominant_relation = max(relation_counts.items(), key=lambda x: x[1])[0] if relation_counts else None
#                 dominant_concept = max(concept_counts.items(), key=lambda x: x[1])[0] if concept_counts else None
# 
#                 # Calculate component diversity (normalized entropy)
#                 def calculate_diversity(counts):
#                     if not counts:
#                         return 0.0
#                     total = sum(counts.values())
#                     probabilities = [count/total for count in counts.values()]
#                     entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
#                     max_entropy = np.log2(len(counts))
#                     return entropy / max_entropy if max_entropy > 0 else 0.0
# 
#                 descriptor_diversity = calculate_diversity(descriptor_counts)
#                 relation_diversity = calculate_diversity(relation_counts)
#                 concept_diversity = calculate_diversity(concept_counts)
# 
#                 # Find common sequences in the pattern history
#                 sequence_patterns = {}
#                 if len(self.pattern_history) > 3:
#                     for i in range(len(self.pattern_history) - 2):
#                         seq = (
#                             self.pattern_history[i]['descriptor'],
#                             self.pattern_history[i+1]['descriptor'],
#                             self.pattern_history[i+2]['descriptor']
#                         )
#                         sequence_patterns[seq] = sequence_patterns.get(seq, 0) + 1
# 
#                 # Find most common sequence
#                 common_sequence = max(sequence_patterns.items(), key=lambda x: x[1])[0] if sequence_patterns else None
# 
#                 # Typical expression
#                 typical_expression = f"{dominant_descriptor} {dominant_relation} {dominant_concept}."
# 
#                 # Calculate complexity trend
#                 complexity_values = [e.get('complexity', 1.0) for e in self.expression_history[-10:]]
#                 complexity_trend = np.mean(np.diff(complexity_values)) if len(complexity_values) > 1 else 0.0
# 
#                 # Calculate transition matrix entropy (measure of pattern predictability)
#                 transition_entropy = 0.0
#                 if hasattr(self, 'transition_matrix') and isinstance(self.transition_matrix, np.ndarray):
#                     for row in self.transition_matrix:
#                         row_probs = row[row > 0]  # Only consider non-zero probabilities
#                         if len(row_probs) > 0:
#                             row_entropy = -np.sum(row_probs * np.log2(row_probs))
#                             transition_entropy += row_entropy
# 
#                     transition_entropy /= max(1, np.sum(self.transition_matrix > 0))  # Normalize
# 
#                 return {
#                     'emergence_count': len(self.emergence_events),
#                     'expression_count': len(self.expression_history),
#                     'coherence_stability': float(coherence_stability),
#                     'distinction_stability': float(distinction_stability),
#                     'component_diversity': {
#                         'descriptor': float(descriptor_diversity),
#                         'relation': float(relation_diversity),
#                         'concept': float(concept_diversity),
#                         'overall': float((descriptor_diversity + relation_diversity + concept_diversity) / 3)
#                     },
#                     'dominant_patterns': {
#                         'descriptor': dominant_descriptor,
#                         'relation': dominant_relation,
#                         'concept': dominant_concept
#                     },
#                     'common_sequence': common_sequence,
#                     'typical_expression': typical_expression,
#                     'complexity_trend': float(complexity_trend),
#                     'transition_entropy': float(transition_entropy),
#                     'current_complexity': float(self.expression_complexity)
#                 }
# 
#             # Simplified analysis if frequency data isn't available
#             return {
#                 'emergence_count': len(self.emergence_events),
#                 'expression_count': len(self.expression_history),
#                 'coherence_stability': float(coherence_stability),
#                 'distinction_stability': float(distinction_stability)
#             }
# 
#         except Exception as e:
#             print(f"Error analyzing emergence patterns: {e}")
#             return {
#                 'error': str(e),
#                 'emergence_count': len(self.emergence_events),
#                 'expression_count': len(self.expression_history)
#             }
# 
#     def get_expression_history(self, limit: int = 10) -> List[Dict[str, Any]]:
#         """
#         Returns the most recent expression history.
# 
#         Args:
#             limit: Maximum number of expressions to return
# 
#         Returns:
#             List of recent expression entries
#         """
#         try:
#             if not self.expression_history:
#                 return []
# 
#             recent = self.expression_history[-limit:]
#             return recent
# 
#         except Exception as e:
#             print(f"Error getting expression history: {e}")
#             return []
# 
# # Example usage
# if __name__ == "__main__":
#     symbolic_system = SymbolicOutput()
#     example_expression = symbolic_system.handle_post_emergence(surplus=1.5, distinction=0.4, coherence=0.8)
#     print("Generated Symbolic Expression:", example_expression)
#

"""# 15. Semantic Enhanced Output"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile semantic_enhanced_output.py
# """
# Semantic Enhanced Output Module for Émile-2 Simulation
# ------------------------------------------------------
# This module provides functions to process agent output, enhance its
# semantic content, and integrate with different data stores.
# """
# import logging
# import json
# import torch
# import torch.nn as nn
# import numpy as np
# from typing import Dict, List, Optional, Tuple, Union, Any
# import traceback
# from collections import deque
# from dataclasses import dataclass, field
# from abc import ABC, abstractmethod
# import time
# import os
# import hashlib
# from datetime import datetime
# import random
# from transformers import AutoTokenizer, AutoModel
# from sentence_transformers import SentenceTransformer
# 
# 
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.semantic_enhanced_output")
# 
# # Import necessary constants
# from utilities import (
#     DEVICE,
#     MAX_ENTROPY,
#     MINIMUM_COHERENCE_FLOOR,
#     MOMENTUM_DECAY,
#     DISTINCTION_ANCHOR_WEIGHT,
#     PHASE_SCALING_FACTOR,
#     COLLAPSE_DISSIPATION_THRESHOLD,
#     COLLAPSE_DISSIPATION_RATE,
#     CORE_DISTINCTION_UPDATE_RATE,
#     HIDDEN_DIM,
#     NUM_TRANSFORMER_HEADS,
#     NUM_TRANSFORMER_LAYERS,
#     GRADIENT_CLIP_VALUE,
#     WEIGHT_DECAY,
#     LEARNING_RATE,
#     LEARNING_RATE_MIN,
#     LEARNING_RATE_MAX,
#     REWARD_SCALING,
#     INSTABILITY_GRACE_PERIOD,
#     SURPLUS_ADJUSTMENT_RATE,
#     SURPLUS_THRESHOLD,
#     MAX_SURPLUS,
#     EXPULSION_RECOVERY_RATE,
#     SURPLUS_RECYCLE_FRACTION,
#     EVOLUTION_TIME
# )
# from data_classes import SurplusState, TransformerOutput
# from base_quantum import BaseQuantumState
# 
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.semantic_enhanced_output")
# 
# # Import necessary constants
# from utilities import (
#     DEVICE,
#     MAX_ENTROPY,
#     MINIMUM_COHERENCE_FLOOR,
#     MOMENTUM_DECAY,
#     DISTINCTION_ANCHOR_WEIGHT,
#     PHASE_SCALING_FACTOR,
#     COLLAPSE_DISSIPATION_THRESHOLD,
#     COLLAPSE_DISSIPATION_RATE,
#     CORE_DISTINCTION_UPDATE_RATE,
#     HIDDEN_DIM,
#     NUM_TRANSFORMER_HEADS,
#     NUM_TRANSFORMER_LAYERS,
#     GRADIENT_CLIP_VALUE,
#     WEIGHT_DECAY,
#     LEARNING_RATE,
#     LEARNING_RATE_MIN,
#     LEARNING_RATE_MAX,
#     REWARD_SCALING,
#     INSTABILITY_GRACE_PERIOD,
#     SURPLUS_ADJUSTMENT_RATE,
#     SURPLUS_THRESHOLD,
#     MAX_SURPLUS,
#     EXPULSION_RECOVERY_RATE,
#     SURPLUS_RECYCLE_FRACTION,
#     EVOLUTION_TIME
# )
# from data_classes import SurplusState, TransformerOutput
# from base_quantum import BaseQuantumState
# 
# # =============================================================================
# # Data Structures for Tracking and Processing Semantic Data
# # =============================================================================
# @dataclass
# class SemanticContext:
#     """
#     Container for the semantic context of an agent interaction.
# 
#     Includes the description, relevant state data, time, and other relevant
#     information to track, process and analyze semantic context over time.
#     """
#     description: str
#     timestamp: float
#     agent_id: str
#     surplus_state: SurplusState
#     quantum_state_metrics: Dict[str, float]
#     transformer_output: TransformerOutput
#     external_context: Dict[str, Any] = field(default_factory=dict)
#     metadata: Dict[str, Any] = field(default_factory=dict)
# 
#     def __post_init__(self):
#         """Ensure values are correctly initialized."""
#         try:
#             if not isinstance(self.timestamp, (int, float)):
#                 self.timestamp = time.time()
#                 logger.warning("Invalid timestamp, using current time.")
#             if not isinstance(self.agent_id, str):
#                 self.agent_id = str(self.agent_id)  # Attempt to convert
#                 logger.warning(f"Invalid agent_id type, converted to string: {self.agent_id}")
#             if not isinstance(self.description, str):
#                 self.description = str(self.description)  # Attempt to convert
#                 logger.warning(f"Invalid description type, converted to string: {self.description}")
#             if not isinstance(self.surplus_state, SurplusState):
#                 logger.error("Invalid surplus_state type, defaulting to empty SurplusState")
#                 self.surplus_state = SurplusState()
#             if not isinstance(self.quantum_state_metrics, dict):
#                 logger.error("Invalid quantum_state_metrics type, defaulting to empty dict")
#                 self.quantum_state_metrics = {}
#             if not isinstance(self.transformer_output, TransformerOutput):
#                  logger.error("Invalid transformer_output type, defaulting to default TransformerOutput")
#                  self.transformer_output = TransformerOutput(torch.tensor(0.0))
# 
#             if not isinstance(self.external_context, dict):
#                 self.external_context = {}
#                 logger.warning("Invalid external_context type, defaulted to dict")
#             if not isinstance(self.metadata, dict):
#                 self.metadata = {}
#                 logger.warning("Invalid metadata type, defaulted to dict")
#         except Exception as e:
#             logger.error(f"Error in SemanticContext post-init: {e}")
#             self.timestamp = time.time()
#             self.agent_id = "unknown"
#             self.description = "default"
#             self.surplus_state = SurplusState()
#             self.quantum_state_metrics = {}
#             self.transformer_output = TransformerOutput(torch.tensor(0.0))
#             self.external_context = {}
#             self.metadata = {}
# 
#     def validate(self) -> bool:
#         """Validate SemanticContext fields."""
#         try:
#             if not isinstance(self.timestamp, (int, float)):
#                 logger.error(f"Invalid type for timestamp: {type(self.timestamp)}")
#                 return False
#             if not isinstance(self.agent_id, str):
#                 logger.error(f"Invalid type for agent_id: {type(self.agent_id)}")
#                 return False
#             if not isinstance(self.description, str):
#                 logger.error(f"Invalid type for description: {type(self.description)}")
#                 return False
#             if not isinstance(self.surplus_state, SurplusState):
#                 logger.error(f"Invalid type for surplus_state: {type(self.surplus_state)}")
#                 return False
#             if not isinstance(self.quantum_state_metrics, dict):
#                 logger.error(f"Invalid type for quantum_state_metrics: {type(self.quantum_state_metrics)}")
#                 return False
#             if not isinstance(self.transformer_output, TransformerOutput):
#                 logger.error(f"Invalid type for transformer_output: {type(self.transformer_output)}")
#                 return False
#             if not isinstance(self.external_context, dict):
#                 logger.error(f"Invalid type for external_context: {type(self.external_context)}")
#                 return False
#             if not isinstance(self.metadata, dict):
#                 logger.error(f"Invalid type for metadata: {type(self.metadata)}")
#                 return False
#             if not self.surplus_state.validate():
#                  logger.error("Surplus state is invalid")
#                  return False
#             if not self.transformer_output.validate():
#                 logger.error("Transformer output is invalid")
#                 return False
# 
#             return True
# 
#         except Exception as e:
#             logger.error(f"Error validating SemanticContext: {e}")
#             return False
# 
#     def copy(self) -> 'SemanticContext':
#         """
#         Create a deep copy of the SemanticContext object.
# 
#         Returns:
#             A new SemanticContext object with copied attributes, or a default
#             instance if copying fails
#         """
#         try:
#             return SemanticContext(
#                 description=str(self.description) if self.description else "",  # Ensure string
#                 timestamp=float(self.timestamp),  # Ensure float
#                 agent_id=str(self.agent_id) if self.agent_id else "default_agent",  # Ensure string
#                 surplus_state=self.surplus_state.copy(),
#                 quantum_state_metrics=dict(self.quantum_state_metrics),
#                 transformer_output=self.transformer_output.to(self.transformer_output.device),
#                 external_context=dict(self.external_context),
#                 metadata=dict(self.metadata)
#             )
#         except Exception as e:
#             logger.error(f"Error in copying SemanticContext: {e}")
#             # Return default
#             return SemanticContext(
#                     description = "default",
#                     timestamp = time.time(),
#                     agent_id = "default_agent",
#                     surplus_state = SurplusState(),
#                     quantum_state_metrics = {},
#                     transformer_output = TransformerOutput(torch.tensor(0.0)),
#                     external_context={},
#                     metadata={}
#                 )
# 
# @dataclass
# class SemioticModelOutput:
#     """Container for the output of the semiotic model."""
#     prediction: torch.Tensor
#     value_estimate: torch.Tensor
#     relevance_score: float
#     attention_weights: Dict[str, torch.Tensor]
#     entropy: Optional[torch.Tensor] = None
#     coherence_estimate: Optional[torch.Tensor] = None
# 
#     def __post_init__(self):
#         """
#         Ensure all tensors are initialized and on the correct device.
#         """
#         try:
#             # Ensure prediction is a tensor
#             if not isinstance(self.prediction, torch.Tensor):
#                 logger.warning(f"Prediction is not a tensor, converting from {type(self.prediction)}")
#                 try:
#                     self.prediction = torch.tensor(self.prediction, dtype=torch.float32)
#                 except Exception as e:
#                     logger.error(f"Could not convert prediction to tensor: {e}")
#                     self.prediction = torch.tensor(0.0)
# 
#             # Ensure value_estimate is a tensor
#             if not isinstance(self.value_estimate, torch.Tensor):
#                 logger.warning(f"Value estimate is not a tensor, converting from {type(self.value_estimate)}")
#                 try:
#                     self.value_estimate = torch.tensor(self.value_estimate, dtype=torch.float32)
#                 except Exception as e:
#                     logger.error(f"Could not convert value estimate to tensor: {e}")
#                     self.value_estimate = torch.tensor(0.0)
# 
#             # Ensure relevance_score is a float
#             if not isinstance(self.relevance_score, float):
#                 logger.warning(f"Relevance score is not a float, converting from {type(self.relevance_score)}")
#                 try:
#                    self.relevance_score = float(self.relevance_score)
#                 except Exception as e:
#                     logger.error(f"Could not convert relevance score to float: {e}, using default 0.0")
#                     self.relevance_score = 0.0
# 
#             # Handle entropy
#             if self.entropy is None:
#                 self.entropy = torch.tensor(0.0, device=self.prediction.device)
#             elif not isinstance(self.entropy, torch.Tensor):
#                 try:
#                     self.entropy = torch.tensor(self.entropy, device=self.prediction.device)
#                 except Exception as e:
#                     logger.error(f"Could not convert entropy to tensor: {e}")
#                     self.entropy = torch.tensor(0.0, device=self.prediction.device)
# 
#             # Handle coherence estimate
#             if self.coherence_estimate is None:
#                 self.coherence_estimate = torch.tensor(MINIMUM_COHERENCE_FLOOR, device=self.prediction.device)
#             elif not isinstance(self.coherence_estimate, torch.Tensor):
#                 try:
#                     self.coherence_estimate = torch.tensor(self.coherence_estimate, device=self.prediction.device)
#                 except Exception as e:
#                     logger.error(f"Could not convert coherence_estimate to tensor: {e}")
#                     self.coherence_estimate = torch.tensor(MINIMUM_COHERENCE_FLOOR, device=self.prediction.device)
# 
#             # Ensure attention weights are proper tensors
#             if not isinstance(self.attention_weights, dict):
#                 logger.warning(f"Attention weights is not a dict, initializing empty dict")
#                 self.attention_weights = {}
#             else:
#                 for key, value in list(self.attention_weights.items()):
#                     if not isinstance(value, torch.Tensor):
#                         try:
#                             self.attention_weights[key] = torch.tensor(value, device=self.prediction.device)
#                         except Exception as e:
#                             logger.error(f"Could not convert attention weight {key} to tensor: {e}")
#                             del self.attention_weights[key]
#         except Exception as e:
#                 logger.error(f"Error during SemioticModelOutput initialization: {e}")
# 
#                 device = getattr(self.prediction, 'device', None)
#                 if device is None:
#                   device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
#                 self.prediction = torch.tensor(0.0, device=device)
#                 self.value_estimate = torch.tensor(0.0, device=device)
#                 self.relevance_score = 0.0
#                 self.attention_weights = {}
#                 self.entropy = torch.tensor(0.0, device=device)
#                 self.coherence_estimate = torch.tensor(MINIMUM_COHERENCE_FLOOR, device=device)
# 
#     def validate(self) -> bool:
#          """Validate SemioticModelOutput fields."""
#          try:
#               # Check prediction tensor
#              if not isinstance(self.prediction, torch.Tensor):
#                   logger.error("Invalid prediction type")
#                   return False
#              if torch.isnan(self.prediction).any():
#                   logger.error("NaN values in prediction")
#                   return False
# 
#              # Check value estimate tensor
#              if not isinstance(self.value_estimate, torch.Tensor):
#                  logger.error("Invalid value estimate type")
#                  return False
#              if torch.isnan(self.value_estimate).any():
#                  logger.error("NaN values in value estimate")
#                  return False
# 
#              # Check relevance score
#              if not isinstance(self.relevance_score, float):
#                  logger.error(f"Invalid type for relevance_score: {type(self.relevance_score)}")
#                  return False
#              if np.isnan(self.relevance_score):
#                  logger.error("NaN value in relevance score")
#                  return False
# 
#              # Check attention weights
#              if not isinstance(self.attention_weights, dict):
#                  logger.error("Invalid attention weights type")
#                  return False
# 
#              for key, value in self.attention_weights.items():
#                  if not isinstance(value, torch.Tensor):
#                       logger.error(f"Invalid attention weight tensor for {key}")
#                       return False
#                  if torch.isnan(value).any():
#                      logger.error(f"NaN values in attention weights for {key}")
#                      return False
# 
#              return True
#          except Exception as e:
#               logger.error(f"Error validating SemioticModelOutput: {e}")
#               return False
# 
#     @property
#     def device(self) -> torch.device:
#         """Get the device of the prediction tensor."""
#         return self.prediction.device
# 
#     def to(self, device: torch.device) -> 'SemioticModelOutput':
#         """Move all tensors to specified device."""
#         try:
#             self.prediction = self.prediction.to(device)
#             self.value_estimate = self.value_estimate.to(device)
#             if self.entropy is not None:
#                 self.entropy = self.entropy.to(device)
#             if self.coherence_estimate is not None:
#                 self.coherence_estimate = self.coherence_estimate.to(device)
# 
#             # Move attention weights
#             for k, v in self.attention_weights.items():
#                  if isinstance(v, torch.Tensor):
#                       self.attention_weights[k] = v.to(device)
# 
#             return self
#         except Exception as e:
#             logger.error(f"Error moving tensors to device: {e}")
#             return self
# 
#     def get_prediction_value(self) -> float:
#         """Safely extract prediction value as float."""
#         try:
#             if self.prediction is None:
#                 return 0.0
# 
#             # Handle different tensor shapes
#             if self.prediction.dim() == 0:  # Scalar
#                 return self.prediction.item()
#             elif self.prediction.dim() == 1:  # Vector
#                 return self.prediction[0].item()
#             elif self.prediction.dim() == 2:  # Matrix
#                 return self.prediction[0, 0].item()
#             elif self.prediction.dim() == 3:  # 3D tensor
#                 return self.prediction[0, 0, 0].item()
#             else:
#                 return self.prediction.mean().item()
#         except Exception as e:
#             logger.error(f"Error extracting prediction value: {e}")
#             return 0.0
# 
# @dataclass
# class SemioticCacheEntry:
#     """Container for cached semiotic context and model output."""
#     context: SemanticContext
#     model_output: SemioticModelOutput
#     creation_time: float = field(default_factory=time.time)
# 
# # =============================================================================
# # Core Semiotic Processing Classes
# # =============================================================================
# class AbstractSemioticModel(ABC):
#     """
#     Abstract base class for semiotic models.
# 
#     Defines the basic interface for semiotic models used to analyze and
#     process semantic information.
#     """
#     def __init__(self):
#         """Initialize the semiotic model."""
#         self.cache = {}
#         self.cache_limit = 100
#         self.cache_lifespan = 60  # in seconds
#         self.logger = logger.getChild(self.__class__.__name__)  # Set up logger for each class
#         self.device = DEVICE
# 
#     def _generate_key(self, context: SemanticContext) -> str:
#         """
#         Generate a unique key for the given semantic context.
# 
#         Uses a hash of the description and agent ID to create a key.
# 
#         Args:
#             context: SemanticContext object
# 
#         Returns:
#             A string representing the unique cache key
#         """
#         try:
#             key_string = f"{context.description}-{context.agent_id}"
#             return hashlib.sha256(key_string.encode()).hexdigest()
#         except Exception as e:
#            self.logger.error(f"Error generating cache key: {e}")
#            return "default_key" # Default key to prevent further issues
# 
#     def _cache_cleanup(self):
#         """
#         Clean up the cache by removing expired entries.
# 
#         Iterates through the cache and removes any entry whose lifespan has
#         exceeded the defined limit.
#         """
#         now = time.time()
#         keys_to_remove = [
#             key for key, entry in self.cache.items()
#             if (now - entry.creation_time) > self.cache_lifespan
#         ]
#         for key in keys_to_remove:
#             del self.cache[key]
# 
#         # Trim cache if it exceeds the limit
#         if len(self.cache) > self.cache_limit:
#             sorted_cache = sorted(self.cache.items(), key=lambda item: item[1].creation_time)
#             keys_to_trim = [key for key, _ in sorted_cache[:len(self.cache) - self.cache_limit]]
#             for key in keys_to_trim:
#                  del self.cache[key]
# 
#     def cache_context(self, context: SemanticContext, model_output: SemioticModelOutput):
#         """Cache the semantic context and model output."""
#         try:
#             key = self._generate_key(context)
#             self.cache[key] = SemioticCacheEntry(context=context.copy(), model_output=model_output.to(self.device))
#             self._cache_cleanup()
#         except Exception as e:
#             self.logger.error(f"Error caching context: {e}")
# 
#     def get_cached_context(self, context: SemanticContext) -> Optional[SemioticCacheEntry]:
#         """
#         Retrieve the cached context if available.
# 
#         Args:
#             context: SemanticContext object
# 
#         Returns:
#             Cached SemioticCacheEntry or None if not found
#         """
#         try:
#             key = self._generate_key(context)
#             if key in self.cache:
#                 return self.cache[key]
#             return None
#         except Exception as e:
#             self.logger.error(f"Error getting cached context: {e}")
#             return None
# 
#     @abstractmethod
#     def process_context(self, context: SemanticContext) -> SemioticModelOutput:
#          """
#          Abstract method to process a semantic context.
# 
#          Args:
#              context: Semantic context object
# 
#          Returns:
#              Semiotic model output
#          """
#          pass
# 
#     # =============================================================================
# # Concrete Semiotic Model Implementations
# # =============================================================================
# class LegacySemioticModel(nn.Module):
#     """
#     Legacy semiotic model using a simple feed-forward network.
# 
#     This model processes the semantic context data by converting it to numerical
#     vectors, passing it through a linear layer, and producing a prediction.
# 
#     This is intended as a simplified baseline to compare against transformer based models.
#     """
#     def __init__(self, input_dim: int = 20, hidden_dim: int = 30):
#         """
#         Initialize the LegacySemioticModel.
# 
#         Args:
#             input_dim: Dimensionality of the input vector.
#             hidden_dim: Dimensionality of the hidden layer.
#         """
#         super(LegacySemioticModel, self).__init__()
#         self.logger = logger.getChild(self.__class__.__name__)  # Set up logger for each class
#         self.device = DEVICE
# 
#         self.fc1 = nn.Linear(input_dim, hidden_dim)
#         self.relu = nn.ReLU()
#         self.fc2 = nn.Linear(hidden_dim, 1) # Single output for relevance score
#         self.value_fc = nn.Linear(hidden_dim,1)  # Linear layer for value estimation
#         self.sigmoid = nn.Sigmoid()  # Use Sigmoid for relevance between 0 and 1
#         self.softmax = nn.Softmax(dim=-1)  # For attention weights
#         self.dropout = nn.Dropout(0.1)  # Optional dropout for regularization
#         self.hidden_dim = hidden_dim # Store for output
#         self.to(self.device)  # Move model to correct device
#         self.logger.info(f"Legacy Semiotic Model initialized on device: {self.device}")
# 
# 
#     def _process_input(self, context: SemanticContext) -> torch.Tensor:
#         """
#         Process the semantic context into a numerical input vector.
# 
#         This method combines different aspects of the context:
#           * Quantum state metrics (phase, coherence, entropy)
#           * Total surplus
#           * Transformer output prediction
# 
#         Args:
#             context: SemanticContext object
# 
#         Returns:
#             A PyTorch tensor representing the processed input vector, or an empty tensor if an error occurs.
#         """
#         try:
#              # Get relevant metrics
#              quantum_metrics = context.quantum_state_metrics
#              total_surplus = context.surplus_state.total_surplus()
#              transformer_prediction = context.transformer_output.get_prediction_value()
# 
#              # Convert metrics to a numpy array
#              input_array = np.array([
#                 quantum_metrics.get('phase', 0.0),
#                 quantum_metrics.get('phase_coherence', MINIMUM_COHERENCE_FLOOR),
#                 quantum_metrics.get('normalized_entropy', 0.0),
#                 total_surplus,
#                 transformer_prediction
#              ])
# 
#              # Add padding with zeros if less than input_dim
#              if len(input_array) < self.hidden_dim:
#                  padding = np.zeros(self.hidden_dim - len(input_array))
#                  input_array = np.concatenate((input_array, padding))
# 
#              # Convert to tensor
#              input_tensor = torch.tensor(input_array, dtype=torch.float32).unsqueeze(0).to(self.device)  # Add batch dimension
# 
#              return input_tensor
#         except Exception as e:
#              self.logger.error(f"Error processing input for legacy model: {e}")
#              return torch.empty(0).to(self.device)
# 
# 
#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:
#         """
#         Forward pass of the LegacySemioticModel.
# 
#         Args:
#             x: Input tensor.
# 
#         Returns:
#             Tuple of:
#             - Prediction tensor
#             - Value Estimate tensor
#             - Attention weights dictionary
#         """
#         try:
#            # Pass through fully connected layers
#            x = self.fc1(x)
#            x = self.relu(x)
#            x = self.dropout(x)  # Apply dropout
#            prediction = self.sigmoid(self.fc2(x)) # Use Sigmoid for prediction
#            value_estimate = self.value_fc(x) # Output for value estimate
# 
#            # Create dummy attention weights (for compatibility with other models)
#            attention_weights = {
#                "dummy_attention": torch.ones(x.shape[0],self.hidden_dim).to(self.device)
#            }
# 
#            return prediction, value_estimate, attention_weights
#         except Exception as e:
#             self.logger.error(f"Error in forward pass of legacy model: {e}")
#             # Return default
#             return torch.tensor(0.0).to(self.device), torch.tensor(0.0).to(self.device), {}
# 
# 
# class EnhancedSemioticModel(AbstractSemioticModel):
#     """
#     Enhanced semiotic model using a transformer-based network.
# 
#     This model processes the semantic context using a transformer encoder,
#     allowing for more nuanced and contextual understanding of semantic information.
#     """
#     def __init__(self, input_dim: int = 20,
#                  hidden_dim: int = HIDDEN_DIM,
#                  num_heads: int = NUM_TRANSFORMER_HEADS,
#                  num_layers: int = NUM_TRANSFORMER_LAYERS):
#         """
#         Initialize the EnhancedSemioticModel.
# 
#         Args:
#             input_dim: Dimensionality of the input vector.
#             hidden_dim: Dimensionality of the transformer model's hidden layers.
#             num_heads: Number of attention heads in the transformer model.
#             num_layers: Number of transformer encoder layers.
#         """
#         super().__init__()
#         self.logger = logger.getChild(self.__class__.__name__)  # Set up logger for each class
#         self.device = DEVICE
# 
#         self.input_dim = input_dim
#         self.hidden_dim = hidden_dim
#         self.num_heads = num_heads
#         self.num_layers = num_layers
#         self.dropout = nn.Dropout(0.1)
#         self.transformer_encoder = nn.TransformerEncoder(
#             nn.TransformerEncoderLayer(d_model=hidden_dim, nhead=num_heads, dropout=0.1, batch_first=True),
#             num_layers=num_layers
#         )
#         self.fc_prediction = nn.Linear(hidden_dim, 1) # single output for relevance score
#         self.fc_value = nn.Linear(hidden_dim, 1)  # Linear layer for value estimation
#         self.sigmoid = nn.Sigmoid()  # For relevance score output between 0 and 1
#         self.softmax = nn.Softmax(dim=-1)  # For attention weights
#         self.to(self.device)
#         self.logger.info(f"Enhanced Semiotic Model initialized on device: {self.device}")
# 
#     def _process_input(self, context: SemanticContext) -> torch.Tensor:
#         """
#         Process semantic context into a numerical input vector for transformer.
# 
#         Combines quantum metrics (phase, coherence, entropy), total surplus, and
#         transformer output, padding to match model's hidden dimension.
# 
#         Args:
#             context: SemanticContext object
# 
#         Returns:
#             A PyTorch tensor representing the processed input vector, or an empty tensor on error
#         """
#         try:
#              # Get relevant metrics
#              quantum_metrics = context.quantum_state_metrics
#              total_surplus = context.surplus_state.total_surplus()
#              transformer_prediction = context.transformer_output.get_prediction_value()
# 
#              # Convert metrics to a numpy array
#              input_array = np.array([
#                 quantum_metrics.get('phase', 0.0),
#                 quantum_metrics.get('phase_coherence', MINIMUM_COHERENCE_FLOOR),
#                 quantum_metrics.get('normalized_entropy', 0.0),
#                 total_surplus,
#                 transformer_prediction
#              ])
#              # Add padding with zeros if less than hidden_dim
#              if len(input_array) < self.hidden_dim:
#                  padding = np.zeros(self.hidden_dim - len(input_array))
#                  input_array = np.concatenate((input_array, padding))
# 
#              # Convert to tensor and add batch and sequence dimensions
#              input_tensor = torch.tensor(input_array, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(self.device) # B, S, F
#              return input_tensor
#         except Exception as e:
#              self.logger.error(f"Error processing input for enhanced model: {e}")
#              return torch.empty(0).to(self.device)
# 
#     def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor, Dict[str, torch.Tensor]]:
#         """
#         Forward pass of the EnhancedSemioticModel.
# 
#         Args:
#             x: Input tensor.
# 
#         Returns:
#             Tuple of:
#             - Prediction tensor
#             - Value Estimate tensor
#             - Attention weights dictionary
#         """
#         try:
#            # Pass through transformer encoder
#            encoded_output = self.transformer_encoder(x) # B, S, F
#            encoded_output = self.dropout(encoded_output) #Apply dropout
#            # Get the output from the last sequence element
#            last_seq_output = encoded_output[:, -1, :]
# 
#            # Pass the last element through fully connected layer
#            prediction = self.sigmoid(self.fc_prediction(last_seq_output)) # Use Sigmoid for prediction
#            value_estimate = self.fc_value(last_seq_output)  #Output for value estimate
# 
#            # Extract attention weights (average across heads for all layers)
#            attention_weights = {}
#            for layer_idx, layer in enumerate(self.transformer_encoder.layers):
#                attn_weights = layer.self_attn.attn_output_weights
#                if attn_weights is not None:
#                    avg_attn_weights = torch.mean(attn_weights, dim=1)  # Average across heads
#                    attention_weights[f'layer_{layer_idx}_attention'] = avg_attn_weights
# 
#            return prediction, value_estimate, attention_weights
#         except Exception as e:
#            self.logger.error(f"Error in forward pass of enhanced model: {e}")
#            return torch.tensor(0.0).to(self.device), torch.tensor(0.0).to(self.device), {}
# 
#     def process_context(self, context: SemanticContext) -> SemioticModelOutput:
#         """
#         Process the semantic context using the enhanced transformer model.
# 
#         This includes caching, preprocessing the input, and making predictions.
# 
#         Args:
#             context: SemanticContext object
# 
#         Returns:
#             SemioticModelOutput object with prediction and attention weights, or default output on error
#         """
#         try:
#             cached_output = self.get_cached_context(context)
#             if cached_output:
#                 self.logger.debug("Retrieved context from cache")
#                 return cached_output.model_output.to(self.device)
# 
#             input_tensor = self._process_input(context)
#             if input_tensor.numel() == 0:  # Check if tensor is empty
#                 self.logger.warning("Empty input tensor, returning default semiotic output")
#                 return SemioticModelOutput(prediction=torch.tensor(0.0).to(self.device), value_estimate=torch.tensor(0.0).to(self.device), relevance_score=0.0, attention_weights={})
# 
#             prediction, value_estimate, attention_weights = self.forward(input_tensor)
#             relevance_score = prediction.item() # Extract relevance score
# 
#             output = SemioticModelOutput(
#                 prediction=prediction.to(self.device),
#                 value_estimate=value_estimate.to(self.device),
#                 relevance_score=float(relevance_score),
#                 attention_weights=attention_weights
#             )
#             self.cache_context(context, output)
#             return output.to(self.device)
# 
#         except Exception as e:
#             self.logger.error(f"Error in processing context with enhanced model: {e}")
#             return SemioticModelOutput(prediction=torch.tensor(0.0).to(self.device), value_estimate=torch.tensor(0.0).to(self.device), relevance_score=0.0, attention_weights={})
# # =============================================================================
# # Functions to orchestrate semantic processing and output
# # =============================================================================
# class SemanticEnhancedOutput:
#     """
#     Enhanced symbolic output generator that integrates with the semantic ML model
#     to dynamically adapt and enrich Émile's symbolic expressions.
# 
#     This creates a bi-directional learning loop where:
#     1. The semantic model learns from Émile's expressions
#     2. Émile's expressions adapt based on semantic model feedback
# 
#     Enhancements:
#     - Improved handling of numeric values in semantic expression generation
#     - Numeric-semantic coupling for more informative symbolic outputs
#     - Enhanced dimensionality representation with quantitative descriptors
#     """
#     def __init__(self, model_type: str = "enhanced"):
#         """
#         Initialize the SemanticEnhancedOutput module.
# 
#         Args:
#              model_type: Type of semiotic model to use ('legacy' or 'enhanced')
#         """
#         self.logger = logger.getChild(self.__class__.__name__)  # Set up logger for each class
#         self.device = DEVICE
#         self.model_type = model_type
#         self.semiotic_model = self._initialize_semiotic_model()
#         self.output_buffer = deque(maxlen=50)
#         self.last_processed_context = None
#         self.logger.info(f"Semantic Enhanced Output Initialized with {model_type} model.")
# 
#         # Integrations with semantic model
#         self.semantic_model_path = semantic_model_path
#         self.device = device
#         self.semantic_embeddings = {}
#         self.semantic_coherence_threshold = 0.6
#         self.semantic_update_frequency = 10  # Update vocabulary after every N expressions
#         self.expression_counter = 0
#         self.dynamic_vocabulary_enabled = True
# 
#         # Maintain original basic vocabulary
#         self.state_descriptors = [
#             "Flux", "Equilibrium", "Distinction", "Recursion",
#             "Convergence", "Divergence", "Resonance", "Coherence",
#             "Entanglement", "Superposition", "Bifurcation", "Integration"
#         ]
# 
#         self.relations = [
#             "aligns with", "dissolves across", "bends toward",
#             "extends beyond", "contracts into", "resonates within",
#             "differentiates from", "converges upon", "enfolds",
#             "stabilizes within", "emerges through", "transcends"
#         ]
# 
#         self.surplus_concepts = [
#             "stability", "recursion", "entropy", "phase shift",
#             "emergence", "ontology", "distinction", "coherence",
#             "complexity", "dimensionality", "feedback", "symmetry"
#         ]
# 
#         # Additional vocabularies for more complex expressions
#         self.modifiers = [
#             "partially", "deeply", "recursively", "gradually",
#             "suddenly", "coherently", "chaotically", "uniquely",
#             "systematically", "emergently", "distinctively", "subtly"
#         ]
# 
#         self.secondary_concepts = [
#             "phase space", "attractor", "strange loop", "dynamic pattern",
#             "boundary condition", "information field", "critical point", "nonlinearity",
#             "fractal domain", "resonant structure", "emergent property", "computational ontology"
#         ]
# 
#         # Add numeric modifiers to enhance quantitative descriptions
#         self.numeric_modifiers = [
#             "increasing", "decreasing", "oscillating", "accelerating",
#             "decelerating", "threshold", "critical", "harmonic",
#             "resonant", "quantized", "continuous", "discrete"
#         ]
# 
#         # Add numeric transformations for embedding numeric values in expressions
#         self.numeric_transformations = [
#             "multiplied", "divided", "amplified", "attenuated",
#             "exponential", "logarithmic", "scaled", "normalized",
#             "bounded", "unbounded", "fractional", "integral"
#         ]
# 
#         # For dynamic vocabulary expansion
#         self.dynamic_state_descriptors = self.state_descriptors.copy()
#         self.dynamic_relations = self.relations.copy()
#         self.dynamic_surplus_concepts = self.surplus_concepts.copy()
#         self.dynamic_modifiers = self.modifiers.copy()
#         self.dynamic_secondary_concepts = self.secondary_concepts.copy()
#         self.dynamic_numeric_modifiers = self.numeric_modifiers.copy()
#         self.dynamic_numeric_transformations = self.numeric_transformations.copy()
# 
#         # Semantic coherence tracking for each vocabulary term
#         self.vocabulary_coherence = {
#             'descriptors': {term: 0.5 for term in self.state_descriptors},
#             'relations': {term: 0.5 for term in self.relations},
#             'concepts': {term: 0.5 for term in self.surplus_concepts},
#             'modifiers': {term: 0.5 for term in self.modifiers},
#             'secondary': {term: 0.5 for term in self.secondary_concepts},
#             'numeric_modifiers': {term: 0.5 for term in self.numeric_modifiers},
#             'numeric_transformations': {term: 0.5 for term in self.numeric_transformations}
#         }
# 
#         # Track historical expressions
#         self.expression_history = []
#         self.emergence_events = []
#         self.pattern_history = deque(maxlen=100)
#         self.frequency_analysis = {}
# 
#         # Store last expression components for external access
#         self.last_expression_components = {}
# 
#         # Thresholds for state categorization
#         self.coherence_thresholds = {
#             'low': 0.3,
#             'medium': 0.6,
#             'high': 0.8
#         }
#         self.distinction_thresholds = {
#             'low': 0.3,
#             'medium': 0.6,
#             'high': 0.8
#         }
#         self.surplus_thresholds = {
#             'low': 1.0,
#             'medium': 3.0,
#             'high': 6.0
#         }
# 
#         # Advanced pattern recognition
#         self.transition_matrix = np.zeros((vocabulary_size, vocabulary_size))
#         self.transition_counts = np.zeros((vocabulary_size, vocabulary_size))
#         self.descriptor_index = {desc: i for i, desc in enumerate(self.state_descriptors)}
# 
#         # Numeric integration parameters
#         self.numeric_influence = 0.4  # Weight of numeric values in expression generation
#         self.numeric_coherence_threshold = 0.3  # Threshold for numeric coherence
#         self.numeric_memory = deque(maxlen=50)  # Remember recent numeric values
#         self.numeric_trends = {}  # Track trends in numeric values
# 
#         # Tracks the last generated elements for transition analysis
#         self.last_descriptor = None
#         self.expression_complexity = 1.0
# 
#         # Initialize timestamp for real-time tracking
#         self.start_time = time.time()
# 
#         # Load semantic model if path provided
#         if semantic_model_path and os.path.exists('/content/emile_semantic_ml_mini.pt'):
#             self.load_semantic_model()
# 
#         # Initialize semantic cache directory
#         self.semantic_cache_dir = os.path.join(os.path.dirname(os.path.abspath(__file__)), "semantic_cache")
#         os.makedirs(self.semantic_cache_dir, exist_ok=True)
# 
#         # Try to load cached semantic embeddings
#         self.load_semantic_cache()
# 
#         # Start background thread for adaptive vocabulary updates
#         self.semantic_update_thread = threading.Thread(target=self._background_vocabulary_update, daemon=True)
#         self.semantic_update_thread.start()
# 
#     def _initialize_semiotic_model(self) -> AbstractSemioticModel:
#         """Initialize the semiotic model based on the specified type."""
#         try:
#             if self.model_type == 'legacy':
#                 self.logger.info("Initializing Legacy Semiotic Model.")
#                 return LegacySemioticModel()
#             elif self.model_type == 'enhanced':
#                 self.logger.info("Initializing Enhanced Semiotic Model.")
#                 return EnhancedSemioticModel()
#             else:
#                 self.logger.warning(f"Invalid model type: {self.model_type}, defaulting to enhanced model")
#                 return EnhancedSemioticModel()
#         except Exception as e:
#             self.logger.error(f"Error initializing semiotic model: {e}")
#             return EnhancedSemioticModel
# 
#     def load_semantic_model(self):
#         """
#         Loads the semantic model for integration with symbolic expression generation.
#         This establishes the connection to the ML model with improved model architecture handling.
#         """
#         try:
#             # Import necessary modules here to avoid import errors if not available
#             from transformers import AutoTokenizer, AutoModel
#             from sentence_transformers import SentenceTransformer
#             import traceback
# 
#             print(f"Loading semantic model from {self.semantic_model_path}")
# 
#             # Load sentence transformer for concept embeddings
#             self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(self.device)
#             self.sentence_model.eval()
# 
#             # Load the semantic model checkpoint
#             checkpoint = torch.load(self.semantic_model_path, map_location=self.device)
# 
#             print("Detected model architecture mismatch, remapping keys...")
# 
#             # Create our custom model that matches the checkpoint architecture
#             self.semantic_model = LegacySemioticModel(hidden_dim=384).to(self.device)
# 
#             # Try to load transformer model
#             try:
#                 transformer_model = AutoModel.from_pretrained("bert-base-uncased").to(self.device)
#                 transformer_model.eval()
#                 self.semantic_model.encoder = transformer_model
#             except Exception as e:
#                 print(f"Error loading transformer model: {e}")
# 
#             # Prepare state dict from checkpoint
#             state_dict = None
#             if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
#                 state_dict = checkpoint['model_state_dict']
#             else:
#                 # Try direct state_dict loading
#                 state_dict = checkpoint
# 
#             # Try loading with flexible options
#             try:
#                 # First, try strict loading
#                 self.semantic_model.load_state_dict(state_dict)
#             except Exception as strict_error:
#                 print(f"Warning: Strict loading failed, attempting with strict=False: {strict_error}")
#                 try:
#                     # Try non-strict loading to allow missing or unexpected keys
#                     self.semantic_model.load_state_dict(state_dict, strict=False)
#                 except Exception as nonstrict_error:
#                     print(f"Error loading semantic model: {nonstrict_error}")
#                     traceback.print_exc()
#                     self.semantic_model = None
#                     return False
# 
#             self.semantic_model.eval()
# 
#             # Load tokenizer
#             try:
#                 self.tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")
#             except Exception as e:
#                 print(f"Error loading tokenizer: {e}")
#                 # Create a simple tokenizer as fallback
#                 class SimpleTokenizer:
#                     def __call__(self, text, return_tensors="pt", padding=True, truncation=True, max_length=512):
#                         return {
#                             'input_ids': torch.tensor([[1, 2, 3]]).to(self.device),
#                             'attention_mask': torch.tensor([[1, 1, 1]]).to(self.device)
#                         }
#                 self.tokenizer = SimpleTokenizer()
# 
#             print("✅ Semantic model loaded successfully")
# 
#             # Pre-compute embeddings for vocabulary terms
#             self._precompute_vocabulary_embeddings()
# 
#             return True
#         except Exception as e:
#             print(f"Error loading semantic model: {e}")
#             traceback.print_exc()
#             print("⚠️ Semantic integration will be disabled")
#             self.semantic_model = None
#             return False
# 
#     def _precompute_vocabulary_embeddings(self):
#         """
#         Precompute embeddings for all vocabulary terms for faster semantic matching
#         """
#         if not self.semantic_model:
#             return
# 
#         try:
#             print("Precomputing semantic embeddings for vocabulary...")
# 
#             # Combine all vocabulary terms
#             all_terms = (
#                 self.state_descriptors +
#                 self.relations +
#                 self.surplus_concepts +
#                 self.modifiers +
#                 self.secondary_concepts +
#                 self.numeric_modifiers +
#                 self.numeric_transformations
#             )
# 
#             # Compute embeddings in batches
#             batch_size = 16
#             for i in range(0, len(all_terms), batch_size):
#                 batch = all_terms[i:min(i+batch_size, len(all_terms))]
# 
#                 # Get embeddings from sentence model
#                 with torch.no_grad():
#                     embeddings = self.sentence_model.encode(batch, convert_to_tensor=True)
# 
#                 # Store embeddings in dictionary
#                 for j, term in enumerate(batch):
#                     self.semantic_embeddings[term] = embeddings[j].cpu()
# 
#             print(f"✅ Computed embeddings for {len(self.semantic_embeddings)} vocabulary terms")
# 
#             # Save embeddings to cache
#             self.save_semantic_cache()
# 
#         except Exception as e:
#             print(f"Error precomputing vocabulary embeddings: {e}")
# 
#     def save_semantic_cache(self):
#         """Save semantic embeddings to cache file"""
#         try:
#             cache_file = os.path.join(self.semantic_cache_dir, "vocab_embeddings.pt")
# 
#             # Convert embeddings to CPU tensors
#             cpu_embeddings = {k: v.cpu() for k, v in self.semantic_embeddings.items()}
# 
#             # Save to file
#             torch.save(cpu_embeddings, cache_file)
#             print(f"Saved semantic embeddings cache to {cache_file}")
# 
#             # Also save vocabulary coherence
#             coherence_file = os.path.join(self.semantic_cache_dir, "vocab_coherence.json")
#             with open(coherence_file, 'w') as f:
#                 json.dump(self.vocabulary_coherence, f)
# 
#         except Exception as e:
#             print(f"Error saving semantic cache: {e}")
# 
#     def load_semantic_cache(self):
#         """Load semantic embeddings from cache file"""
#         try:
#             cache_file = os.path.join(self.semantic_cache_dir, "vocab_embeddings.pt")
#             if os.path.exists(cache_file):
#                 self.semantic_embeddings = torch.load(cache_file)
#                 print(f"Loaded semantic embeddings for {len(self.semantic_embeddings)} terms from cache")
# 
#             # Load vocabulary coherence
#             coherence_file = os.path.join(self.semantic_cache_dir, "vocab_coherence.json")
#             if os.path.exists(coherence_file):
#                 with open(coherence_file, 'r') as f:
#                     self.vocabulary_coherence = json.load(f)
#                 print("Loaded vocabulary coherence from cache")
# 
#             return True
#         except Exception as e:
#             print(f"Error loading semantic cache: {e}")
#             return False
# 
#     def _calculate_semantic_coherence(self, expression):
#         """
#         Calculate the semantic coherence of an expression using the semantic model
# 
#         Args:
#             expression: The symbolic expression to evaluate
# 
#         Returns:
#             Coherence score between 0-1
#         """
#         if not self.semantic_model:
#             return 0.5  # Default if no model
# 
#         try:
#             # Tokenize the expression
#             inputs = self.tokenizer(expression, return_tensors="pt", padding=True,
#                                   truncation=True, max_length=512).to(self.device)
# 
#             # Get expression embedding from model
#             with torch.no_grad():
#                 # First encode with base transformer
#                 base_outputs = self.transformer_base(**inputs)
#                 pooled_output = base_outputs.last_hidden_state[:, 0, :]
#                 # Then use our semantic model
#                 expression_embedding = self.semantic_model(pooled_output)
# 
#             # Get reference embeddings
#             if self.expression_history:
#                 # Use historical expressions as reference
#                 reference_texts = [entry['expression'] for entry in self.expression_history[-10:]]
# 
#                 with torch.no_grad():
#                     reference_embeddings = self.sentence_model.encode(reference_texts, convert_to_tensor=True).to(self.device)
# 
#                 # Calculate average cosine similarity
#                 cos_sims = torch.nn.functional.cosine_similarity(
#                     expression_embedding.unsqueeze(0), reference_embeddings
#                 )
# 
#                 # Return maximum similarity as coherence score
#                 return torch.max(cos_sims).item()
#             else:
#                 # If no history, return 0.5 as default
#                 return 0.5
# 
#         except Exception as e:
#             print(f"Error calculating semantic coherence: {e}")
#             return 0.5
# 
#     def _update_vocabulary_coherence(self, components):
#         """
#         Update coherence scores for vocabulary terms based on semantic model feedback
# 
#         Args:
#             components: Dictionary of components used in the expression
#         """
#         if not self.semantic_model:
#             return
# 
#         try:
#             # Get components
#             descriptor = components.get('descriptor')
#             relation = components.get('relation')
#             concept = components.get('concept')
#             modifier = components.get('modifier')
#             secondary = components.get('secondary')
#             numeric_modifier = components.get('numeric_modifier')
#             numeric_transformation = components.get('numeric_transformation')
# 
#             # Get full expression
#             expression = components.get('full_expression', '')
# 
#             # Calculate coherence for the full expression
#             expression_coherence = self._calculate_semantic_coherence(expression)
# 
#             # Update coherence for each component
#             alpha = 0.2  # Learning rate for updates
# 
#             if descriptor in self.vocabulary_coherence['descriptors']:
#                 current = self.vocabulary_coherence['descriptors'][descriptor]
#                 self.vocabulary_coherence['descriptors'][descriptor] = (1-alpha) * current + alpha * expression_coherence
# 
#             if relation in self.vocabulary_coherence['relations']:
#                 current = self.vocabulary_coherence['relations'][relation]
#                 self.vocabulary_coherence['relations'][relation] = (1-alpha) * current + alpha * expression_coherence
# 
#             if concept in self.vocabulary_coherence['concepts']:
#                 current = self.vocabulary_coherence['concepts'][concept]
#                 self.vocabulary_coherence['concepts'][concept] = (1-alpha) * current + alpha * expression_coherence
# 
#             if modifier and modifier in self.vocabulary_coherence['modifiers']:
#                 current = self.vocabulary_coherence['modifiers'][modifier]
#                 self.vocabulary_coherence['modifiers'][modifier] = (1-alpha) * current + alpha * expression_coherence
# 
#             if secondary and secondary in self.vocabulary_coherence['secondary']:
#                 current = self.vocabulary_coherence['secondary'][secondary]
#                 self.vocabulary_coherence['secondary'][secondary] = (1-alpha) * current + alpha * expression_coherence
# 
#             # Update coherence for numeric components
#             if numeric_modifier and numeric_modifier in self.vocabulary_coherence['numeric_modifiers']:
#                 current = self.vocabulary_coherence['numeric_modifiers'][numeric_modifier]
#                 self.vocabulary_coherence['numeric_modifiers'][numeric_modifier] = (1-alpha) * current + alpha * expression_coherence
# 
#             if numeric_transformation and numeric_transformation in self.vocabulary_coherence['numeric_transformations']:
#                 current = self.vocabulary_coherence['numeric_transformations'][numeric_transformation]
#                 self.vocabulary_coherence['numeric_transformations'][numeric_transformation] = (1-alpha) * current + alpha * expression_coherence
# 
#         except Exception as e:
#             print(f"Error updating vocabulary coherence: {e}")
# 
#     def _semantic_find_related_terms(self, query, vocabulary_list, top_n=3):
#         """
#         Find semantically related terms from vocabulary using the semantic model
# 
#         Args:
#             query: Query term to find related terms for
#             vocabulary_list: List of vocabulary terms to search
#             top_n: Number of top terms to return
# 
#         Returns:
#             List of related terms
#         """
#         if not self.semantic_model or not self.semantic_embeddings:
#             return random.sample(vocabulary_list, min(top_n, len(vocabulary_list)))
# 
#         try:
#             # Get query embedding
#             with torch.no_grad():
#                 query_embedding = self.sentence_model.encode([query], convert_to_tensor=True)[0]
# 
#             # Calculate similarity with vocabulary terms
#             similarities = []
#             for term in vocabulary_list:
#                 if term in self.semantic_embeddings:
#                     term_embedding = self.semantic_embeddings[term]
#                     similarity = torch.nn.functional.cosine_similarity(
#                         query_embedding.unsqueeze(0),
#                         term_embedding.unsqueeze(0)
#                     ).item()
#                     similarities.append((term, similarity))
# 
#             # Sort by similarity and return top_n
#             similarities.sort(key=lambda x: x[1], reverse=True)
#             return [term for term, _ in similarities[:top_n]]
# 
#         except Exception as e:
#             print(f"Error finding related terms: {e}")
#             return random.sample(vocabulary_list, min(top_n, len(vocabulary_list)))
# 
#     def _expand_vocabulary_from_semantic_model(self, category, seed_term):
#         """
#         Use the semantic model to expand vocabulary with related terms
# 
#         Args:
#             category: Vocabulary category to expand ('descriptors', 'relations', etc.)
#             seed_term: Seed term to find related concepts for
#         """
#         if not self.semantic_model:
#             return []
# 
#         try:
#             # Query the model to find related concepts using find_related_concepts function
#             from semantic_trainer import find_related_concepts
#             results = find_related_concepts(seed_term, top_n=5)
# 
#             # Extract candidate terms
#             candidates = []
#             for _, sentence, similarity in results:
#                 # Extract potential terms based on category
#                 if category == 'descriptors':
#                     # Look for capitalized terms likely to be descriptors
#                     words = [w for w in sentence.split() if w[0].isupper() and len(w) > 3]
#                     candidates.extend(words)
#                 elif category == 'relations':
#                     # Look for verb phrases
#                     if ' with ' in sentence or ' within ' in sentence or ' through ' in sentence:
#                         for phrase in [' with ', ' within ', ' through ', ' into ', ' from ', ' upon ']:
#                             if phrase in sentence:
#                                 idx = sentence.find(phrase)
#                                 if idx > 0 and idx + len(phrase) < len(sentence):
#                                     # Get word before the phrase and word after
#                                     before = sentence[:idx].split()[-1]
#                                     after = sentence[idx+len(phrase):].split()[0]
#                                     if before and after:
#                                         candidates.append(f"{before}{phrase}{after}")
#                 elif category == 'numeric_modifiers' or category == 'numeric_transformations':
#                     # Extract terms related to quantities and numeric relationships
#                     numeric_indicators = ['increase', 'decrease', 'multiply', 'divide', 'scale',
#                                          'threshold', 'critical', 'exponential', 'logarithmic',
#                                          'oscillate', 'accelerate', 'decelerate', 'quantize']
# 
#                     # Look for words related to numeric concepts
#                     for word in sentence.split():
#                         if any(indicator in word.lower() for indicator in numeric_indicators) and len(word) > 3:
#                             candidates.append(word)
#                 else:
#                     # For other categories, look for relevant n-grams that aren't too long
#                     words = sentence.split()
#                     for i in range(len(words)-1):
#                         if 3 < len(words[i]) + len(words[i+1]) < 15:
#                             candidates.append(f"{words[i]} {words[i+1]}")
# 
#             # Return unique candidates
#             return list(set(candidates))
# 
#         except Exception as e:
#             print(f"Error expanding vocabulary from semantic model: {e}")
#             return []
# 
#     def _background_vocabulary_update(self):
#         """
#         Background thread for updating vocabulary based on semantic model
#         """
#         while True:
#             try:
#                 # Sleep to prevent excessive CPU usage
#                 time.sleep(60)  # Check every minute
# 
#                 if not self.dynamic_vocabulary_enabled or not self.semantic_model:
#                     continue
# 
#                 # Update vocabulary if we have enough data
#                 if len(self.expression_history) > 10:
#                     # Select random category to expand
#                     category = random.choice(['descriptors', 'relations', 'concepts',
#                                             'modifiers', 'secondary', 'numeric_modifiers',
#                                             'numeric_transformations'])
# 
#                     # Select seed term with high coherence
#                     if category == 'descriptors':
#                         seed_terms = self.dynamic_state_descriptors
#                     elif category == 'relations':
#                         seed_terms = self.dynamic_relations
#                     elif category == 'concepts':
#                         seed_terms = self.dynamic_surplus_concepts
#                     elif category == 'modifiers':
#                         seed_terms = self.dynamic_modifiers
#                     elif category == 'secondary':
#                         seed_terms = self.dynamic_secondary_concepts
#                     elif category == 'numeric_modifiers':
#                         seed_terms = self.dynamic_numeric_modifiers
#                     else:  # numeric_transformations
#                         seed_terms = self.dynamic_numeric_transformations
# 
#                     # Select seed with higher probability for high-coherence terms
#                     seed_term = random.choice(seed_terms)
# 
#                     # Find new candidate terms
#                     new_terms = self._expand_vocabulary_from_semantic_model(category, seed_term)
# 
#                     # Add at most one new term to avoid rapid vocabulary changes
#                     if new_terms:
#                         new_term = random.choice(new_terms)
# 
#                         # Add to appropriate vocabulary with proper coherence tracking
#                         if category == 'descriptors' and new_term not in self.dynamic_state_descriptors:
#                             self.dynamic_state_descriptors.append(new_term)
#                             self.vocabulary_coherence['descriptors'][new_term] = 0.5
#                             print(f"Added new descriptor: {new_term}")
#                         elif category == 'relations' and new_term not in self.dynamic_relations:
#                             self.dynamic_relations.append(new_term)
#                             self.vocabulary_coherence['relations'][new_term] = 0.5
#                             print(f"Added new relation: {new_term}")
#                         elif category == 'concepts' and new_term not in self.dynamic_surplus_concepts:
#                             self.dynamic_surplus_concepts.append(new_term)
#                             self.vocabulary_coherence['concepts'][new_term] = 0.5
#                             print(f"Added new concept: {new_term}")
#                         elif category == 'modifiers' and new_term not in self.dynamic_modifiers:
#                             self.dynamic_modifiers.append(new_term)
#                             self.vocabulary_coherence['modifiers'][new_term] = 0.5
#                             print(f"Added new modifier: {new_term}")
#                         elif category == 'secondary' and new_term not in self.dynamic_secondary_concepts:
#                             self.dynamic_secondary_concepts.append(new_term)
#                             self.vocabulary_coherence['secondary'][new_term] = 0.5
#                             print(f"Added new secondary concept: {new_term}")
#                         elif category == 'numeric_modifiers' and new_term not in self.dynamic_numeric_modifiers:
#                             self.dynamic_numeric_modifiers.append(new_term)
#                             self.vocabulary_coherence['numeric_modifiers'][new_term] = 0.5
#                             print(f"Added new numeric modifier: {new_term}")
#                         elif category == 'numeric_transformations' and new_term not in self.dynamic_numeric_transformations:
#                             self.dynamic_numeric_transformations.append(new_term)
#                             self.vocabulary_coherence['numeric_transformations'][new_term] = 0.5
#                             print(f"Added new numeric transformation: {new_term}")
# 
#                     # Save updated coherence and embeddings
#                     if new_terms:
#                         self._precompute_vocabulary_embeddings()
#                         self.save_semantic_cache()
# 
#             except Exception as e:
#                 print(f"Error in vocabulary update thread: {e}")
#                 time.sleep(300)  # Sleep longer after error
# 
#     def _update_transition_statistics(self, current_descriptor: str):
#         """
#         Update the transition matrix for pattern analysis.
# 
#         Args:
#             current_descriptor: The descriptor used in the current expression
#         """
#         try:
#             if self.last_descriptor is not None and current_descriptor in self.descriptor_index:
#                 prev_idx = self.descriptor_index.get(self.last_descriptor)
#                 curr_idx = self.descriptor_index.get(current_descriptor)
# 
#                 if prev_idx is not None and curr_idx is not None:
#                     # Increment transition count
#                     self.transition_counts[prev_idx, curr_idx] += 1
# 
#                     # Update transition probability
#                     row_sum = np.sum(self.transition_counts[prev_idx, :])
#                     if row_sum > 0:
#                         self.transition_matrix[prev_idx, :] = self.transition_counts[prev_idx, :] / row_sum
# 
#             # Update last descriptor
#             self.last_descriptor = current_descriptor
#         except Exception as e:
#             print(f"Error updating transition statistics: {e}")
# 
#     def _track_numeric_trends(self, metrics: Dict[str, float]):
#         """
#         Track trends in numeric values across expressions.
# 
#         Args:
#             metrics: Dictionary of current metrics
#         """
#         try:
#             # Store the full metrics object
#             self.numeric_memory.append(metrics)
# 
#             # Calculate trends for each numeric value
#             if len(self.numeric_memory) >= 3:
#                 for key in metrics:
#                     if key not in self.numeric_trends:
#                         self.numeric_trends[key] = {
#                             'increasing': False,
#                             'decreasing': False,
#                             'oscillating': False,
#                             'stable': True,
#                             'rate_of_change': 0.0,
#                             'acceleration': 0.0
#                         }
# 
#                     # Get recent values for this metric
#                     recent_values = [entry.get(key, 0.0) for entry in self.numeric_memory if key in entry]
#                     if len(recent_values) >= 3:
#                         # Calculate first and second derivatives
#                         first_diff = np.diff(recent_values[-3:])
#                         rate_of_change = np.mean(first_diff)
# 
#                         # Calculate acceleration if we have enough points
#                         if len(first_diff) >= 2:
#                             acceleration = np.diff(first_diff)[0]
#                         else:
#                             acceleration = 0.0
# 
#                         # Update trend information
#                         self.numeric_trends[key] = {
#                             'increasing': rate_of_change > 0.01,
#                             'decreasing': rate_of_change < -0.01,
#                             'oscillating': (first_diff[0] * first_diff[-1] < 0) if len(first_diff) >= 2 else False,
#                             'stable': abs(rate_of_change) < 0.01,
#                             'rate_of_change': rate_of_change,
#                             'acceleration': acceleration
#                         }
#         except Exception as e:
#             print(f"Error tracking numeric trends: {e}")
# 
#     def _calculate_weights(self,
#                           surplus: float,
#                           distinction: float,
#                           coherence: float,
#                           entropy: Optional[float] = None,
#                           dimensionality: Optional[int] = None,
#                           numeric_metrics: Optional[Dict[str, float]] = None) -> Tuple[List[float], List[float], List[float], List[float], List[float]]:
#         """
#         Calculate vocabulary selection weights based on current metrics,
#         now with enhanced numeric influence.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             entropy: Optional entropy metric
#             dimensionality: Optional detected dimensionality
#             numeric_metrics: Optional additional numeric metrics
# 
#         Returns:
#             Tuple of weights for descriptors, relations, concepts, numeric_modifiers, and numeric_transformations
#         """
#         try:
#             # Choose which vocabulary to use (original or dynamic)
#             descriptors = self.dynamic_state_descriptors if self.dynamic_vocabulary_enabled else self.state_descriptors
#             relations = self.dynamic_relations if self.dynamic_vocabulary_enabled else self.relations
#             concepts = self.dynamic_surplus_concepts if self.dynamic_vocabulary_enabled else self.surplus_concepts
#             numeric_modifiers = self.dynamic_numeric_modifiers if self.dynamic_vocabulary_enabled else self.numeric_modifiers
#             numeric_transformations = self.dynamic_numeric_transformations if self.dynamic_vocabulary_enabled else self.numeric_transformations
# 
#             # Initialize weights
#             descriptor_weights = np.ones(len(descriptors)) / len(descriptors)
#             relation_weights = np.ones(len(relations)) / len(relations)
#             concept_weights = np.ones(len(concepts)) / len(concepts)
#             n_modifier_weights = np.ones(len(numeric_modifiers)) / len(numeric_modifiers)
#             n_transform_weights = np.ones(len(numeric_transformations)) / len(numeric_transformations)
# 
#             # Track numeric trends if metrics provided
#             if numeric_metrics:
#                 self._track_numeric_trends(numeric_metrics)
# 
#             # Apply coherence-based weighting if semantic model is available
#             if self.semantic_model and self.dynamic_vocabulary_enabled:
#                 # Get coherence scores for each vocabulary type
#                 descriptor_coherence = [self.vocabulary_coherence['descriptors'].get(term, 0.5) for term in descriptors]
#                 relation_coherence = [self.vocabulary_coherence['relations'].get(term, 0.5) for term in relations]
#                 concept_coherence = [self.vocabulary_coherence['concepts'].get(term, 0.5) for term in concepts]
#                 n_modifier_coherence = [self.vocabulary_coherence['numeric_modifiers'].get(term, 0.5) for term in numeric_modifiers]
#                 n_transform_coherence = [self.vocabulary_coherence['numeric_transformations'].get(term, 0.5) for term in numeric_transformations]
# 
#                 # Convert to numpy arrays
#                 descriptor_coherence = np.array(descriptor_coherence)
#                 relation_coherence = np.array(relation_coherence)
#                 concept_coherence = np.array(concept_coherence)
#                 n_modifier_coherence = np.array(n_modifier_coherence)
#                 n_transform_coherence = np.array(n_transform_coherence)
# 
#                 # Apply softmax to get weights
#                 def softmax(x, temperature=1.0):
#                     exp_x = np.exp((x - np.mean(x)) / temperature)
#                     return exp_x / np.sum(exp_x)
# 
#                 # Higher temperature (>1.0) makes distribution more uniform
#                 # Lower temperature (<1.0) makes distribution more peaked
#                 temp = 2.0  # Start with more exploration
# 
#                 # Reduce temperature as we gather more data for more focused selection
#                 if len(self.expression_history) > 50:
#                     temp = 1.0
#                 if len(self.expression_history) > 100:
#                     temp = 0.5
# 
#                 # Calculate weights using softmax
#                 descriptor_weights = softmax(descriptor_coherence, temp)
#                 relation_weights = softmax(relation_coherence, temp)
#                 concept_weights = softmax(concept_coherence, temp)
#                 n_modifier_weights = softmax(n_modifier_coherence, temp)
#                 n_transform_weights = softmax(n_transform_coherence, temp)
# 
#             # Apply numeric trends-based adjustments if available
#             if numeric_metrics and self.numeric_trends:
#                 # Find trends that are significant
#                 significant_trends = {}
#                 for key, trend in self.numeric_trends.items():
#                     if trend['increasing'] or trend['decreasing'] or trend['oscillating']:
#                         significant_trends[key] = trend
# 
#                 # Adjust weights for numeric modifiers based on trends
#                 if significant_trends:
#                     for i, modifier in enumerate(numeric_modifiers):
#                         # Boost weight for relevant modifiers
#                         if modifier == "increasing" and any(t['increasing'] for t in significant_trends.values()):
#                             n_modifier_weights[i] *= 2.0
#                         elif modifier == "decreasing" and any(t['decreasing'] for t in significant_trends.values()):
#                             n_modifier_weights[i] *= 2.0
#                         elif modifier == "oscillating" and any(t['oscillating'] for t in significant_trends.values()):
#                             n_modifier_weights[i] *= 2.0
#                         elif modifier == "accelerating" and any(t['acceleration'] > 0.05 for t in significant_trends.values()):
#                             n_modifier_weights[i] *= 2.0
#                         elif modifier == "decelerating" and any(t['acceleration'] < -0.05 for t in significant_trends.values()):
#                             n_modifier_weights[i] *= 2.0
# 
#                     # Normalize numeric modifier weights
#                     if np.sum(n_modifier_weights) > 0:
#                         n_modifier_weights = n_modifier_weights / np.sum(n_modifier_weights)
# 
#             # Apply standard metric-based adjustments
#             if coherence > self.coherence_thresholds['high']:
#                 # High coherence: favor structured, aligned, stabilized expressions
#                 # Apply weights to the indices that exist in both original and dynamic vocabulary
#                 for i, desc in enumerate(descriptors):
#                     if desc in ["Equilibrium", "Convergence", "Coherence", "Integration"]:
#                         descriptor_weights[i] *= 2.0
# 
#                 for i, rel in enumerate(relations):
#                     if rel in ["aligns with", "resonates within", "converges upon", "stabilizes within"]:
#                         relation_weights[i] *= 2.0
# 
#                 for i, concept in enumerate(concepts):
#                     if concept in ["stability", "coherence", "symmetry"]:
#                         concept_weights[i] *= 2.0
# 
#                 # For numeric components, favor stability and harmony
#                 for i, modifier in enumerate(numeric_modifiers):
#                     if modifier in ["harmonic", "resonant", "continuous", "quantized"]:
#                         n_modifier_weights[i] *= 2.0
# 
#                 for i, transform in enumerate(numeric_transformations):
#                     if transform in ["normalized", "bounded", "integral"]:
#                         n_transform_weights[i] *= 2.0
# 
#             elif coherence < self.coherence_thresholds['low']:
#                 # Low coherence: favor flux, entropy, dissolution
#                 for i, desc in enumerate(descriptors):
#                     if desc in ["Flux", "Divergence", "Bifurcation"]:
#                         descriptor_weights[i] *= 2.0
# 
#                 for i, rel in enumerate(relations):
#                     if rel in ["dissolves across", "differentiates from"]:
#                         relation_weights[i] *= 2.0
# 
#                 for i, concept in enumerate(concepts):
#                     if concept in ["entropy", "complexity"]:
#                         concept_weights[i] *= 2.0
# 
#                 # For numeric components, favor instability and change
#                 for i, modifier in enumerate(numeric_modifiers):
#                     if modifier in ["oscillating", "chaotic", "threshold", "critical"]:
#                         n_modifier_weights[i] *= 2.0
# 
#                 for i, transform in enumerate(numeric_transformations):
#                     if transform in ["unbounded", "exponential", "amplified"]:
#                         n_transform_weights[i] *= 2.0
# 
#             # Adjust based on distinction level
#             if distinction > self.distinction_thresholds['high']:
#                 # Favor distinction and emergence concepts
#                 for i, desc in enumerate(descriptors):
#                     if desc in ["Distinction", "Recursion"]:
#                         descriptor_weights[i] *= 2.0
# 
#                 for i, rel in enumerate(relations):
#                     if rel in ["emerges through", "differentiates from"]:
#                         relation_weights[i] *= 2.0
# 
#                 for i, concept in enumerate(concepts):
#                     if concept in ["emergence", "distinction"]:
#                         concept_weights[i] *= 2.0
# 
#                 # For numeric components, favor discrete and quantized
#                 for i, modifier in enumerate(numeric_modifiers):
#                     if modifier in ["discrete", "quantized", "threshold"]:
#                         n_modifier_weights[i] *= 2.0
# 
#             elif distinction < self.distinction_thresholds['low']:
#                 # Favor flux and entropy
#                 for i, desc in enumerate(descriptors):
#                     if desc in ["Flux", "Entanglement"]:
#                         descriptor_weights[i] *= 2.0
# 
#                 for i, rel in enumerate(relations):
#                     if rel in ["dissolves across", "contracts into"]:
#                         relation_weights[i] *= 2.0
# 
#                 for i, concept in enumerate(concepts):
#                     if concept in ["entropy", "complexity"]:
#                         concept_weights[i] *= 2.0
# 
#                 # For numeric components, favor continuous and flowing
#                 for i, modifier in enumerate(numeric_modifiers):
#                     if modifier in ["continuous", "increasing", "decreasing"]:
#                         n_modifier_weights[i] *= 2.0
# 
#             # Adjust for surplus level
#             if surplus > self.surplus_thresholds['high']:
#                 # High surplus: favor differentiation and expansion
#                 for i, rel in enumerate(relations):
#                     if rel in ["differentiates from", "extends beyond", "transcends"]:
#                         relation_weights[i] *= 2.0
# 
#                 for i, concept in enumerate(concepts):
#                     if concept in ["distinction", "recursion", "dimensionality"]:
#                         concept_weights[i] *= 2.0
# 
#                 # For numeric components, favor amplification and exponential
#                 for i, transform in enumerate(numeric_transformations):
#                     if transform in ["amplified", "exponential", "multiplied"]:
#                         n_transform_weights[i] *= 2.0
# 
#             elif surplus < self.surplus_thresholds['low']:
#                 # Low surplus: favor contraction and stability
#                 for i, rel in enumerate(relations):
#                     if rel in ["contracts into", "stabilizes within"]:
#                         relation_weights[i] *= 2.0
# 
#                 for i, concept in enumerate(concepts):
#                     if concept in ["stability", "feedback"]:
#                         concept_weights[i] *= 2.0
# 
#                 # For numeric components, favor attenuation and stability
#                 for i, transform in enumerate(numeric_transformations):
#                     if transform in ["attenuated", "divided", "normalized"]:
#                         n_transform_weights[i] *= 2.0
# 
#             # Adjust for entropy if provided
#             if entropy is not None:
#                 if entropy > 0.7:  # High entropy
#                     for i, desc in enumerate(descriptors):
#                         if desc in ["Flux", "Divergence"]:
#                             descriptor_weights[i] *= 2.0
# 
#                     for i, concept in enumerate(concepts):
#                         if concept in ["entropy", "complexity"]:
#                             concept_weights[i] *= 2.0
# 
#                     # For numeric components, favor chaos and randomness
#                     for i, modifier in enumerate(numeric_modifiers):
#                         if modifier in ["chaotic", "oscillating"]:
#                             n_modifier_weights[i] *= 2.0
# 
#                 elif entropy < 0.3:  # Low entropy
#                     for i, desc in enumerate(descriptors):
#                         if desc in ["Equilibrium", "Coherence"]:
#                             descriptor_weights[i] *= 2.0
# 
#                     for i, concept in enumerate(concepts):
#                         if concept in ["stability", "symmetry"]:
#                             concept_weights[i] *= 2.0
# 
#                     # For numeric components, favor order and stability
#                     for i, modifier in enumerate(numeric_modifiers):
#                         if modifier in ["harmonic", "resonant"]:
#                             n_modifier_weights[i] *= 2.0
# 
#             # Adjust for dimensionality if provided
#             if dimensionality is not None:
#                 if dimensionality > 3:  # Higher dimensions
#                     for i, desc in enumerate(descriptors):
#                         if desc in ["Integration", "Recursion"]:
#                             descriptor_weights[i] *= 2.0
# 
#                     for i, rel in enumerate(relations):
#                         if rel in ["transcends", "emerges through"]:
#                             relation_weights[i] *= 2.0
# 
#                     for i, concept in enumerate(concepts):
#                         if concept in ["dimensionality", "emergence"]:
#                             concept_weights[i] *= 2.0
# 
#                     # For numeric transformations, favor higher-order operations
#                     for i, transform in enumerate(numeric_transformations):
#                         if transform in ["exponential", "logarithmic"]:
#                             n_transform_weights[i] *= 2.0
# 
#             # Normalize weights
#             descriptor_weights = descriptor_weights / np.sum(descriptor_weights)
#             relation_weights = relation_weights / np.sum(relation_weights)
#             concept_weights = concept_weights / np.sum(concept_weights)
#             n_modifier_weights = n_modifier_weights / np.sum(n_modifier_weights)
#             n_transform_weights = n_transform_weights / np.sum(n_transform_weights)
# 
#             return descriptor_weights.tolist(), relation_weights.tolist(), concept_weights.tolist(), n_modifier_weights.tolist(), n_transform_weights.tolist()
# 
#         except Exception as e:
#             print(f"Error calculating expression weights: {e}")
#             # Return uniform weights as fallback
#             uniform_desc = [1.0/len(descriptors)] * len(descriptors)
#             uniform_rel = [1.0/len(relations)] * len(relations)
#             uniform_con = [1.0/len(concepts)] * len(concepts)
#             uniform_n_mod = [1.0/len(numeric_modifiers)] * len(numeric_modifiers)
#             uniform_n_trans = [1.0/len(numeric_transformations)] * len(numeric_transformations)
#             return uniform_desc, uniform_rel, uniform_con, uniform_n_mod, uniform_n_trans
# 
#     def _generate_expression_components(self,
#                                        descriptor_weights: List[float],
#                                        relation_weights: List[float],
#                                        concept_weights: List[float],
#                                        numeric_modifier_weights: List[float],
#                                        numeric_transform_weights: List[float],
#                                        metrics: Dict[str, float]) -> Dict[str, str]:
#         """
#         Generate components for a symbolic expression based on weighted vocabularies.
#         Uses semantic model feedback to guide selection when available.
#         Now with enhanced numeric integration.
# 
#         Args:
#             descriptor_weights: Weights for selecting state descriptors
#             relation_weights: Weights for selecting relations
#             concept_weights: Weights for selecting concepts
#             numeric_modifier_weights: Weights for selecting numeric modifiers
#             numeric_transform_weights: Weights for selecting numeric transformations
#             metrics: Dictionary of current system metrics
# 
#         Returns:
#             Dictionary of expression components
#         """
#         try:
#             # Choose which vocabulary to use (original or dynamic)
#             descriptors = self.dynamic_state_descriptors if self.dynamic_vocabulary_enabled else self.state_descriptors
#             relations = self.dynamic_relations if self.dynamic_vocabulary_enabled else self.relations
#             concepts = self.dynamic_surplus_concepts if self.dynamic_vocabulary_enabled else self.surplus_concepts
#             modifiers = self.dynamic_modifiers if self.dynamic_vocabulary_enabled else self.modifiers
#             num_modifiers = self.dynamic_numeric_modifiers if self.dynamic_vocabulary_enabled else self.numeric_modifiers
#             num_transforms = self.dynamic_numeric_transformations if self.dynamic_vocabulary_enabled else self.numeric_transformations
# 
#             # Select components based on weighted probabilities
#             descriptor = random.choices(descriptors, weights=descriptor_weights, k=1)[0]
# 
#             # If semantic model is available, use it to find semantically coherent combinations
#             if self.semantic_model and random.random() < 0.7:  # 70% chance to use semantic guidance
#                 # First, try to find a relation that semantically fits with the descriptor
#                 related_relations = self._semantic_find_related_terms(descriptor, relations, top_n=3)
#                 relation = random.choice(related_relations)
# 
#                 # Then find a concept that fits with the descriptor + relation combination
#                 desc_rel_combo = f"{descriptor} {relation}"
#                 related_concepts = self._semantic_find_related_terms(desc_rel_combo, concepts, top_n=3)
#                 concept = random.choice(related_concepts)
# 
#                 # Find semantically fitting numeric components
#                 related_num_modifiers = self._semantic_find_related_terms(concept, num_modifiers, top_n=2)
#                 numeric_modifier = random.choice(related_num_modifiers)
# 
#                 related_num_transforms = self._semantic_find_related_terms(concept, num_transforms, top_n=2)
#                 numeric_transform = random.choice(related_num_transforms)
#             else:
#                 # Fall back to standard weighted selection
#                 relation = random.choices(relations, weights=relation_weights, k=1)[0]
#                 concept = random.choices(concepts, weights=concept_weights, k=1)[0]
#                 numeric_modifier = random.choices(num_modifiers, weights=numeric_modifier_weights, k=1)[0]
#                 numeric_transform = random.choices(num_transforms, weights=numeric_transform_weights, k=1)[0]
# 
#             # Determine if we should use modifiers based on complexity
#             use_standard_modifier = random.random() < self.expression_complexity * 0.5
#             use_numeric_modifier = random.random() < self.expression_complexity * self.numeric_influence
#             use_numeric_transform = random.random() < self.expression_complexity * self.numeric_influence
# 
#             standard_modifier = random.choice(modifiers) if use_standard_modifier else None
# 
#             # Determine if we should include numeric values
#             include_numeric_values = random.random() < self.numeric_influence
# 
#             # Format numeric values if we're including them
#             numeric_value_str = ""
#             if include_numeric_values and metrics:
#                 # Choose a key metric to highlight
#                 key_metrics = ['surplus', 'distinction', 'coherence', 'entropy']
#                 # Filter to metrics that actually exist
#                 available_metrics = [k for k in key_metrics if k in metrics]
# 
#                 if available_metrics:
#                     # Select a metric to highlight
#                     selected_metric = random.choice(available_metrics)
#                     value = metrics[selected_metric]
# 
#                     # Format the value with potential transformation
#                     if use_numeric_transform and random.random() < 0.7:
#                         # Apply transformation phrase
#                         numeric_value_str = f"{selected_metric}={value:.2f} {numeric_transform}"
#                     else:
#                         # Simple value
#                         numeric_value_str = f"{selected_metric}={value:.2f}"
# 
#                     # Apply numeric modifier if applicable
#                     if use_numeric_modifier:
#                         # Check if trend matches the modifier
#                         if selected_metric in self.numeric_trends:
#                             trend = self.numeric_trends[selected_metric]
#                             # Only use modifier if it matches the trend
#                             if (numeric_modifier == "increasing" and trend.get('increasing', False)) or \
#                                (numeric_modifier == "decreasing" and trend.get('decreasing', False)) or \
#                                (numeric_modifier == "oscillating" and trend.get('oscillating', False)):
#                                 numeric_value_str = f"{numeric_modifier} {numeric_value_str}"
#                             else:
#                                 # Use generic modifier
#                                 numeric_value_str = f"{numeric_modifier} {numeric_value_str}"
#                         else:
#                             # No trend data, just use the modifier
#                             numeric_value_str = f"{numeric_modifier} {numeric_value_str}"
# 
#             # Special case for extreme states
#             coherence = metrics.get('coherence', 0.5)
#             distinction = metrics.get('distinction', 0.5)
# 
#             if coherence > 0.95 and distinction > 0.9:
#                 descriptor = "Coherent Distinction"
#                 relation = "stabilizes within"
#                 concept = "emergent ontology"
#                 standard_modifier = "systematically"
#                 # Include dimensionality if available
#                 if 'dimensionality' in metrics:
#                     numeric_value_str = f"dim={metrics['dimensionality']}"
#             elif coherence < 0.1 and distinction < 0.1:
#                 descriptor = "Entropic Flux"
#                 relation = "dissolves across"
#                 concept = "undifferentiated phase space"
#                 standard_modifier = "chaotically"
#                 # Include entropy if available
#                 if 'entropy' in metrics:
#                     numeric_value_str = f"entropy={metrics['entropy']:.2f} amplified"
# 
#             # Update transition statistics for pattern analysis
#             self._update_transition_statistics(descriptor)
# 
#             # Create component dictionary
#             components = {
#                 'descriptor': descriptor,
#                 'relation': relation,
#                 'concept': concept,
#                 'modifier': standard_modifier,
#                 'numeric_modifier': numeric_modifier if use_numeric_modifier else None,
#                 'numeric_transformation': numeric_transform if use_numeric_transform else None,
#                 'numeric_value_str': numeric_value_str if numeric_value_str else None
#             }
# 
#             return components
# 
#         except Exception as e:
#             print(f"Error generating expression components: {e}")
#             return {}
# 
#     def generate_symbolic_expression(self,
#                                 surplus: float,
#                                 distinction: float,
#                                 coherence: float,
#                                 entropy: Optional[float] = None,
#                                 dimensionality: Optional[int] = None,
#                                 additional_metrics: Optional[Dict[str, float]] = None) -> str:
#         """
#         Generates a symbolic expression based on the system's current metrics.
#         Now enhanced with numeric value integration and semantic model guidance.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             entropy: Optional entropy metric
#             dimensionality: Optional detected dimensionality
#             additional_metrics: Optional additional metrics to incorporate
# 
#         Returns:
#             A symbolic expression representing the current state
#         """
#         try:
#             # Ensure inputs are proper floats for stability
#             surplus = float(np.clip(surplus, 0.1, 10.0))
#             distinction = float(np.clip(distinction, 0.0, 1.0))
#             coherence = float(np.clip(coherence, 0.0, 1.0))
#             if entropy is not None:
#                 entropy = float(np.clip(entropy, 0.0, 1.0))
# 
#             # Prepare metrics for component generation
#             metrics = {
#                 'surplus': surplus,
#                 'distinction': distinction,
#                 'coherence': coherence,
#                 'entropy': entropy,
#                 'dimensionality': dimensionality,
#                 'time_elapsed': time.time() - self.start_time
#             }
# 
#             # Add any additional metrics
#             if additional_metrics:
#                 metrics.update(additional_metrics)
# 
#             # Calculate vocabulary selection weights with numeric components
#             descriptor_weights, relation_weights, concept_weights, numeric_modifier_weights, numeric_transform_weights = self._calculate_weights(
#                 surplus, distinction, coherence, entropy, dimensionality, metrics
#             )
# 
#             # Generate expression components
#             components = self._generate_expression_components(
#                 descriptor_weights, relation_weights, concept_weights,
#                 numeric_modifier_weights, numeric_transform_weights, metrics
#             )
# 
#             # Adapt expression complexity based on system metrics
#             self.expression_complexity = min(2.0, 0.5 + 0.5 * coherence + 0.3 * distinction + 0.2 * (surplus / 10.0))
# 
#             # Determine whether to use secondary concepts
#             use_secondary = random.random() < self.expression_complexity * 0.3
#             secondary = None
# 
#             if use_secondary:
#                 # Choose which secondary concepts to use
#                 sec_concepts = self.dynamic_secondary_concepts if self.dynamic_vocabulary_enabled else self.secondary_concepts
# 
#                 if self.semantic_model and random.random() < 0.7:
#                     # Use semantic model to find related secondary concept
#                     related_secondary = self._semantic_find_related_terms(components['concept'], sec_concepts, top_n=3)
#                     secondary = random.choice(related_secondary)
#                 else:
#                     secondary = random.choice(sec_concepts)
# 
#                 # Update concept with secondary component
#                 components['concept'] = f"{components['concept']} within {secondary}"
#                 components['secondary'] = secondary
# 
#             # Assemble the expression based on components
#             descriptor = components['descriptor']
#             relation = components['relation']
#             concept = components['concept']
#             modifier = components['modifier']
#             numeric_value_str = components['numeric_value_str']
# 
#             # Assemble the expression with appropriate formatting
#             if modifier:
#                 base_expression = f"{descriptor} {modifier} {relation} {concept}"
#             else:
#                 base_expression = f"{descriptor} {relation} {concept}"
# 
#             # Add numeric information if present
#             if numeric_value_str:
#                 # Format numeric values in brackets for clarity
#                 symbolic_expression = f"{base_expression} [{numeric_value_str}]."
#             else:
#                 symbolic_expression = f"{base_expression}."
# 
#             # Store components for external access and analysis
#             self.last_expression_components = components
# 
#             # Update expression counter
#             self.expression_counter += 1
# 
#             # Store in history with metadata
#             expression_entry = {
#                 'expression': symbolic_expression,
#                 'components': components,
#                 'metrics': metrics.copy(),
#                 'timestamp': time.time(),
#                 'complexity': self.expression_complexity
#             }
# 
#             self.expression_history.append(expression_entry)
# 
#             # Update pattern history
#             pattern_entry = {
#                 'descriptor': descriptor,
#                 'relation': relation,
#                 'concept': concept,
#                 'numeric_info': numeric_value_str if numeric_value_str else None
#             }
#             self.pattern_history.append(pattern_entry)
# 
#             # Update frequency analysis
#             self._update_frequency_analysis(descriptor, relation, concept)
# 
#             # Update semantic vocabulary coherence if model is available
#             if self.semantic_model and self.expression_counter % self.semantic_update_frequency == 0:
#                 self._update_vocabulary_coherence(components)
# 
#                 # Periodically save cache
#                 if self.expression_counter % (self.semantic_update_frequency * 5) == 0:
#                     self.save_semantic_cache()
# 
#             # Apply semantic refinement if coherence is low
#             if self.semantic_model:
#                 semantic_coherence = self._calculate_semantic_coherence(symbolic_expression)
#                 if semantic_coherence < self.semantic_coherence_threshold:
#                     refined_expression = self._semantic_refine_expression(components, semantic_coherence)
#                     if refined_expression:
#                         symbolic_expression = refined_expression
#                         # Update the expression in history
#                         expression_entry['expression'] = symbolic_expression
#                         expression_entry['refined'] = True
#                         self.expression_history[-1] = expression_entry
# 
#             return symbolic_expression
# 
#         except Exception as e:
#             print(f"Error generating symbolic expression: {e}")
#             return "Flux aligns with stability."  # Safe fallback
# 
#     def handle_post_emergence(self,
#                      surplus: float,
#                      distinction: float,
#                      coherence: float,
#                      dimensionality: Optional[int] = None,
#                      entropy: Optional[float] = None) -> str:
#         """
#         Triggers symbolic output generation after dimensional emergence is detected.
#         Records emergence event and generates an appropriate symbolic expression.
#         Enhanced with semantic model integration and numeric value incorporation.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             dimensionality: Optional detected dimensionality
#             entropy: Optional entropy metric
# 
#         Returns:
#             A symbolic expression representing the emergent state
#         """
#         try:
#             # Add randomness to prevent identical outputs
#             noise_factor = random.random() * 0.1
# 
#             # Record emergence event with timestamp and more detailed metrics
#             emergence_event = {
#                 'metrics': {
#                     'surplus': float(surplus) + noise_factor,
#                     'distinction': float(distinction) + noise_factor,
#                     'coherence': float(coherence) + noise_factor,
#                     'dimensionality': dimensionality,
#                     'entropy': entropy + noise_factor if entropy is not None else None,
#                     'timestamp': time.time(),
#                     'elapsed_time': time.time() - self.start_time
#                 },
#                 'event_id': len(self.emergence_events)
#             }
# 
#             self.emergence_events.append(emergence_event)
# 
#             # Increase expression complexity for emergence events
#             self.expression_complexity = min(2.0, self.expression_complexity * 1.5)
# 
#             # Generate expression with varied metrics to ensure diversity
#             varied_surplus = surplus * (1.0 + (random.random() - 0.5) * 0.2)  # +/- 10%
#             varied_distinction = distinction * (1.0 + (random.random() - 0.5) * 0.2)  # +/- 10%
#             varied_coherence = coherence * (1.0 + (random.random() - 0.5) * 0.2)  # +/- 10%
#             varied_entropy = entropy * (1.0 + (random.random() - 0.5) * 0.2) if entropy is not None else None
# 
#             # Always include dimensionality for emergence expressions
#             additional_metrics = {
#                 'emergence': True,
#                 'emergence_id': len(self.emergence_events)
#             }
# 
#             # Include dimensional information directly in the expression
#             if dimensionality is not None:
#                 additional_metrics['dim_transition'] = f"{dimensionality-1 if dimensionality > 1 else 1}→{dimensionality}"
#                 additional_metrics['dimensional_coherence'] = coherence * distinction  # Combined coherence in higher dimension
# 
#             # Generate the base expression
#             expression = self.generate_symbolic_expression(
#                 varied_surplus, varied_distinction, varied_coherence,
#                 entropy=varied_entropy, dimensionality=dimensionality,
#                 additional_metrics=additional_metrics
#             )
# 
#             # For emergence events, use semantic model to generate a more insightful secondary expression
#             if self.semantic_model and len(self.emergence_events) > 1:
#                 try:
#                     # Get patterns from emergence analysis
#                     patterns = self.analyze_emergence_patterns()
# 
#                     # Generate a semantically coherent follow-up expression
#                     if random.random() < 0.7 and patterns.get('dominant_patterns'):
#                         dominant = patterns['dominant_patterns']
# 
#                         # Create base follow-up template with dimensional information
#                         template = f"Dimensional shift to {dimensionality}D reveals {dominant.get('descriptor', 'Emergence')} within {dominant.get('concept', 'complexity')}."
# 
#                         # Get semantically related concepts to create follow-up
#                         from semantic_trainer import find_related_concepts
#                         query = f"dimensional shift {dominant.get('descriptor', '')} {dominant.get('concept', '')}"
#                         results = find_related_concepts(query, top_n=3)
# 
#                         if results:
#                             # Use the most relevant sentence from results
#                             follow_up = results[0][1]
# 
#                             # Add numeric information to the follow-up
#                             if entropy is not None:
#                                 follow_up += f" [entropy={entropy:.2f}, dimensionality={dimensionality}]"
# 
#                             # Combine expressions
#                             expression = f"{expression} {follow_up}"
#                 except Exception as e:
#                     print(f"Error generating semantic follow-up: {e}")
# 
#             # Standard follow-up generation if semantic model not available or failed
#             elif len(self.emergence_events) > 1:
#                 # Analyze emergence patterns
#                 patterns = self.analyze_emergence_patterns()
# 
#                 # Use the pattern analysis to generate a deeper insight with randomization
#                 if random.random() < 0.7 and patterns.get('dominant_patterns'):
#                     dominant = patterns['dominant_patterns']
# 
#                     # Create varied secondary expressions
#                     secondary_expressions = [
#                         f"Pattern analysis indicates {dominant.get('descriptor', 'Emergence')} "
#                         f"{random.choice(self.relations)} "
#                         f"{dominant.get('concept', 'complexity')} "
#                         f"across {dimensionality if dimensionality else 'multiple'} dimensions "
#                         f"[coherence={coherence:.2f}].",
# 
#                         f"Dimensional shift to {dimensionality}D reveals {dominant.get('descriptor', 'Emergence')} "
#                         f"{random.choice(self.relations)} "
#                         f"{random.choice(self.surplus_concepts)} "
#                         f"[distinction={distinction:.2f}].",
# 
#                         f"The {dimensionality}D structure {random.choice(self.modifiers)} "
#                         f"{random.choice(self.relations)} "
#                         f"{dominant.get('concept', 'ontology')} "
#                         f"[entropy={entropy:.2f if entropy is not None else 0.5}].",
# 
#                         f"Analysis suggests {random.choice(self.modifiers)} {dominant.get('descriptor', 'Distinction')} "
#                         f"within the emergent {dimensionality}D domain "
#                         f"[surplus={surplus:.2f}, coherence={coherence:.2f}]."
#                     ]
# 
#                     # Choose one secondary expression randomly
#                     follow_up = random.choice(secondary_expressions)
#                     expression = f"{expression} {follow_up}"
# 
#             return expression
# 
#         except Exception as e:
#             print(f"Error handling post-emergence: {e}")
#             return self.generate_symbolic_expression(surplus, distinction, coherence)
# 
#     def _update_frequency_analysis(self, descriptor: str, relation: str, concept: str, numeric_info: str = None):
#         """
#         Update frequency analysis of expression components, including numeric components.
# 
#         Args:
#             descriptor: The descriptor used
#             relation: The relation used
#             concept: The concept used
#             numeric_info: Optional numeric information string
#         """
#         try:
#             if 'descriptors' not in self.frequency_analysis:
#                 self.frequency_analysis = {
#                     'descriptors': {},
#                     'relations': {},
#                     'concepts': {},
#                     'numeric_modifiers': {},
#                     'numeric_transformations': {},
#                     'selected_metrics': {}
#                 }
# 
#             # Update descriptor frequency
#             self.frequency_analysis['descriptors'][descriptor] = (
#                 self.frequency_analysis['descriptors'].get(descriptor, 0) + 1
#             )
# 
#             # Update relation frequency
#             self.frequency_analysis['relations'][relation] = (
#                 self.frequency_analysis['relations'].get(relation, 0) + 1
#             )
# 
#             # Update concept frequency
#             self.frequency_analysis['concepts'][concept] = (
#                 self.frequency_analysis['concepts'].get(concept, 0) + 1
#             )
# 
#             # Update numeric component frequencies if available
#             if numeric_info:
#                 # Extract modifiers and metrics from numeric info
#                 parts = numeric_info.split()
# 
#                 # Look for modifiers like "increasing", "decreasing", etc.
#                 for modifier in self.numeric_modifiers:
#                     if modifier in numeric_info:
#                         self.frequency_analysis['numeric_modifiers'][modifier] = (
#                             self.frequency_analysis['numeric_modifiers'].get(modifier, 0) + 1
#                         )
# 
#                 # Look for transformations like "amplified", "normalized", etc.
#                 for transform in self.numeric_transformations:
#                     if transform in numeric_info:
#                         self.frequency_analysis['numeric_transformations'][transform] = (
#                             self.frequency_analysis['numeric_transformations'].get(transform, 0) + 1
#                         )
# 
#                 # Extract the selected metric (e.g., entropy=0.5, surplus=2.3)
#                 if "=" in numeric_info:
#                     metric_name = numeric_info.split("=")[0].strip()
#                     self.frequency_analysis['selected_metrics'][metric_name] = (
#                         self.frequency_analysis['selected_metrics'].get(metric_name, 0) + 1
#                     )
# 
#         except Exception as e:
#             print(f"Error updating frequency analysis: {e}")
# 
#     def analyze_emergence_patterns(self) -> Dict[str, Any]:
#         """
#         Analyzes patterns in emergence events and generated expressions.
#         Returns statistics and patterns detected in the symbolic outputs.
#         Enhanced with semantic clustering when model is available and
#         now including numeric component analysis.
# 
#         Returns:
#             Dictionary containing pattern analysis results
#         """
#         try:
#             if not self.emergence_events or not self.expression_history:
#                 return {'patterns': 'Insufficient data for pattern analysis'}
# 
#             # Extract metrics from history
#             coherence_values = [e['metrics'].get('coherence', 0.5) for e in self.emergence_events]
#             distinction_values = [e['metrics'].get('distinction', 0.5) for e in self.emergence_events]
# 
#             # Only use recent expressions for pattern analysis if we have many
#             expressions_to_analyze = self.expression_history
#             if len(expressions_to_analyze) > 20:
#                 expressions_to_analyze = expressions_to_analyze[-20:]
# 
#             # Calculate emergence stability
#             coherence_stability = float(np.std(coherence_values)) if len(coherence_values) > 1 else 0
#             distinction_stability = float(np.std(distinction_values)) if len(distinction_values) > 1 else 0
# 
#             # More advanced pattern analysis with the frequency analysis
#             if hasattr(self, 'frequency_analysis') and self.frequency_analysis:
#                 # Find most common components
#                 descriptor_counts = self.frequency_analysis.get('descriptors', {})
#                 relation_counts = self.frequency_analysis.get('relations', {})
#                 concept_counts = self.frequency_analysis.get('concepts', {})
#                 n_modifier_counts = self.frequency_analysis.get('numeric_modifiers', {})
#                 n_transform_counts = self.frequency_analysis.get('numeric_transformations', {})
#                 metric_counts = self.frequency_analysis.get('selected_metrics', {})
# 
#                 # Find dominant patterns
#                 dominant_descriptor = max(descriptor_counts.items(), key=lambda x: x[1])[0] if descriptor_counts else None
#                 dominant_relation = max(relation_counts.items(), key=lambda x: x[1])[0] if relation_counts else None
#                 dominant_concept = max(concept_counts.items(), key=lambda x: x[1])[0] if concept_counts else None
#                 dominant_n_modifier = max(n_modifier_counts.items(), key=lambda x: x[1])[0] if n_modifier_counts else None
#                 dominant_n_transform = max(n_transform_counts.items(), key=lambda x: x[1])[0] if n_transform_counts else None
#                 dominant_metric = max(metric_counts.items(), key=lambda x: x[1])[0] if metric_counts else None
# 
#                 # Calculate component diversity (normalized entropy)
#                 def calculate_diversity(counts):
#                     if not counts:
#                         return 0.0
#                     total = sum(counts.values())
#                     probabilities = [count/total for count in counts.values()]
#                     entropy = -sum(p * np.log2(p) for p in probabilities if p > 0)
#                     max_entropy = np.log2(len(counts))
#                     return entropy / max_entropy if max_entropy > 0 else 0.0
# 
#                 descriptor_diversity = calculate_diversity(descriptor_counts)
#                 relation_diversity = calculate_diversity(relation_counts)
#                 concept_diversity = calculate_diversity(concept_counts)
#                 n_modifier_diversity = calculate_diversity(n_modifier_counts)
#                 n_transform_diversity = calculate_diversity(n_transform_counts)
#                 metric_diversity = calculate_diversity(metric_counts)
# 
#                 # Find common sequences in the pattern history
#                 sequence_patterns = {}
#                 numeric_transitions = {}
# 
#                 if len(self.pattern_history) > 3:
#                     for i in range(len(self.pattern_history) - 2):
#                         # Track descriptor sequences
#                         seq = (
#                             self.pattern_history[i].get('descriptor', ''),
#                             self.pattern_history[i+1].get('descriptor', ''),
#                             self.pattern_history[i+2].get('descriptor', '')
#                         )
#                         sequence_patterns[seq] = sequence_patterns.get(seq, 0) + 1
# 
#                         # Track numeric transitions if present
#                         if ('numeric_info' in self.pattern_history[i] and
#                             'numeric_info' in self.pattern_history[i+1] and
#                             self.pattern_history[i]['numeric_info'] and
#                             self.pattern_history[i+1]['numeric_info']):
# 
#                             num_trans = (
#                                 self.pattern_history[i]['numeric_info'],
#                                 self.pattern_history[i+1]['numeric_info']
#                             )
#                             numeric_transitions[num_trans] = numeric_transitions.get(num_trans, 0) + 1
# 
#                 # Find most common sequence
#                 common_sequence = max(sequence_patterns.items(), key=lambda x: x[1])[0] if sequence_patterns else None
#                 common_numeric_transition = max(numeric_transitions.items(), key=lambda x: x[1])[0] if numeric_transitions else None
# 
#                 # Typical expression
#                 typical_expression = f"{dominant_descriptor} {dominant_relation} {dominant_concept}."
# 
#                 # Include numeric information if present
#                 if dominant_n_modifier and dominant_metric and dominant_n_transform:
#                     typical_numeric = f"{dominant_n_modifier} {dominant_metric}={0.5:.2f} {dominant_n_transform}"
#                     typical_expression = f"{typical_expression[:-1]} [{typical_numeric}]."
# 
#                 # Calculate complexity trend
#                 complexity_values = [e.get('complexity', 1.0) for e in self.expression_history[-10:]]
#                 complexity_trend = np.mean(np.diff(complexity_values)) if len(complexity_values) > 1 else 0.0
# 
#                 # Calculate transition matrix entropy (measure of pattern predictability)
#                 transition_entropy = 0.0
#                 if hasattr(self, 'transition_matrix') and isinstance(self.transition_matrix, np.ndarray):
#                     for row in self.transition_matrix:
#                         row_probs = row[row > 0]  # Only consider non-zero probabilities
#                         if len(row_probs) > 0:
#                             row_entropy = -np.sum(row_probs * np.log2(row_probs))
#                             transition_entropy += row_entropy
# 
#                     transition_entropy /= max(1, np.sum(self.transition_matrix > 0))  # Normalize
# 
#                 # Analyze numeric trends
#                 numeric_trend_analysis = {}
#                 if hasattr(self, 'numeric_trends') and self.numeric_trends:
#                     # Count trends by type
#                     trend_counts = {
#                         'increasing': sum(1 for t in self.numeric_trends.values() if t.get('increasing', False)),
#                         'decreasing': sum(1 for t in self.numeric_trends.values() if t.get('decreasing', False)),
#                         'oscillating': sum(1 for t in self.numeric_trends.values() if t.get('oscillating', False)),
#                         'stable': sum(1 for t in self.numeric_trends.values() if t.get('stable', False))
#                     }
# 
#                     # Calculate average rate of change and acceleration
#                     avg_rate = np.mean([t.get('rate_of_change', 0.0) for t in self.numeric_trends.values()])
#                     avg_accel = np.mean([t.get('acceleration', 0.0) for t in self.numeric_trends.values()])
# 
#                     numeric_trend_analysis = {
#                         'trend_counts': trend_counts,
#                         'avg_rate_of_change': float(avg_rate),
#                         'avg_acceleration': float(avg_accel),
#                         'dominant_trend': max(trend_counts.items(), key=lambda x: x[1])[0]
#                     }
# 
#                 # Add semantic coherence analysis if model is available
#                 semantic_analysis = {}
#                 if self.semantic_model and len(self.expression_history) > 5:
#                     try:
#                         # Compute average semantic coherence of recent expressions
#                         recent_expressions = [e['expression'] for e in self.expression_history[-5:]]
#                         coherence_scores = [self._calculate_semantic_coherence(expr) for expr in recent_expressions]
#                         semantic_analysis['recent_coherence'] = float(np.mean(coherence_scores))
# 
#                         # Compare current vocabulary coherence to initial
#                         for vocab_type in ['descriptors', 'relations', 'concepts', 'numeric_modifiers', 'numeric_transformations']:
#                             if vocab_type in self.vocabulary_coherence:
#                                 values = list(self.vocabulary_coherence[vocab_type].values())
#                                 semantic_analysis[f'{vocab_type}_coherence'] = float(np.mean(values))
#                     except Exception as e:
#                         print(f"Error in semantic analysis: {e}")
# 
#                 return {
#                     'emergence_count': len(self.emergence_events),
#                     'expression_count': len(self.expression_history),
#                     'coherence_stability': float(coherence_stability),
#                     'distinction_stability': float(distinction_stability),
#                     'component_diversity': {
#                         'descriptor': float(descriptor_diversity),
#                         'relation': float(relation_diversity),
#                         'concept': float(concept_diversity),
#                         'numeric_modifier': float(n_modifier_diversity),
#                         'numeric_transform': float(n_transform_diversity),
#                         'metric': float(metric_diversity),
#                         'overall': float((descriptor_diversity + relation_diversity + concept_diversity +
#                                   n_modifier_diversity + n_transform_diversity + metric_diversity) / 6)
#                     },
#                     'dominant_patterns': {
#                         'descriptor': dominant_descriptor,
#                         'relation': dominant_relation,
#                         'concept': dominant_concept,
#                         'numeric_modifier': dominant_n_modifier,
#                         'numeric_transformation': dominant_n_transform,
#                         'selected_metric': dominant_metric
#                     },
#                     'common_sequence': common_sequence,
#                     'common_numeric_transition': common_numeric_transition,
#                     'typical_expression': typical_expression,
#                     'complexity_trend': float(complexity_trend),
#                     'transition_entropy': float(transition_entropy),
#                     'current_complexity': float(self.expression_complexity),
#                     'numeric_trend_analysis': numeric_trend_analysis,
#                     'semantic_analysis': semantic_analysis if self.semantic_model else {}
#                 }
# 
#             # Simplified analysis if frequency data isn't available
#             return {
#                 'emergence_count': len(self.emergence_events),
#                 'expression_count': len(self.expression_history),
#                 'coherence_stability': float(coherence_stability),
#                 'distinction_stability': float(distinction_stability)
#             }
# 
#         except Exception as e:
#             print(f"Error analyzing emergence patterns: {e}")
#             return {
#                 'error': str(e),
#                 'emergence_count': len(self.emergence_events),
#                 'expression_count': len(self.expression_history)
#             }
# 
#     def get_vocabulary_status(self) -> Dict[str, Any]:
#         """
#         Returns the current vocabulary status, including dynamic expansions
#         and semantic coherence metrics.
# 
#         Returns:
#             Dictionary containing vocabulary statistics
#         """
#         try:
#             # Count original and dynamic vocabulary
#             orig_counts = {
#                 'descriptors': len(self.state_descriptors),
#                 'relations': len(self.relations),
#                 'concepts': len(self.surplus_concepts),
#                 'modifiers': len(self.modifiers),
#                 'secondary': len(self.secondary_concepts),
#                 'numeric_modifiers': len(self.numeric_modifiers),
#                 'numeric_transformations': len(self.numeric_transformations)
#             }
# 
#             dynamic_counts = {
#                 'descriptors': len(self.dynamic_state_descriptors),
#                 'relations': len(self.dynamic_relations),
#                 'concepts': len(self.dynamic_surplus_concepts),
#                 'modifiers': len(self.dynamic_modifiers),
#                 'secondary': len(self.dynamic_secondary_concepts),
#                 'numeric_modifiers': len(self.dynamic_numeric_modifiers),
#                 'numeric_transformations': len(self.dynamic_numeric_transformations)
#             }
# 
#             # Calculate added terms
#             added_terms = {
#                 'descriptors': list(set(self.dynamic_state_descriptors) - set(self.state_descriptors)),
#                 'relations': list(set(self.dynamic_relations) - set(self.relations)),
#                 'concepts': list(set(self.dynamic_surplus_concepts) - set(self.surplus_concepts)),
#                 'modifiers': list(set(self.dynamic_modifiers) - set(self.modifiers)),
#                 'secondary': list(set(self.dynamic_secondary_concepts) - set(self.secondary_concepts)),
#                 'numeric_modifiers': list(set(self.dynamic_numeric_modifiers) - set(self.numeric_modifiers)),
#                 'numeric_transformations': list(set(self.dynamic_numeric_transformations) - set(self.numeric_transformations))
#             }
# 
#             # Get top coherence terms if semantic model is used
#             top_coherence = {}
#             if self.semantic_model:
#                 for vocab_type in ['descriptors', 'relations', 'concepts', 'modifiers',
#                                   'secondary', 'numeric_modifiers', 'numeric_transformations']:
#                     if vocab_type in self.vocabulary_coherence:
#                         # Sort by coherence and get top 5
#                         sorted_terms = sorted(
#                             self.vocabulary_coherence[vocab_type].items(),
#                             key=lambda x: x[1],
#                             reverse=True
#                         )[:5]
#                         top_coherence[vocab_type] = sorted_terms
# 
#             # Calculate numeric integration statistics
#             numeric_stats = {}
#             if hasattr(self, 'numeric_memory') and len(self.numeric_memory) > 0:
#                 # Calculate frequency of numeric inclusion
#                 expressions_with_numeric = sum(1 for e in self.expression_history
#                                             if 'components' in e and e['components'].get('numeric_value_str'))
# 
#                 numeric_stats = {
#                     'inclusion_rate': expressions_with_numeric / max(1, len(self.expression_history)),
#                     'num_memory_size': len(self.numeric_memory),
#                     'trends_tracked': len(self.numeric_trends) if hasattr(self, 'numeric_trends') else 0,
#                     'numeric_influence': self.numeric_influence
#                 }
# 
#                 # Add trend summary if available
#                 if hasattr(self, 'numeric_trends') and self.numeric_trends:
#                     trend_summary = {
#                         'increasing': sum(1 for t in self.numeric_trends.values() if t.get('increasing', False)),
#                         'decreasing': sum(1 for t in self.numeric_trends.values() if t.get('decreasing', False)),
#                         'oscillating': sum(1 for t in self.numeric_trends.values() if t.get('oscillating', False)),
#                         'stable': sum(1 for t in self.numeric_trends.values() if t.get('stable', False))
#                     }
#                     numeric_stats['trend_summary'] = trend_summary
# 
#                 # Add frequency analysis for numeric components if available
#                 if 'numeric_modifiers' in self.frequency_analysis:
#                     top_modifiers = sorted(
#                         self.frequency_analysis['numeric_modifiers'].items(),
#                         key=lambda x: x[1],
#                         reverse=True
#                     )[:3]
#                     numeric_stats['top_modifiers'] = top_modifiers
# 
#                 if 'numeric_transformations' in self.frequency_analysis:
#                     top_transforms = sorted(
#                         self.frequency_analysis['numeric_transformations'].items(),
#                         key=lambda x: x[1],
#                         reverse=True
#                     )[:3]
#                     numeric_stats['top_transforms'] = top_transforms
# 
#                 if 'selected_metrics' in self.frequency_analysis:
#                     top_metrics = sorted(
#                         self.frequency_analysis['selected_metrics'].items(),
#                         key=lambda x: x[1],
#                         reverse=True
#                     )[:3]
#                     numeric_stats['top_metrics'] = top_metrics
# 
#             return {
#                 'original_counts': orig_counts,
#                 'dynamic_counts': dynamic_counts,
#                 'added_terms': added_terms,
#                 'top_coherence_terms': top_coherence,
#                 'numeric_stats': numeric_stats,
#                 'dynamic_vocabulary_enabled': self.dynamic_vocabulary_enabled,
#                 'semantic_model_active': self.semantic_model is not None,
#                 'expression_complexity': float(self.expression_complexity),
#                 'vocabulary_sizes': {
#                     'total_original': sum(orig_counts.values()),
#                     'total_dynamic': sum(dynamic_counts.values()),
#                     'total_added': sum(len(terms) for terms in added_terms.values())
#                 }
#             }
# 
#         except Exception as e:
#             print(f"Error getting vocabulary status: {e}")
#             return {
#                 'error': str(e),
#                 'semantic_model_active': self.semantic_model is not None
#             }
# 
#     def get_semantic_analysis(self, query):
#         """
#         Performs semantic analysis on a query using the semantic model,
#         now with enhanced numeric value consideration.
# 
#         Args:
#             query: Text to analyze
# 
#         Returns:
#             Dictionary with semantic analysis results
#         """
#         if not self.semantic_model:
#             return {"error": "Semantic model not available"}
# 
#         try:
#             # Calculate coherence with existing expressions
#             coherence = self._calculate_semantic_coherence(query)
# 
#             # Find related vocabulary terms
#             related_descriptors = self._semantic_find_related_terms(query, self.dynamic_state_descriptors, top_n=3)
#             related_concepts = self._semantic_find_related_terms(query, self.dynamic_surplus_concepts, top_n=3)
# 
#             # Also find related numeric modifiers and transformations
#             related_num_modifiers = self._semantic_find_related_terms(query, self.dynamic_numeric_modifiers, top_n=2)
#             related_num_transforms = self._semantic_find_related_terms(query, self.dynamic_numeric_transformations, top_n=2)
# 
#             # Find related historical expressions
#             related_expressions = []
#             if self.expression_history:
#                 # Get query embedding
#                 with torch.no_grad():
#                     query_embedding = self.sentence_model.encode([query], convert_to_tensor=True)[0]
# 
#                 # Compare with historical expressions
#                 similarities = []
#                 for entry in self.expression_history[-20:]:  # Last 20 expressions
#                     expr = entry['expression']
#                     expr_embedding = self.sentence_model.encode([expr], convert_to_tensor=True)[0]
# 
#                     similarity = torch.nn.functional.cosine_similarity(
#                         query_embedding.unsqueeze(0),
#                         expr_embedding.unsqueeze(0)
#                     ).item()
# 
#                     # Also include numeric information if available
#                     numeric_info = None
#                     if 'components' in entry and entry['components'].get('numeric_value_str'):
#                         numeric_info = entry['components']['numeric_value_str']
# 
#                     similarities.append((expr, similarity, numeric_info))
# 
#                 # Get top 3 similar expressions
#                 similarities.sort(key=lambda x: x[1], reverse=True)
#                 related_expressions = similarities[:3]
# 
#             # Analyze numeric relevance if query contains numeric indicators
#             numeric_relevance = self._analyze_numeric_relevance(query)
# 
#             # Generate a numeric expression component if query seems to focus on numeric aspects
#             suggested_numeric_component = None
#             if numeric_relevance > 0.5 and self.numeric_memory and len(self.numeric_memory) > 0:
#                 # Get recent metrics
#                 recent_metrics = self.numeric_memory[-1]
# 
#                 # Select a related numeric modifier and transformation
#                 num_modifier = related_num_modifiers[0] if related_num_modifiers else random.choice(self.numeric_modifiers)
#                 num_transform = related_num_transforms[0] if related_num_transforms else random.choice(self.numeric_transformations)
# 
#                 # Select a metric from recent memory
#                 if 'entropy' in recent_metrics:
#                     metric_name = 'entropy'
#                     value = recent_metrics['entropy']
#                 elif 'coherence' in recent_metrics:
#                     metric_name = 'coherence'
#                     value = recent_metrics['coherence']
#                 elif 'distinction' in recent_metrics:
#                     metric_name = 'distinction'
#                     value = recent_metrics['distinction']
#                 else:
#                     # Pick the first numeric value found
#                     for k, v in recent_metrics.items():
#                         if isinstance(v, (int, float)):
#                             metric_name = k
#                             value = v
#                             break
#                     else:
#                         metric_name = 'value'
#                         value = 0.5
# 
#                 # Format the suggested numeric component
#                 suggested_numeric_component = f"{num_modifier} {metric_name}={value:.2f} {num_transform}"
# 
#             return {
#                 "query": query,
#                 "coherence": coherence,
#                 "numeric_relevance": numeric_relevance,
#                 "related_descriptors": related_descriptors,
#                 "related_concepts": related_concepts,
#                 "related_numeric_modifiers": related_num_modifiers,
#                 "related_numeric_transformations": related_num_transforms,
#                 "related_expressions": related_expressions,
#                 "suggested_numeric_component": suggested_numeric_component,
#                 "analysis_time": datetime.now().isoformat()
#             }
# 
#         except Exception as e:
#             print(f"Error in semantic analysis: {e}")
#             return {"error": str(e)}
# 
#     def _analyze_numeric_relevance(self, text: str) -> float:
#         """
#         Analyze how numerically relevant a text query is.
# 
#         Args:
#             text: The text to analyze
# 
#         Returns:
#             Numeric relevance score between 0.0 and 1.0
#         """
#         try:
#             # Define indicators of numeric focus
#             numeric_indicators = [
#                 'number', 'value', 'quantity', 'measure', 'metric', 'parameter',
#                 'increase', 'decrease', 'grow', 'shrink', 'rise', 'fall',
#                 'trend', 'pattern', 'oscillation', 'fluctuation',
#                 'amplitude', 'frequency', 'period', 'phase',
#                 'threshold', 'critical', 'amplitude', 'scale',
#                 'dimension', 'coordinate', 'vector', 'matrix',
#                 'graph', 'chart', 'plot', 'axis', 'data',
#                 'statistical', 'correlation', 'distribution',
#                 'entropy', 'coherence', 'surplus', 'distinction'
#             ]
# 
#             # Additionally check for any of our numeric vocabulary terms
#             numeric_vocab = self.numeric_modifiers + self.numeric_transformations
#             all_indicators = numeric_indicators + numeric_vocab
# 
#             # Count occurrences of numeric indicators
#             count = 0
#             text_lower = text.lower()
#             for indicator in all_indicators:
#                 if indicator.lower() in text_lower:
#                     count += 1
# 
#             # Also check for actual numbers
#             import re
#             numbers = re.findall(r'\d+\.?\d*', text)
#             count += len(numbers)
# 
#             # Calculate relevance score (capped at 1.0)
#             return min(1.0, count / 10.0)  # Normalize (10+ indicators = 1.0)
# 
#         except Exception as e:
#             print(f"Error analyzing numeric relevance: {e}")
#             return 0.0  # Default to non-relevant
# 
# class SemanticRefiner:  # Class to hold the function
#     def __init__(self,
#                  transformer_base_name="bert-base-uncased",
#                  semantic_model_name="bert-base-uncased",
#                  sentence_model_name='all-MiniLM-L6-v2',
#                  semantic_coherence_threshold = 0.5,
#                  device = torch.device("cuda" if torch.cuda.is_available() else "cpu"),
#                  **kwargs):
#         """Initialize with a semantic model and coherence threshold."""
#         self.semantic_coherence_threshold = semantic_coherence_threshold
#         self.expression_history = [] #  Dummy
#         self.device = device
# 
#         try:
#             # Load tokenizer and models
#             self.tokenizer = AutoTokenizer.from_pretrained(transformer_base_name)
#             self.transformer_base = AutoModel.from_pretrained(transformer_base_name).to(self.device)
#             self.semantic_model = AutoModel.from_pretrained(semantic_model_name).to(self.device)
#             self.sentence_model = SentenceTransformer(sentence_model_name).to(self.device)
#             logger.info("Models loaded successfully")
# 
#         except Exception as e:
#               logger.error(f"Error loading models: {e}")
#               self.tokenizer = None
#               self.transformer_base = None
#               self.semantic_model = None
#               self.sentence_model = None
# 
# 
#     def _semantic_refine_expression(self, components: List[str], semantic_coherence: float) -> Optional[str]:
#         """
#         Refines a symbolic expression based on its components and semantic coherence.
# 
#         Args:
#             components: List of strings representing the components of the symbolic expression.
#             semantic_coherence: The semantic coherence of the expression.
# 
#         Returns:
#             A refined symbolic expression as a string, or None if no refinement is needed.
#         """
#         try:
#             if not components:
#                 logger.warning("No components provided for refinement.")
#                 return None
# 
#             if self.semantic_model is None:
#                 logger.warning("No semantic model available for refinement.")
#                 return None
#             # Implement a placeholder for the semantic refinement
#             logger.info(f"Attempting to refine expression with components {components} and coherence {semantic_coherence}.")
# 
#             # Example of potential actions
#             if semantic_coherence < self.semantic_coherence_threshold:
#                  # Dummy Refinement for testing.
#                 refined_components = [f"refined_{c}" for c in components]
#                 refined_expression = " ".join(refined_components)
#                 logger.info(f"Expression refined to: {refined_expression}")
#                 return refined_expression
#             else:
#                 logger.info("No refinement needed as semantic coherence is above threshold")
#                 return None
# 
#         except Exception as e:
#             logger.error(f"Error in semantic refinement: {e}")
#             return None
# 
#     def _calculate_semantic_coherence(self, expression: str) -> float:
#         """
#         Calculate the semantic coherence of an expression using the semantic model
# 
#         Args:
#             expression: The symbolic expression to evaluate
# 
#         Returns:
#             Coherence score between 0-1
#         """
#         if not self.semantic_model:
#             return 0.5  # Default if no model
# 
#         try:
#             # Tokenize the expression
#             inputs = self.tokenizer(expression, return_tensors="pt", padding=True,
#                                   truncation=True, max_length=512).to(self.device)
# 
#             # Get expression embedding from model
#             with torch.no_grad():
#                 # First encode with base transformer
#                 base_outputs = self.transformer_base(**inputs)
#                 pooled_output = base_outputs.last_hidden_state[:, 0, :]
#                 # Then use our semantic model
#                 expression_embedding = self.semantic_model(pooled_output)
# 
#             # Get reference embeddings
#             if self.expression_history:
#                 # Use historical expressions as reference
#                 reference_texts = [entry['expression'] for entry in self.expression_history[-10:]]
# 
#                 with torch.no_grad():
#                     reference_embeddings = self.sentence_model.encode(reference_texts, convert_to_tensor=True).to(self.device)
# 
#                 # Calculate average cosine similarity
#                 cos_sims = torch.nn.functional.cosine_similarity(
#                     expression_embedding.unsqueeze(0), reference_embeddings
#                 )
# 
#                 # Return maximum similarity as coherence score
#                 return torch.max(cos_sims).item()
#             else:
#                 # If no history, return 0.5 as default
#                 return 0.5
# 
#         except Exception as e:
#             print(f"Error calculating semantic coherence: {e}")
#             return 0.5
# 
#     def generate_symbolic_expression(self, components:List[str]) -> str:
#         """Dummy expression generator to show usage"""
#         symbolic_expression = " ".join(components)
# 
#         expression_entry = {'expression': symbolic_expression, 'refined': False}
#         self.expression_history.append(expression_entry)
#         # Apply semantic refinement if coherence is low
#         if self.semantic_model:
#             semantic_coherence = self._calculate_semantic_coherence(symbolic_expression)
#             if semantic_coherence < self.semantic_coherence_threshold:
#                 refined_expression = self._semantic_refine_expression(components, semantic_coherence)
#                 if refined_expression:
#                     symbolic_expression = refined_expression
#                     # Update the expression in history
#                     expression_entry['expression'] = symbolic_expression
#                     expression_entry['refined'] = True
#                     self.expression_history[-1] = expression_entry
# 
#         return symbolic_expression
# 
# # Define custom model class to match the saved model architecture
# class LegacySemioticModel(nn.Module):
#     """
#     Custom version of the SemioticExtractor that matches the model architecture
#     in the checkpoint, specifically using output dimensions of 384 instead of 256.
#     """
#     def __init__(self, hidden_dim=384, numeric_dim=8):
#         super().__init__()
#         self.hidden_dim = hidden_dim
# 
#         # Create encoder placeholder (will be replaced in load_semantic_model)
#         self.encoder = None
# 
#         # Text processing pathway
#         self.fc1 = nn.Linear(768, hidden_dim)  # 768 is BERT's output size
#         self.activation = nn.Tanh()
#         self.dropout = nn.Dropout(0.1)
# 
#         # Numeric processing pathway
#         self.numeric_enabled = numeric_dim > 0
#         if self.numeric_enabled:
#             self.fc1_numeric = nn.Linear(numeric_dim, hidden_dim // 2)
#             # Important: Use 384 as output size to match checkpoint
#             self.fc_combined = nn.Linear(hidden_dim + (hidden_dim // 2), 384)
#         else:
#             # Use 384 as output size to match checkpoint
#             self.fc2 = nn.Linear(hidden_dim, 384)
# 
#         # Final normalization - use 384 to match checkpoint
#         self.norm = nn.LayerNorm(384)
# 
#     def forward(self, input_ids, attention_mask, numeric_values=None):
#         # If encoder is None, create a placeholder output
#         if self.encoder is None:
#             # Create a placeholder embedding of the right shape
#             batch_size = input_ids.shape[0]
#             pooled_output = torch.zeros((batch_size, 768), device=input_ids.device)
#         else:
#             # Use the encoder
#             with torch.no_grad():
#                 outputs = self.encoder(input_ids, attention_mask=attention_mask)
#             pooled_output = outputs.last_hidden_state[:, 0, :]
# 
#         # Process text features
#         text_features = self.activation(self.fc1(pooled_output))
#         text_features = self.dropout(text_features)
# 
#         # Process and combine with numeric features if provided
#         if self.numeric_enabled and numeric_values is not None:
#             numeric_features = self.activation(self.fc1_numeric(numeric_values))
#             combined_features = torch.cat([text_features, numeric_features], dim=1)
#             output = self.fc_combined(combined_features)
#         else:
#             output = self.fc2(text_features) if hasattr(self, 'fc2') else text_features
# 
#         return self.norm(output)
# 
#

"""# 16. Emergence Monitor"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile emergence_monitor.py
# import time
# import numpy as np
# import traceback
# from typing import Dict, Tuple, List, Any, Optional
# from collections import deque
# 
# class EmergenceEvent:
#     """Represents a dimensional emergence event with enhanced tracking capabilities."""
#     def __init__(self, shape, timestamp, metrics=None, resource_data=None):
#         """
#         Initialize an emergence event with shape, timing, and associated metrics.
# 
#         Args:
#             shape: The tensor shape where emergence was detected
#             timestamp: Time when emergence was detected
#             metrics: Optional metrics associated with the emergence event
#             resource_data: Optional resource usage data during emergence
#         """
#         self.shape = shape
#         self.dimensionality = len(shape) if isinstance(shape, (tuple, list)) else 0
#         self.timestamp = timestamp
#         self.metrics = metrics or {}
#         self.resource_data = resource_data or {}
#         self.duration = 0
#         self.previous_dimensionality = 3  # Default assumption
#         self.coherence_before = self.metrics.get('coherence', 0.0)
#         self.stability_before = self.metrics.get('stability', 0.0)
#         self.emergence_intensity = 0.0  # Will be calculated when emergence ends
# 
#     def update_with_end_data(self, end_timestamp, end_metrics=None):
#         """Update the event with data from when emergence ends."""
#         self.duration = end_timestamp - self.timestamp
#         if end_metrics:
#             self.coherence_after = end_metrics.get('coherence', 0.0)
#             self.stability_after = end_metrics.get('stability', 0.0)
#             # Calculate emergence intensity from metrics change
#             coherence_delta = abs(self.coherence_after - self.coherence_before)
#             stability_delta = abs(self.stability_after - self.stability_before)
#             self.emergence_intensity = (coherence_delta + stability_delta) / 2
# 
#     def __str__(self):
#         return f"EmergenceEvent: {self.dimensionality}D {self.shape} at {self.timestamp:.2f}s (duration: {self.duration:.2f}s)"
# 
# class EmergenceTracker:
#     """Tracks and analyzes emergent computational phenomena with enhanced pattern recognition."""
#     def __init__(self):
#         self.emergence_events = []
#         self.dimension_transitions = []
#         self.stability_history = []
#         self.last_emergence_time = 0
#         self.emergence_duration = 0
#         self.is_emergence_active = False
#         self.active_emergence_event = None
# 
#         # Resource monitoring data
#         self.baseline_resource_usage = {'cpu_percent': 0, 'memory_percent': 0}
#         self.resource_samples_count = 0
# 
#         # Enhanced emergence pattern analysis
#         self.emergence_patterns = {}
#         self.resource_correlation = []
#         self.emergence_prediction_model = None
#         self.occurrence_frequency = {}
#         self.pattern_sequences = deque(maxlen=10)
#         self.emergence_periodicity = None
# 
#         # Additional metrics tracking
#         self.coherence_during_emergence = []
#         self.distinction_during_emergence = []
#         self.performance_impact = []
# 
#     def monitor_resources(self) -> Dict[str, float]:
#         """Enhanced resource monitoring with better memory management."""
#         try:
#             import psutil
#             import gc
# 
#             # Run garbage collection to clean up unused objects
#             gc.collect()
# 
#             # Get process-specific info
#             process = psutil.Process()
#             cpu_percent = process.cpu_percent()
#             memory_info = process.memory_info()
# 
#             # Get system-wide info
#             system_memory = psutil.virtual_memory()
# 
#             resource_data = {
#                 'timestamp': time.time(),
#                 'cpu_percent': cpu_percent,
#                 'memory_percent': system_memory.percent,
#                 'memory_used_mb': memory_info.rss / (1024 * 1024),  # Process memory in MB
#                 'memory_available_mb': system_memory.available / (1024 * 1024),  # System available in MB
#                 'memory_total_mb': system_memory.total / (1024 * 1024)  # Total system memory in MB
#             }
# 
#             # Track resource history
#             if hasattr(self, 'resource_history'):
#                 self.resource_history.append(resource_data)
#                 # Limit history size
#                 if len(self.resource_history) > 1000:
#                     self.resource_history = self.resource_history[-1000:]
# 
#             return resource_data
#         except Exception as e:
#             print(f"Error monitoring resources: {e}")
#             return {
#                 'cpu_percent': 0.0,
#                 'memory_percent': 0.0,
#                 'memory_available_mb': 0.0
#             }
# 
#     def _safe_correlation(self, x: np.ndarray, y: np.ndarray) -> float:
#         """
#         Calculate correlation with proper error handling for division by zero issues.
# 
#         Args:
#             x: First data array
#             y: Second data array
# 
#         Returns:
#             Correlation coefficient or 0.0 if calculation fails
#         """
#         try:
#             if len(x) < 2 or len(y) < 2:
#                 return 0.0
# 
#             # First, handle NaN values
#             x = np.nan_to_num(x, nan=0.0)
#             y = np.nan_to_num(y, nan=0.0)
# 
#             # Calculate standard deviations
#             std_x = np.std(x)
#             std_y = np.std(y)
# 
#             # Check for constant arrays which would cause division by zero
#             if std_x < 1e-10 or std_y < 1e-10:
#                 # Add small random noise to prevent constant arrays
#                 if std_x < 1e-10:
#                     x = x + np.random.normal(0, 1e-5, size=x.shape)
#                     std_x = np.std(x)
# 
#                 if std_y < 1e-10:
#                     y = y + np.random.normal(0, 1e-5, size=y.shape)
#                     std_y = np.std(y)
# 
#                 # If still constant after adding noise, return 0
#                 if std_x < 1e-10 or std_y < 1e-10:
#                     return 0.0
# 
#             # Manually calculate correlation to avoid NumPy warning
#             x_normalized = (x - np.mean(x)) / std_x
#             y_normalized = (y - np.mean(y)) / std_y
#             correlation = np.mean(x_normalized * y_normalized)
# 
#             # Check for NaN results
#             if np.isnan(correlation):
#                 return 0.0
# 
#             return float(correlation)
#         except Exception as e:
#             print(f"Error calculating correlation: {e}")
#             return 0.0
# 
#     def record_emergence(self, tensor_shape: Tuple, timestamp: float,
#                      resource_usage: Dict, agent_metrics: Dict) -> Dict:
#         """
#         Record and analyze an emergence event with enhanced pattern tracking.
# 
#         Args:
#             tensor_shape: The tensor shape where emergence was detected
#             timestamp: Time when emergence was detected
#             resource_usage: Resource usage data during emergence
#             agent_metrics: Agent state metrics during emergence
# 
#         Returns:
#             Dictionary with emergence data
#         """
#         try:
#             # Calculate the total dimensionality and elements
#             total_elements = np.prod(tensor_shape)
# 
#             # Create emergence data record
#             emergence_data = {
#                 'timestamp': timestamp,
#                 'tensor_shape': tensor_shape,
#                 'total_elements': total_elements,
#                 'resource_usage': resource_usage.copy(),
#                 'agent_metrics': agent_metrics.copy(),
#                 'elapsed_time': 0,
#                 'dimensionality': len(tensor_shape),
#                 'previous_dimensionality': 3 if not self.dimension_transitions else self.dimension_transitions[-1]['new_dimensionality']
#             }
# 
#             # Record dimension transition
#             if not self.dimension_transitions or self.dimension_transitions[-1]['new_dimensionality'] != len(tensor_shape):
#                 self.dimension_transitions.append({
#                     'timestamp': timestamp,
#                     'old_dimensionality': 3 if not self.dimension_transitions else self.dimension_transitions[-1]['new_dimensionality'],
#                     'new_dimensionality': len(tensor_shape),
#                     'stability_before': agent_metrics.get('stability', 0),
#                     'coherence_before': agent_metrics.get('coherence', 0)
#                 })
# 
#             # If this is the start of a new emergence period
#             if not self.is_emergence_active:
#                 self.is_emergence_active = True
#                 self.last_emergence_time = timestamp
#                 # Create new emergence event object
#                 self.active_emergence_event = EmergenceEvent(
#                     tensor_shape, timestamp, agent_metrics, resource_usage
#                 )
#                 # Record baseline resource usage
#                 if self.resource_samples_count < 5:  # If we don't have good baseline data yet
#                     self.baseline_resource_usage = resource_usage.copy()
#                     self.resource_samples_count = 1
#                 else:
#                     # Update baseline with exponential moving average
#                     for key in ['cpu_percent', 'memory_percent']:
#                         if key in resource_usage and key in self.baseline_resource_usage:
#                             self.baseline_resource_usage[key] = 0.9 * self.baseline_resource_usage[key] + 0.1 * resource_usage[key]
#             else:
#                 # Update existing emergence period
#                 emergence_data['elapsed_time'] = timestamp - self.last_emergence_time
#                 self.emergence_duration += emergence_data['elapsed_time']
#                 self.last_emergence_time = timestamp
# 
#                 # Track metrics during emergence
#                 if agent_metrics:
#                     self.coherence_during_emergence.append(agent_metrics.get('coherence', 0))
#                     self.distinction_during_emergence.append(agent_metrics.get('distinction_level', 0))
# 
#                 # Track performance impact
#                 if resource_usage and self.baseline_resource_usage:
#                     cpu_impact = resource_usage.get('cpu_percent', 0) - self.baseline_resource_usage.get('cpu_percent', 0)
#                     memory_impact = resource_usage.get('memory_percent', 0) - self.baseline_resource_usage.get('memory_percent', 0)
#                     self.performance_impact.append((cpu_impact, memory_impact))
# 
#             # Track and analyze pattern
#             shape_key = 'x'.join(str(dim) for dim in tensor_shape)
#             if shape_key not in self.emergence_patterns:
#                 self.emergence_patterns[shape_key] = {
#                     'count': 0,
#                     'first_seen': timestamp,
#                     'resource_impact': [],
#                     'associated_metrics': [],
#                     'durations': []
#                 }
# 
#             pattern = self.emergence_patterns[shape_key]
#             pattern['count'] += 1
#             pattern['last_seen'] = timestamp
# 
#             # Record resource impact
#             for key in ['cpu_percent', 'memory_percent']:
#                 if key in resource_usage and key in self.baseline_resource_usage:
#                     impact = resource_usage[key] - self.baseline_resource_usage[key]
#                     pattern['resource_impact'].append(impact)
# 
#             # Record associated metrics
#             pattern['associated_metrics'].append({
#                 'distinction': agent_metrics.get('distinction_level', 0),
#                 'coherence': agent_metrics.get('coherence', 0),
#                 'stability': agent_metrics.get('stability', 0)
#             })
# 
#             # Update occurrence frequency for periodicity analysis
#             hours_bucket = int(timestamp // 3600)
#             minutes_bucket = int(timestamp // 60)
#             self.occurrence_frequency[hours_bucket] = self.occurrence_frequency.get(hours_bucket, 0) + 1
# 
#             # Add to pattern sequence for sequence analysis
#             self.pattern_sequences.append(shape_key)
# 
#             # Calculate correlation between emergence and resources
#             if len(self.emergence_events) > 5:
#                 recent_events = self.emergence_events[-5:]
#                 cpu_trend = [e['resource_usage']['cpu_percent'] for e in recent_events]
#                 coherence_trend = [e['agent_metrics'].get('coherence', 0) for e in recent_events]
# 
#                 if len(cpu_trend) > 1 and len(coherence_trend) > 1:
#                     cpu_changes = np.diff(cpu_trend)
#                     coherence_changes = np.diff(coherence_trend)
# 
#                     if len(cpu_changes) > 0 and len(coherence_changes) > 0:
#                         try:
#                             correlation = self._safe_correlation(cpu_changes, coherence_changes)
#                             emergence_data['resource_correlation'] = correlation
#                             self.resource_correlation.append(correlation)
#                         except Exception as e:
#                             emergence_data['resource_correlation'] = 0
#                             print(f"Error calculating correlation: {e}")
# 
#             self.emergence_events.append(emergence_data)
#             self._try_detect_periodicity()
#             return emergence_data
# 
#         except Exception as e:
#             print(f"Error in record_emergence: {e}")
#             traceback.print_exc()
#             return {'error': str(e), 'timestamp': timestamp}
# 
#     def end_emergence(self, timestamp: float, metrics: Dict = None) -> Dict:
#         """
#         Mark the end of an emergence event and complete the analysis.
# 
#         Args:
#             timestamp: Time when emergence ended
#             metrics: Current agent metrics
# 
#         Returns:
#             Dictionary with emergence summary data
#         """
#         if not hasattr(self, 'is_emergence_active') or not self.is_emergence_active or not hasattr(self, 'active_emergence_event') or self.active_emergence_event is None:
#             return {"status": "No active emergence to end"}
# 
#         try:
#             # Calculate final duration
#             duration = timestamp - self.active_emergence_event.timestamp
# 
#             # Update the active emergence event
#             self.active_emergence_event.update_with_end_data(timestamp, metrics)
# 
#             # Add the completed event to our events list
#             self.emergence_events.append(self.active_emergence_event)
# 
#             # Update pattern durations
#             if hasattr(self.active_emergence_event, 'shape'):
#                 shape_key = 'x'.join(str(dim) for dim in self.active_emergence_event.shape)
#                 if hasattr(self, 'emergence_patterns') and shape_key in self.emergence_patterns:
#                     self.emergence_patterns[shape_key]['durations'].append(duration)
# 
#             # Reset active emergence tracking
#             self.is_emergence_active = False
#             self.active_emergence_event = None
#             if hasattr(self, 'coherence_during_emergence'):
#                 self.coherence_during_emergence = []
#             if hasattr(self, 'distinction_during_emergence'):
#                 self.distinction_during_emergence = []
#             if hasattr(self, 'performance_impact'):
#                 self.performance_impact = []
# 
#             return {
#                 "status": "Emergence ended",
#                 "duration": duration,
#                 "timestamp": timestamp
#             }
# 
#         except Exception as e:
#             print(f"Error ending emergence: {e}")
#             traceback.print_exc()
#             self.is_emergence_active = False
#             self.active_emergence_event = None
#             return {"status": "Error ending emergence", "error": str(e)}
# 
#     def update_stability(self, stability: float, timestamp: float):
#         """
#         Track stability during emergence for correlation analysis.
# 
#         Args:
#             stability: Current stability metric
#             timestamp: Current timestamp
#         """
#         self.stability_history.append({
#             'timestamp': timestamp,
#             'stability': stability
#         })
# 
#     def get_emergence_summary(self) -> Dict:
#         """
#         Get a comprehensive summary of emergence information with pattern analysis.
# 
#         Returns:
#             Dictionary of emergence summary statistics and patterns
#         """
#         if not self.emergence_events:
#             return {"status": "No emergence detected"}
# 
#         try:
#             # Get latest emergence data
#             latest = self.emergence_events[-1] if isinstance(self.emergence_events[-1], dict) else vars(self.emergence_events[-1])
# 
#             # Count dimensions across events
#             dimension_counts = {}
#             for event in self.emergence_events:
#                 if isinstance(event, dict):
#                     dim = event['dimensionality']
#                 else:
#                     dim = event.dimensionality
#                 dimension_counts[dim] = dimension_counts.get(dim, 0) + 1
# 
#             # Calculate average stability change during emergence
#             stability_during_emergence = 0
#             if len(self.stability_history) > 1:
#                 stability_changes = [abs(s['stability'] - self.stability_history[i-1]['stability'])
#                                    for i, s in enumerate(self.stability_history) if i > 0]
#                 if stability_changes:
#                     stability_during_emergence = sum(stability_changes) / len(stability_changes)
# 
#             # Calculate resource impact
#             avg_cpu_impact = 0
#             avg_memory_impact = 0
#             if self.performance_impact:
#                 cpu_impacts = [impact[0] for impact in self.performance_impact]
#                 memory_impacts = [impact[1] for impact in self.performance_impact]
#                 avg_cpu_impact = sum(cpu_impacts) / len(cpu_impacts) if cpu_impacts else 0
#                 avg_memory_impact = sum(memory_impacts) / len(memory_impacts) if memory_impacts else 0
# 
#             # Calculate pattern diversity
#             pattern_diversity = len(self.emergence_patterns) / max(1, len(self.emergence_events))
# 
#             # Calculate correlation between resource usage and emergence
#             resource_correlation = np.mean(self.resource_correlation) if self.resource_correlation else 0
# 
#             # Identify most common pattern
#             most_common_pattern = max(self.emergence_patterns.items(),
#                                     key=lambda x: x[1]['count']) if self.emergence_patterns else (None, {})
#             most_common_pattern_key = most_common_pattern[0]
#             most_common_pattern_data = most_common_pattern[1]
# 
#             return {
#                 "status": "Active" if self.is_emergence_active else "Inactive",
#                 "emergence_count": len(self.emergence_events),
#                 "current_dimensionality": latest.get('dimensionality', 0)
#                     if isinstance(latest, dict) else latest.get('dimensionality', 0),
#                 "dimension_history": dimension_counts,
#                 "current_shape": latest.get('tensor_shape', None)
#                     if isinstance(latest, dict) else latest.get('shape', None),
#                 "total_duration": self.emergence_duration,
#                 "dimension_transitions": len(self.dimension_transitions),
#                 "stability_impact": stability_during_emergence,
#                 "resource_impact": avg_cpu_impact,
#                 "memory_impact": avg_memory_impact,
#                 "pattern_diversity": pattern_diversity,
#                 "resource_correlation": resource_correlation,
#                 "most_common_pattern": most_common_pattern_key,
#                 "most_common_count": most_common_pattern_data.get('count', 0),
#                 "periodicity": self.emergence_periodicity
#             }
# 
#         except Exception as e:
#             print(f"Error getting emergence summary: {e}")
#             traceback.print_exc()
#             return {
#                 "status": "Error in summary generation",
#                 "emergence_count": len(self.emergence_events),
#                 "error": str(e)
#             }
# 
#     def get_detailed_analysis(self) -> Dict:
#         """
#         Get detailed analysis of emergence patterns with comprehensive metrics.
# 
#         Returns:
#             Dictionary with detailed pattern analysis
#         """
#         if not self.emergence_patterns:
#             return {"status": "No patterns to analyze"}
# 
#         try:
#             pattern_analysis = {}
#             total_events = sum(pattern.get('count', 0) for pattern in self.emergence_patterns.values())
# 
#             for shape_key, pattern in self.emergence_patterns.items():
#                 if pattern['count'] < 2:
#                     continue
# 
#                 # Calculate stability during this pattern
#                 stability_values = [m['stability'] for m in pattern['associated_metrics']]
#                 coherence_values = [m['coherence'] for m in pattern['associated_metrics']]
#                 distinction_values = [m['distinction'] for m in pattern['associated_metrics']]
# 
#                 # Calculate time between occurrences if this pattern was seen multiple times
#                 if 'first_seen' in pattern and 'last_seen' in pattern and pattern['count'] > 1:
#                     time_span = pattern['last_seen'] - pattern['first_seen']
#                     frequency = pattern['count'] / time_span if time_span > 0 else 0
#                 else:
#                     frequency = 0
# 
#                 # Calculate average duration if available
#                 avg_duration = sum(pattern.get('durations', [0])) / len(pattern.get('durations', [1])) if pattern.get('durations') else 0
# 
#                 pattern_analysis[shape_key] = {
#                     "count": pattern['count'],
#                     "frequency": frequency,
#                     "avg_duration": avg_duration,
#                     "proportion": pattern['count'] / total_events if total_events else 0,
#                     "duration": pattern.get('last_seen', 0) - pattern.get('first_seen', 0),
#                     "avg_cpu_impact": np.mean(pattern['resource_impact']) if pattern['resource_impact'] else 0,
#                     "stability_mean": np.mean(stability_values) if stability_values else 0,
#                     "stability_variance": np.var(stability_values) if stability_values else 0,
#                     "coherence_mean": np.mean(coherence_values) if coherence_values else 0,
#                     "coherence_variance": np.var(coherence_values) if coherence_values else 0,
#                     "distinction_mean": np.mean(distinction_values) if distinction_values else 0,
#                     "distinction_variance": np.var(distinction_values) if distinction_values else 0
#                 }
# 
#             # Find sequences of patterns that commonly occur together
#             sequence_patterns = {}
#             if len(self.pattern_sequences) >= 3:
#                 for i in range(len(self.pattern_sequences) - 2):
#                     seq = (self.pattern_sequences[i], self.pattern_sequences[i+1], self.pattern_sequences[i+2])
#                     sequence_patterns[seq] = sequence_patterns.get(seq, 0) + 1
# 
#             # Get most common sequence
#             common_sequence = max(sequence_patterns.items(), key=lambda x: x[1]) if sequence_patterns else (None, 0)
# 
#             # Find dimension transition patterns
#             dimension_transition_patterns = {}
#             if len(self.dimension_transitions) >= 2:
#                 for i in range(len(self.dimension_transitions) - 1):
#                     dt_pair = (
#                         self.dimension_transitions[i]['new_dimensionality'],
#                         self.dimension_transitions[i+1]['new_dimensionality']
#                     )
#                     dimension_transition_patterns[dt_pair] = dimension_transition_patterns.get(dt_pair, 0) + 1
# 
#             return {
#                 "total_patterns": len(self.emergence_patterns),
#                 "total_events": total_events,
#                 "pattern_details": pattern_analysis,
#                 "dominant_pattern": max(self.emergence_patterns.items(),
#                                      key=lambda x: x[1]['count'])[0] if self.emergence_patterns else None,
#                 "common_sequences": common_sequence[0] if common_sequence[0] else None,
#                 "sequence_frequency": common_sequence[1] if common_sequence[1] else 0,
#                 "dimension_transition_patterns": dimension_transition_patterns
#             }
# 
#         except Exception as e:
#             print(f"Error in detailed analysis: {e}")
#             traceback.print_exc()
#             return {"status": "Error generating detailed analysis", "error": str(e)}
# 
#     def _try_detect_periodicity(self):
#         """Attempt to detect periodicity in emergence events."""
#         try:
#             if len(self.emergence_events) < 5:
#                 return
# 
#             # Extract timestamps
#             if isinstance(self.emergence_events[0], dict):
#                 timestamps = [e['timestamp'] for e in self.emergence_events]
#             else:
#                 timestamps = [e.timestamp for e in self.emergence_events]
# 
#             # Calculate time differences between consecutive events
#             time_diffs = np.diff(timestamps)
# 
#             # Check if time differences are consistent
#             if len(time_diffs) >= 3:
#                 mean_diff = np.mean(time_diffs)
#                 std_diff = np.std(time_diffs)
# 
#                 # If standard deviation is less than 20% of the mean, we have periodicity
#                 if std_diff < 0.2 * mean_diff:
#                     self.emergence_periodicity = {
#                         'period': mean_diff,
#                         'confidence': 1.0 - (std_diff / mean_diff),
#                         'unit': 'seconds'
#                     }
#                 else:
#                     self.emergence_periodicity = None
#         except Exception as e:
#             print(f"Error detecting periodicity: {e}")
#             self.emergence_periodicity = None
# 
# class DimensionMonitor:
#     """Monitors tensor shapes and captures emergence events with improved transition detection."""
#     def __init__(self):
#         """Initialize the dimension monitor with empty tracking structures."""
#         self.past_shapes = {}  # {id: shape} mapping
#         self.shape_transitions = []
#         self.dimension_changes = []
#         self.last_dimensionality = 3  # Start with standard tensor dimensionality
#         self.transition_frequencies = {}  # Track frequency of specific transitions
#         self.stable_periods = []  # Track periods of dimensional stability
#         self.unstable_periods = []  # Track periods of dimensional instability
#         self.stability_threshold = 10  # Number of consecutive identical dimensionality observations
#         self.current_stability_count = 0
#         self.current_shape = None
#         self.dimensionality_histogram = {}  # Track frequency of each dimensionality
# 
#         # Transition pattern tracking
#         self.transition_sequence = deque(maxlen=20)  # Track recent dimension transitions
#         self.transition_patterns = {}  # Common transition sequences
#         self.current_stable_period_start = time.time()
#         self.last_transition_time = None
# 
#     def register_shape(self, tensor_object, tag="unknown"):
#         """
#         Register a tensor and check for dimensional changes with enhanced transition detection.
# 
#         Args:
#             tensor_object: The tensor to analyze for dimensional emergence
#             tag: Optional tag to identify the source of this tensor
# 
#         Returns:
#             EmergenceEvent if dimensional emergence detected, None otherwise
#         """
#         if not hasattr(tensor_object, 'shape'):
#             return None
# 
#         try:
#             shape = tensor_object.shape
#             obj_id = id(tensor_object)
# 
#             # Check if we've seen this object before with a different shape
#             dimensionality = len(shape)
#             event = None
# 
#             # Update dimensionality histogram
#             self.dimensionality_histogram[dimensionality] = self.dimensionality_histogram.get(dimensionality, 0) + 1
# 
#             # Check for shape change in this specific tensor object
#             if obj_id in self.past_shapes and len(self.past_shapes[obj_id]) != dimensionality:
#                 # Dimensional emergence detected!
#                 event = EmergenceEvent(
#                     shape=shape,
#                     timestamp=time.time(),
#                     metrics={'old_shape': self.past_shapes[obj_id], 'tag': tag,
#                              'transition_type': 'object_specific'}
#                 )
#                 self.dimension_changes.append({
#                     'timestamp': time.time(),
#                     'obj_id': obj_id,
#                     'old_shape': self.past_shapes[obj_id],
#                     'new_shape': shape,
#                     'old_dimensionality': len(self.past_shapes[obj_id]),
#                     'new_dimensionality': dimensionality,
#                     'tag': tag,
#                     'transition_type': 'object_specific'
#                 })
# 
#                 # Track this specific transition
#                 transition_key = f"{len(self.past_shapes[obj_id])}D→{dimensionality}D"
#                 self.transition_frequencies[transition_key] = self.transition_frequencies.get(transition_key, 0) + 1
# 
#                 # Reset stability counter
#                 self.current_stability_count = 0
# 
#                 # Record transition time
#                 current_time = time.time()
#                 if self.last_transition_time is not None:
#                     transition_interval = current_time - self.last_transition_time
#                     # Add to unstable periods if transitions are happening rapidly
#                     if transition_interval < 5.0:  # Less than 5 seconds between transitions
#                         self.unstable_periods.append({
#                             'start': self.last_transition_time,
#                             'end': current_time,
#                             'duration': transition_interval,
#                             'transition': transition_key
#                         })
#                 self.last_transition_time = current_time
# 
#             # Update shape record
#             self.past_shapes[obj_id] = shape
# 
#             # If dimensionality changed globally
#             if dimensionality != self.last_dimensionality:
#                 self.shape_transitions.append({
#                     'timestamp': time.time(),
#                     'old_dimensionality': self.last_dimensionality,
#                     'new_dimensionality': dimensionality,
#                     'shape': shape,
#                     'tag': tag,
#                     'transition_type': 'global'
#                 })
# 
#                 # Add to transition sequence and track patterns
#                 transition_str = f"{self.last_dimensionality}→{dimensionality}"
#                 self.transition_sequence.append(transition_str)
# 
#                 # Track transition patterns (sequence of 3 transitions)
#                 if len(self.transition_sequence) >= 3:
#                     pattern = tuple(list(self.transition_sequence)[-3:])
#                     self.transition_patterns[pattern] = self.transition_patterns.get(pattern, 0) + 1
# 
#                 # Record transition key
#                 transition_key = f"{self.last_dimensionality}D→{dimensionality}D"
#                 self.transition_frequencies[transition_key] = self.transition_frequencies.get(transition_key, 0) + 1
# 
#                 # End current stable period if it was stable
#                 if self.current_stability_count >= self.stability_threshold:
#                     current_time = time.time()
#                     self.stable_periods.append({
#                         'start': self.current_stable_period_start,
#                         'end': current_time,
#                         'duration': current_time - self.current_stable_period_start,
#                         'dimensionality': self.last_dimensionality
#                     })
# 
#                 # Start new stable period tracking
#                 self.current_stable_period_start = time.time()
#                 self.current_stability_count = 0
#                 self.last_dimensionality = dimensionality
# 
#                 # If no event was created yet, create one for global dimensional change
#                 if event is None:
#                     event = EmergenceEvent(
#                         shape=shape,
#                         timestamp=time.time(),
#                         metrics={'global_transition': True, 'tag': tag,
#                                  'transition_type': 'global'}
#                     )
#             else:
#                 # Increment stability counter for same dimensionality
#                 self.current_stability_count += 1
# 
#                 # If we've reached stability threshold, record it
#                 if self.current_stability_count == self.stability_threshold:
#                     print(f"Dimension {dimensionality}D has stabilized")
# 
#             # Update current shape
#             self.current_shape = shape
# 
#             return event
# 
#         except Exception as e:
#             print(f"Error in register_shape: {e}")
#             traceback.print_exc()
#             return None
# 
#     def get_transition_statistics(self) -> Dict:
#         """
#         Get statistics about dimensional transitions with pattern analysis.
# 
#         Returns:
#             Dictionary of transition statistics
#         """
#         try:
#             stats = {
#                 'total_transitions': len(self.shape_transitions),
#                 'transition_frequencies': self.transition_frequencies,
#                 'stable_periods_count': len(self.stable_periods),
#                 'unstable_periods_count': len(self.unstable_periods),
#                 'current_dimensionality': self.last_dimensionality,
#                 'dimensionality_histogram': self.dimensionality_histogram,
#             }
# 
#             # Average stable period duration
#             if self.stable_periods:
#                 avg_stable_duration = sum(period['duration'] for period in self.stable_periods) / len(self.stable_periods)
#                 stats['avg_stable_duration'] = avg_stable_duration
# 
#             # Average unstable period duration
#             if self.unstable_periods:
#                 avg_unstable_duration = sum(period['duration'] for period in self.unstable_periods) / len(self.unstable_periods)
#                 stats['avg_unstable_duration'] = avg_unstable_duration
# 
#             # Most common transition
#             if self.transition_frequencies:
#                 most_common_transition = max(self.transition_frequencies.items(), key=lambda x: x[1])
#                 stats['most_common_transition'] = most_common_transition[0]
#                 stats['most_common_transition_count'] = most_common_transition[1]
# 
#             # Most common transition pattern
#             if self.transition_patterns:
#                 most_common_pattern = max(self.transition_patterns.items(), key=lambda x: x[1])
#                 stats['most_common_pattern'] = most_common_pattern[0]
#                 stats['most_common_pattern_count'] = most_common_pattern[1]
# 
#             # Most common dimensionality
#             if self.dimensionality_histogram:
#                 most_common_dim = max(self.dimensionality_histogram.items(), key=lambda x: x[1])
#                 stats['most_common_dimensionality'] = most_common_dim[0]
#                 stats['most_common_dimensionality_count'] = most_common_dim[1]
# 
#                 # Calculate proportions
#                 total_observations = sum(self.dimensionality_histogram.values())
#                 stats['dimensionality_proportions'] = {
#                     dim: count / total_observations
#                     for dim, count in self.dimensionality_histogram.items()
#                 }
# 
#             return stats
# 
#         except Exception as e:
#             print(f"Error getting transition statistics: {e}")
#             traceback.print_exc()
#             return {'error': str(e)}

"""# 17. Logging Setup"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile logging_setup.py
# import logging
# import os
# import sys
# from datetime import datetime
# import traceback
# 
# def setup_logging(log_dir="logs"):
#     """
#     Configure logging for Émile-2 simulation.
# 
#     This redirects all Qiskit and low-level logs to a file while keeping
#     the important simulation metrics in the console.
# 
#     Args:
#         log_dir: Directory to store log files
#     """
#     # Create logs directory if it doesn't exist
#     os.makedirs(log_dir, exist_ok=True)
# 
#     # Generate log filename based on current date/time
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     log_file = os.path.join(log_dir, f"emile4_sim_{timestamp}.log")
# 
#     # Create root logger
#     root_logger = logging.getLogger()
#     root_logger.setLevel(logging.INFO)
# 
#     # Clear existing handlers to avoid duplicate logs
#     if root_logger.handlers:
#         for handler in root_logger.handlers[:]:
#             root_logger.removeHandler(handler)
# 
#     # File handler for all logs
#     file_handler = logging.FileHandler(log_file)
#     file_handler.setLevel(logging.INFO)
#     file_formatter = logging.Formatter(
#         '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
#     )
#     file_handler.setFormatter(file_formatter)
#     root_logger.addHandler(file_handler)
# 
#     # Console handler for simulation metrics only
#     console_handler = logging.StreamHandler(sys.stdout)
#     console_handler.setLevel(logging.INFO)
#     console_formatter = logging.Formatter('%(message)s')
#     console_handler.setFormatter(console_formatter)
# 
#     # Create logger for simulation metrics
#     sim_logger = logging.getLogger("emile4.simulation")
#     sim_logger.setLevel(logging.INFO)
#     sim_logger.addHandler(console_handler)
# 
#     # Set up specific logger configurations
# 
#     # Qiskit loggers - redirect to file only
#     qiskit_loggers = [
#         "qiskit.passmanager",
#         "qiskit.compiler",
#         "qiskit.qobj",
#         "qiskit.providers",
#         "qiskit.transpiler"
#     ]
# 
#     for logger_name in qiskit_loggers:
#         logger = logging.getLogger(logger_name)
#         logger.setLevel(logging.INFO)
#         logger.propagate = False  # Don't propagate to root logger
#         logger.addHandler(file_handler)
# 
#     # Other detailed module loggers - redirect to file only
#     detail_loggers = [
#         "emile4.utilities",
#         "emile4.core_quantum",
#         "emile4.base_quantum",
#         "emile4.data_classes",
#         "emile4.memory_field",
#         "emile4.analysis",
#         "emile4.transformer_modules"
#     ]
# 
#     for logger_name in detail_loggers:
#         logger = logging.getLogger(logger_name)
#         logger.setLevel(logging.INFO)
#         logger.propagate = False  # Don't propagate to root logger
#         logger.addHandler(file_handler)
# 
#     print(f"Logging configured. Detailed logs will be saved to: {log_file}")
#     return sim_logger

"""# 18. Simulation Runner (Logs)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile simulation_runner_logs.py
# # This will be written to simulation_runner_logs.py
# import asyncio
# import threading
# from queue import Queue
# import time
# import os
# import sys
# import select
# from typing import Optional, Dict, Any, List, Tuple
# import psutil
# import traceback
# import numpy as np
# import torch
# import traceback
# # Import the logging setup
# from logging_setup import setup_logging
# 
# # Import required modules
# from agent_classes import EnhancedSingleAgentFinalEvolution
# from emergence_monitor import EmergenceTracker, DimensionMonitor, EmergenceEvent
# from symbolic_output import SymbolicOutput
# from emergent_potential import EmergentPotentialField
# 
# 
# class InteractiveSimulation:
#     def __init__(self, agent):
#         # Set up logging
#         self.logger = setup_logging()
# 
#         self.agent = agent
#         self.running = True
#         self.command_queue = Queue()
#         self.response_queue = Queue()
#         self.symbolic_system = SymbolicOutput()  # Initialize symbolic output system
# 
#         # Emergence tracking
#         self.dimension_increase_detected = False
#         self.running = True
#         self.command_queue = Queue()
#         self.response_queue = Queue()
# 
#         # Resource monitoring
#         self.resource_history = []
#         self.resource_limit_warnings = {
#             'cpu': False,
#             'memory': False
#         }
# 
#         # Performance thresholds
#         self.cpu_warning_threshold = 85.0
#         self.memory_warning_threshold = 80.0
#         self.cpu_emergency_threshold = 95.0
#         self.memory_emergency_threshold = 90.0
# 
#         # Emergence support
#         self.emergence_tracker = EmergenceTracker()
#         self.dimension_monitor = DimensionMonitor()
#         self.emergence_adaptation_active = False
#         self.dimension_increase_detected = False
#         self.tensor_shape_history = []
#         self.adaptation_params = {
#             'resource_allocation_factor': 1.0,
#             'processing_interval': 0.01,
#             'dimension_support_multiplier': 1.0,
#             'adaptive_step_size': True
#         }
# 
#         # Monitor for emergent dimensions
#         self._patch_agent_for_emergence_detection()
# 
#         # Thread and task references
#         self.sim_thread = None
#         self.sim_task = None
# 
#         # Initialize emergence statistics
#         self.emergence_stats = {
#             'events': 0,
#             'last_timestamp': None,
#             'total_duration': 0,
#             'dimensions_reached': set(),
#             'peak_resource_usage': {
#                 'cpu': 0,
#                 'memory': 0
#             }
#         }
# 
#     def _patch_agent_for_emergence_detection(self):
#         """Patch agent methods to detect emergent dimensions with improved integration."""
#         try:
#             if hasattr(self.agent, "prepare_transformer_input"):
#                 original_prepare = self.agent.prepare_transformer_input
# 
#                 def patched_prepare(*p_args, **p_kwargs):
#                     try:
#                         result = original_prepare(*p_args, **p_kwargs)
# 
#                         # Check for emergence in the result
#                         if isinstance(result, torch.Tensor) and result.dim() > 3:
#                             # Only trigger if agent doesn't already know about it
#                             if not getattr(self.agent, 'dimension_increase_detected', False):
#                                 if hasattr(self.agent, 'handle_emergent_dimension'):
#                                     # Use the agent's own handler if available
#                                     self.agent.handle_emergent_dimension(result.shape, "prepare_transformer_input")
#                                 else:
#                                     # Fallback to simulation runner handling
#                                     self._handle_emergent_dimension(result.shape, "prepare_transformer_input")
# 
#                                 # Share the emergence information with the agent
#                                 self.agent.dimension_increase_detected = True
# 
#                         return result
#                     except Exception as e:
#                         self.logger.error(f"Error in patched prepare_transformer_input: {e}")
#                         traceback.print_exc()
#                         # Return the original tensor if possible, otherwise a safe default
#                         try:
#                             return original_prepare(*p_args, **p_kwargs)
#                         except:
#                             return torch.zeros((1, 1, 20), device=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
# 
#                 self.agent.prepare_transformer_input = patched_prepare
# 
#             # Monitor other tensor-producing methods
#             if hasattr(self.agent, "predict"):
#                 original_predict = self.agent.predict
# 
#                 def patched_predict(*p_args, **p_kwargs):
#                     try:
#                         result = original_predict(*p_args, **p_kwargs)
#                         if isinstance(result, torch.Tensor) and result.dim() > 3:
#                             if hasattr(self.agent, 'handle_emergent_dimension'):
#                                 self.agent.handle_emergent_dimension(result.shape, "predict")
#                             else:
#                                 self._handle_emergent_dimension(result.shape, "predict")
#                         return result
#                     except Exception as e:
#                         self.logger.error(f"Error in patched predict: {e}")
#                         traceback.print_exc()
#                         # Return the original result if possible
#                         try:
#                             return original_predict(*p_args, **p_kwargs)
#                         except:
#                             # Return a safe default
#                             return torch.zeros((1, 1, 1), device=torch.device("cuda" if torch.cuda.is_available() else "cpu"))
# 
#                 self.agent.predict = patched_predict
# 
#             self.logger.info("✅ Agent successfully patched for emergence detection")
# 
#         except Exception as e:
#             self.logger.error(f"Error patching agent for emergence detection: {e}")
#             traceback.print_exc()
# 
#     def _handle_emergent_dimension(self, shape, source="unknown"):
#         """Handle detection of an emergent dimension with better agent integration."""
#         try:
#             if len(shape) > 3 and not self.dimension_increase_detected:
#                 self.dimension_increase_detected = True
#                 self.logger.info(f"\n🌟 EMERGENCE DETECTED: Dimensional expansion to {len(shape)}D")
#                 self.logger.info(f"    Tensor shape: {shape}")
#                 self.logger.info(f"    Source: {source}")
#                 self.logger.info(f"    Émile is evolving its computational ontology!")
# 
#                 # Update emergence statistics
#                 self.emergence_stats['events'] += 1
#                 self.emergence_stats['last_timestamp'] = time.time()
#                 self.emergence_stats['dimensions_reached'].add(len(shape))
# 
#                 # Update agent if agent doesn't have its own handler
#                 if hasattr(self.agent, 'dimension_increase_detected'):
#                     self.agent.dimension_increase_detected = True
# 
#                 # Register with dimension monitor
#                 self.dimension_monitor.register_shape(shape, source)
# 
#                 # Record emergence event
#                 try:
#                     # Get agent metrics if possible
#                     if hasattr(self.agent, 'quantum_state') and hasattr(self.agent.quantum_state, 'get_quantum_metrics'):
#                         metrics = self.agent.quantum_state.get_quantum_metrics()
#                     else:
#                         metrics = {}
# 
#                     # Get resource usage
#                     resource_data = self.monitor_resources()
# 
#                     # Update peak resource usage during emergence
#                     self.emergence_stats['peak_resource_usage']['cpu'] = max(
#                         self.emergence_stats['peak_resource_usage']['cpu'],
#                         resource_data['cpu_percent']
#                     )
#                     self.emergence_stats['peak_resource_usage']['memory'] = max(
#                         self.emergence_stats['peak_resource_usage']['memory'],
#                         resource_data['memory_percent']
#                     )
# 
#                     # Record in emergence tracker
#                     self.emergence_tracker.record_emergence(
#                         tensor_shape=shape,
#                         timestamp=time.time(),
#                         resource_usage=resource_data,
#                         agent_metrics=metrics
#                     )
# 
#                     # Activate emergence support systems
#                     self._activate_emergence_support()
# 
#                     # Connect with emergent potential field if agent has one
#                     if hasattr(self.agent, 'emergent_potential_field'):
#                         try:
#                             # Register emergence with the field
#                             self.agent.emergent_potential_field.register_potential(
#                                 component_id=f"dimensional_emergence_{source}",
#                                 potential=0.8,  # High potential for dimensional emergence
#                                 component_type='quantum',
#                                 state_metrics={
#                                     'tensor_shape': shape,
#                                     'dimensionality': len(shape),
#                                     'source': source,
#                                     'distinction': getattr(self.agent, 'distinction_level', 0.5),
#                                     'coherence': metrics.get('phase_coherence', 0.5)
#                                 }
#                             )
#                             self.logger.info("✅ Emergence registered with potential field")
#                         except Exception as epf_error:
#                             self.logger.error(f"Error registering with emergent potential field: {epf_error}")
# 
#                 except Exception as e:
#                     self.logger.error(f"Error recording emergence event: {e}")
#                     traceback.print_exc()
# 
#                 # Trigger symbolic expression
#                 try:
#                     # Extract parameters from agent if possible
#                     if hasattr(self.agent, 'surplus_dynamics') and hasattr(self.agent.surplus_dynamics, 'surplus_state'):
#                         surplus = self.agent.surplus_dynamics.surplus_state.total_surplus()
#                     else:
#                         surplus = 1.0
# 
#                     if hasattr(self.agent, 'distinction_level'):
#                         distinction = self.agent.distinction_level
#                     else:
#                         distinction = 0.5
# 
#                     if hasattr(self.agent, 'quantum_state') and hasattr(self.agent.quantum_state, 'phase_coherence'):
#                         coherence = self.agent.quantum_state.phase_coherence
#                     else:
#                         coherence = 0.5
# 
#                     # Generate symbolic expression
#                     symbolic_expression = self.symbolic_system.handle_post_emergence(
#                         surplus=surplus,
#                         distinction=distinction,
#                         coherence=coherence,
#                         dimensionality=len(shape),
#                         entropy=metrics.get('normalized_entropy', None) if 'metrics' in locals() else None
#                     )
# 
#                     # Log symbolic expression to console
#                     self.logger.info(f"\n🔹 Symbolic Output Post-Emergence: {symbolic_expression}\n")
# 
#                     # Add pattern analysis if we have multiple emergence events
#                     if len(self.symbolic_system.emergence_events) > 1:
#                         patterns = self.symbolic_system.analyze_emergence_patterns()
#                         self.logger.info("\n📊 Symbolic Pattern Analysis:")
#                         self.logger.info(f"  Emergence Events: {patterns['emergence_count']}")
#                         self.logger.info(f"  Expression Stability: {patterns.get('coherence_stability', 0):.3f}")
#                         if patterns.get('typical_expression'):
#                             self.logger.info(f"  Typical Expression: {patterns['typical_expression']}")
# 
#                     # Return the expression to the agent if it has symbolic history
#                     if hasattr(self.agent, 'symbolic_history'):
#                         self.agent.last_symbolic_expression = symbolic_expression
#                         self.agent.symbolic_history.append({
#                             'expression': symbolic_expression,
#                             'type': 'emergence',
#                             'step': getattr(self.agent, 'step_counter', 0),
#                             'dimensionality': len(shape),
#                             'tensor_shape': shape,
#                             'source': source,
#                             'timestamp': time.time()
#                         })
# 
#                 except Exception as e:
#                     self.logger.error(f"Error generating symbolic expression: {e}")
#                     traceback.print_exc()
# 
#             elif len(shape) <= 3 and self.dimension_increase_detected:
#                 self.dimension_increase_detected = False
#                 self.logger.info("\n📉 Dimensional reduction detected: returning to standard dimensionality")
# 
#                 # Calculate emergence duration
#                 if self.emergence_stats['last_timestamp'] is not None:
#                     duration = time.time() - self.emergence_stats['last_timestamp']
#                     self.emergence_stats['total_duration'] += duration
#                     self.logger.info(f"Emergence duration: {duration:.2f} seconds")
# 
#                 # End the emergence event
#                 try:
#                     # Get final metrics
#                     if hasattr(self.agent, 'quantum_state'):
#                         metrics = self.agent.quantum_state.get_quantum_metrics()
#                     else:
#                         metrics = {}
# 
#                     self.emergence_tracker.end_emergence(
#                         timestamp=time.time(),
#                         metrics=metrics
#                     )
#                 except Exception as end_error:
#                     self.logger.error(f"Error ending emergence: {end_error}")
# 
#                 # Deactivate emergence support
#                 self._deactivate_emergence_support()
# 
#                 # Update agent if needed
#                 if hasattr(self.agent, 'dimension_increase_detected'):
#                     self.agent.dimension_increase_detected = False
# 
#         except Exception as e:
#             self.logger.error(f"Error handling emergent dimension: {e}")
#             traceback.print_exc()
# 
# 
#     def _get_symbolic_analysis(self) -> str:
#         """Generate a comprehensive analysis of symbolic expressions."""
#         try:
#             if not hasattr(self, 'symbolic_system') or not self.symbolic_system.emergence_events:
#                 return "No symbolic expressions have been generated yet."
# 
#             patterns = self.symbolic_system.analyze_emergence_patterns()
# 
#             # Format the analysis as a multi-line string
#             analysis = [
#                 "\n🔮 Symbolic Expression Analysis",
#                 "==============================",
#                 f"Total Emergence Events: {patterns['emergence_count']}",
#                 f"Total Expressions Generated: {patterns['expression_count']}",
#                 f"Coherence Stability: {patterns.get('coherence_stability', 0):.4f}",
#                 f"Distinction Stability: {patterns.get('distinction_stability', 0):.4f}",
#                 "\nDominant Patterns:",
#             ]
# 
#             dominant = patterns.get('dominant_patterns', {})
#             if dominant:
#                 analysis.extend([
#                     f"  Primary Descriptor: {dominant.get('descriptor', 'None')}",
#                     f"  Primary Relation: {dominant.get('relation', 'None')}",
#                     f"  Primary Concept: {dominant.get('concept', 'None')}"
#                 ])
# 
#             if patterns.get('typical_expression'):
#                 analysis.extend([
#                     "\nTypical Expression:",
#                     f"  {patterns['typical_expression']}"
#                 ])
# 
#             # Add recent expressions
#             recent_expressions = self.symbolic_system.expression_history[-3:] if self.symbolic_system.expression_history else []
#             if recent_expressions:
#                 analysis.extend([
#                     "\nRecent Expressions:",
#                 ])
#                 for i, expr in enumerate(recent_expressions):
#                     analysis.append(f"  {i+1}. {expr['expression']}")
# 
#             return "\n".join(analysis)
#         except Exception as e:
#             self.logger.error(f"Error generating symbolic analysis: {e}")
#             return f"Error generating symbolic analysis: {e}"
# 
#     def monitor_resources(self):
#         """Monitors and logs CPU and memory usage."""
#         try:
#             cpu_percent = psutil.cpu_percent()
#             memory_info = psutil.virtual_memory()
# 
#             resource_data = {
#                 'timestamp': time.time(),
#                 'cpu_percent': cpu_percent,
#                 'memory_percent': memory_info.percent,
#                 'memory_available': memory_info.available / (1024 * 1024)  # MB
#             }
# 
#             # Store historical data
#             self.resource_history.append(resource_data)
#             if len(self.resource_history) > 1000:  # Keep last 1000 measurements
#                 self.resource_history.pop(0)
# 
#             # Check for resource warnings
#             self._check_resource_warnings(resource_data)
# 
#             return resource_data
#         except Exception as e:
#             self.logger.error(f"Error monitoring resources: {e}")
#             return {
#                 'cpu_percent': 0.0,
#                 'memory_percent': 0.0,
#                 'memory_available': 0.0
#             }
# 
#     def _check_resource_warnings(self, resource_data):
#         """Check if resource usage exceeds thresholds and issue warnings."""
#         try:
#             # CPU warning
#             if resource_data['cpu_percent'] > self.cpu_warning_threshold and not self.resource_limit_warnings['cpu']:
#                 self.logger.info(f"\n⚠️ WARNING: High CPU usage detected ({resource_data['cpu_percent']:.1f}%)")
#                 if self.emergence_tracker.is_emergence_active:
#                     self.logger.info("  This may be related to the ongoing emergence process")
#                     self._adjust_for_high_resource_usage()
#                 self.resource_limit_warnings['cpu'] = True
#             elif resource_data['cpu_percent'] < self.cpu_warning_threshold - 10 and self.resource_limit_warnings['cpu']:
#                 self.resource_limit_warnings['cpu'] = False
#                 if self.emergence_tracker.is_emergence_active:
#                     self._restore_normal_processing()
# 
#             # Memory warning
#             if resource_data['memory_percent'] > self.memory_warning_threshold and not self.resource_limit_warnings['memory']:
#                 self.logger.info(f"\n⚠️ WARNING: High memory usage detected ({resource_data['memory_percent']:.1f}%)")
#                 if self.emergence_tracker.is_emergence_active:
#                     self.logger.info("  This may be related to ongoing emergence processing")
#                     self._adjust_for_high_resource_usage()
#                 self.resource_limit_warnings['memory'] = True
#             elif resource_data['memory_percent'] < self.memory_warning_threshold - 10 and self.resource_limit_warnings['memory']:
#                 self.resource_limit_warnings['memory'] = False
#                 if self.emergence_tracker.is_emergence_active:
#                     self._restore_normal_processing()
# 
#             # Emergency thresholds
#             if resource_data['cpu_percent'] > self.cpu_emergency_threshold:
#                 self.logger.warning(f"\n🚨 CRITICAL: CPU usage at {resource_data['cpu_percent']:.1f}%")
#                 self.logger.warning("  Applying emergency resource management")
#                 self._apply_emergency_resource_management()
# 
#             if resource_data['memory_percent'] > self.memory_emergency_threshold:
#                 self.logger.warning(f"\n🚨 CRITICAL: Memory usage at {resource_data['memory_percent']:.1f}%")
#                 self.logger.warning("  Applying emergency resource management")
#                 self._apply_emergency_resource_management()
# 
#         except Exception as e:
#             self.logger.error(f"Error checking resource warnings: {e}")
# 
#     def _adjust_for_high_resource_usage(self):
#         """Adapt to high resource usage during emergence."""
#         try:
#             # Increase processing interval for breathing room
#             self.adaptation_params['processing_interval'] = min(
#                 0.05, self.adaptation_params['processing_interval'] * 1.5
#             )
#             # Reduce resource allocation factor
#             self.adaptation_params['resource_allocation_factor'] = 0.8
#             self.logger.info(f"⚙️ Adapting to high resource usage during emergence:")
#             self.logger.info(f"  - Adjusted processing interval: {self.adaptation_params['processing_interval']:.3f}s")
#             self.logger.info(f"  - Resource allocation factor: {self.adaptation_params['resource_allocation_factor']:.2f}")
#         except Exception as e:
#             self.logger.error(f"Error adjusting for high resource usage: {e}")
# 
#     def _restore_normal_processing(self):
#         """Restore normal processing parameters."""
#         try:
#             self.adaptation_params['processing_interval'] = max(
#                 0.01, self.adaptation_params['processing_interval'] * 0.8
#             )
#             self.adaptation_params['resource_allocation_factor'] = min(
#                 1.0, self.adaptation_params['resource_allocation_factor'] * 1.1
#             )
#         except Exception as e:
#             self.logger.error(f"Error restoring normal processing: {e}")
# 
#     def _apply_emergency_resource_management(self):
#         """Apply emergency resource management to prevent crashes."""
#         try:
#             # Drastically increase sleep time
#             self.adaptation_params['processing_interval'] = 0.2
#             # Reduce resource allocation to minimum
#             self.adaptation_params['resource_allocation_factor'] = 0.5
# 
#             # Force garbage collection
#             import gc
#             gc.collect()
# 
#             # Clear any unnecessary caches
#             if hasattr(self.agent, 'analysis_history') and isinstance(self.agent.analysis_history, dict):
#                 for k in list(self.agent.analysis_history.keys()):
#                     if hasattr(self.agent.analysis_history[k], 'clear'):
#                         self.agent.analysis_history[k].clear()
# 
#             # Clear tensor memory if torch is available
#             if torch.cuda.is_available():
#                 torch.cuda.empty_cache()
# 
#             self.logger.info("🧹 Applied emergency resource management")
#         except Exception as e:
#             self.logger.error(f"Error applying emergency resource management: {e}")
# 
#     def _activate_emergence_support(self):
#         """Activate support systems for emergence."""
#         try:
#             if self.emergence_adaptation_active:
#                 return
#             self.emergence_adaptation_active = True
#             self.logger.info("\n🔄 Activating emergence support systems")
# 
#             # Adjust parameters to support emergence
#             self.adaptation_params['processing_interval'] = 0.02
#             self.adaptation_params['dimension_support_multiplier'] = 1.2
# 
#             # Track stability during emergence
#             if hasattr(self.agent, 'stability_factor'):
#                 self.emergence_tracker.update_stability(self.agent.stability_factor, time.time())
#         except Exception as e:
#             self.logger.error(f"Error activating emergence support: {e}")
# 
#     def _deactivate_emergence_support(self):
#         """Deactivate emergence support systems."""
#         try:
#             if not self.emergence_adaptation_active:
#                 return
#             self.emergence_adaptation_active = False
#             self.logger.info("\n✓ Deactivating emergence support systems")
# 
#             # Reset parameters
#             self.adaptation_params['processing_interval'] = 0.01
#             self.adaptation_params['dimension_support_multiplier'] = 1.0
# 
#             # Generate emergence summary
#             summary = self.emergence_tracker.get_emergence_summary()
#             detailed = self.emergence_tracker.get_detailed_analysis()
# 
#             self.logger.info("\n📑 Emergence Event Summary:")
#             self.logger.info(f"  Total Events: {summary.get('emergence_count', 0)}")
#             self.logger.info(f"  Duration: {summary.get('total_duration', 0):.2f} seconds")
#             self.logger.info(f"  Dimension Transitions: {summary.get('dimension_transitions', 0)}")
#             self.logger.info(f"  Pattern Diversity: {summary.get('pattern_diversity', 0)}")
#             self.logger.info(f"  Resource Impact: {summary.get('resource_impact', 0):.2f}% CPU")
# 
#             if detailed.get('dominant_pattern'):
#                 self.logger.info(f"  Dominant Pattern: {detailed['dominant_pattern']}")
#         except Exception as e:
#             self.logger.error(f"Error deactivating emergence support: {e}")
# 
#     async def run_simulation(self):
#         """Run the simulation while handling interactive commands."""
#         try:
#             self.logger.info("\n🚀 Starting Interactive Émile-3K Simulation...")
#             step = 0
#             last_resource_log = time.time()
#             resource_log_interval = 5  # Log every 5 seconds
# 
#             # Store task reference
#             self.sim_task = asyncio.current_task()
# 
#             while self.running:
#                 step_start_time = time.time()
# 
#                 # Process any pending commands
#                 while not self.command_queue.empty():
#                     command = self.command_queue.get()
#                     await self.handle_command(command)
# 
#                 # Execute simulation step
#                 try:
#                     step_success = self.agent.step()
#                     if not step_success:
#                         self.logger.warning(f"\n⚠️ Agent step returned unsuccessful status at step {step}")
#                         # Apply small delay after unsuccessful step
#                         await asyncio.sleep(0.1)
#                 except Exception as step_error:
#                     self.logger.error(f"\n❌ Error in agent step: {step_error}")
#                     traceback.print_exc()  # Goes to log file
#                     await asyncio.sleep(0.1)
#                     continue
# 
#                 # Get metrics silently (don't log them)
#                 try:
#                     metrics = {
#                         'distinction_level': float(self.agent.distinction_level),
#                         'coherence': float(self.agent.quantum_state.phase_coherence),
#                         'entropy': float(self.agent.quantum_state.get_quantum_metrics().get('normalized_entropy', 0)),
#                         'stability': float(self.agent.stability_factor),
#                         'surplus_stability': float(self.agent.surplus_dynamics.surplus_state.stability)
#                     }
# 
#                     # Check for dimensional changes without logging
#                     if hasattr(self.agent, 'prepare_transformer_input'):
#                         try:
#                             input_tensor = self.agent.prepare_transformer_input()
#                             if input_tensor is not None and isinstance(input_tensor, torch.Tensor):
#                                 event = self.dimension_monitor.register_shape(input_tensor, "regular_progress_check")
#                                 if event:
#                                     # Process emergence event
#                                     self.logger.info(f"\n🔍 New tensor shape detected: {input_tensor.shape}")
#                         except Exception:
#                             pass  # silent
# 
#                     # Update stability tracking during emergence
#                     if self.emergence_tracker.is_emergence_active:
#                         self.emergence_tracker.update_stability(metrics['stability'], time.time())
# 
#                     # Check for emergent potential field events
#                     if (hasattr(self.agent, 'emergent_potential_field') and
#                         step % 10 == 0):  # Check every 10 steps to avoid overhead
#                         try:
#                             field_state = self.agent.emergent_potential_field.get_field_state()
#                             # Log high emergence probability
#                             if field_state.get('emergence_probability', 0) > 0.8:
#                                 self.logger.info(f"\n⚡ High emergence potential detected: {field_state['emergence_probability']:.2f}")
#                             # Log active emergence
#                             if field_state.get('emergence_active', False) and not getattr(self, 'logged_field_emergence', False):
#                                 self.logger.info(f"\n🌠 Emergent potential field has activated emergence")
#                                 setattr(self, 'logged_field_emergence', True)
#                             # Reset field emergence log flag if no longer active
#                             elif not field_state.get('emergence_active', False) and getattr(self, 'logged_field_emergence', False):
#                                 setattr(self, 'logged_field_emergence', False)
#                         except Exception as field_error:
#                             pass  # silent
# 
#                 except Exception as metrics_error:
#                     metrics = {
#                         'distinction_level': 0.0,
#                         'coherence': 0.0,
#                         'entropy': 0.0,
#                         'stability': 0.0,
#                         'surplus_stability': 0.0
#                     }
# 
#                 # Log progress periodically
#                 if step % 50 == 0:
#                     self._log_progress(step, metrics)
# 
#                 # Monitor resources periodically
#                 current_time = time.time()
#                 if current_time - last_resource_log >= resource_log_interval:
#                     resource_usage = self.monitor_resources()
#                     self._log_resources(resource_usage)
#                     last_resource_log = current_time
# 
#                 step += 1
# 
#                 # Adaptive sleep interval based on emergence
#                 sleep_time = self.adaptation_params['processing_interval']
#                 if self.emergence_adaptation_active:
#                     # Dynamic adjustment if steps are taking a while
#                     step_duration = time.time() - step_start_time
#                     if step_duration > 0.1 and self.adaptation_params['adaptive_step_size']:
#                         sleep_time = min(0.05, sleep_time * 1.2)
# 
#                 await asyncio.sleep(sleep_time)
# 
#         except asyncio.CancelledError:
#             self.logger.info("\n⚠️ Simulation task cancelled")
#         except Exception as e:
#             self.logger.error(f"\n❌ Error in simulation: {e}")
#             traceback.print_exc()  # Goes to log file
#         finally:
#             self.logger.info("\n✨ Simulation ended")
#             # Final resource report
#             self.logger.info("\n📊 Final Resource Usage Report:")
#             if self.resource_history:
#                 avg_cpu = sum(r['cpu_percent'] for r in self.resource_history) / len(self.resource_history)
#                 avg_memory = sum(r['memory_percent'] for r in self.resource_history) / len(self.resource_history)
#                 peak_cpu = max(r['cpu_percent'] for r in self.resource_history)
#                 peak_memory = max(r['memory_percent'] for r in self.resource_history)
# 
#                 self.logger.info(f"  Average CPU: {avg_cpu:.1f}%")
#                 self.logger.info(f"  Average Memory: {avg_memory:.1f}%")
#                 self.logger.info(f"  Peak CPU: {peak_cpu:.1f}%")
#                 self.logger.info(f"  Peak Memory: {peak_memory:.1f}%")
# 
#             # Final emergence report
#             if self.emergence_tracker.emergence_events:
#                 summary = self.emergence_tracker.get_emergence_summary()
#                 self.logger.info("\n🌟 Final Emergence Report:")
#                 self.logger.info(f"  Total Emergence Events: {summary.get('emergence_count', 0)}")
#                 self.logger.info(f"  Dimension Transitions: {summary.get('dimension_transitions', 0)}")
#                 self.logger.info(f"  Total Duration: {summary.get('total_duration', 0):.2f} seconds")
#                 self.logger.info(f"  Pattern Diversity: {summary.get('pattern_diversity', 0)}")
# 
#                 detailed = self.emergence_tracker.get_detailed_analysis()
#                 if detailed.get('dominant_pattern'):
#                     dominant = detailed['dominant_pattern']
#                     pattern_details = detailed['pattern_details'].get(dominant, {})
#                     self.logger.info(f"\n  Dominant Pattern: {dominant}")
#                     self.logger.info(f"    Occurrences: {pattern_details.get('count', 0)}")
#                     self.logger.info(f"    Avg CPU Impact: {pattern_details.get('avg_cpu_impact', 0):.2f}%")
#                     self.logger.info(f"    Stability Mean: {pattern_details.get('stability_mean', 0):.4f}")
#                     self.logger.info(f"    Coherence Mean: {pattern_details.get('coherence_mean', 0):.4f}")
# 
#     def _log_progress(self, step: int, metrics: Dict[str, float]):
#         """Log simulation progress for each step interval."""
#         self.logger.info(
#             f"\n[Step {step}] "
#             f"Distinction: {metrics.get('distinction_level', 0.0):.3f}, "
#             f"Coherence: {metrics.get('coherence', 0.0):.3f}, "
#             f"Entropy: {metrics.get('entropy', 0.0):.3f}, "
#             f"Stability: {metrics.get('stability', 0.0):.3f}, "
#             f"SurplusStab: {metrics.get('surplus_stability', 0.0):.3f}"
#         )
# 
#         # Log emergent potential field data if available
#         if hasattr(self.agent, 'emergent_potential_field') and step % 100 == 0:
#             try:
#                 field_state = self.agent.emergent_potential_field.get_field_state()
#                 self.logger.info(
#                     f"Emergent Potential: {field_state.get('total_potential', 0.0):.3f}, "
#                     f"Threshold: {field_state.get('threshold', 0.0):.3f}, "
#                     f"Prob: {field_state.get('emergence_probability', 0.0):.2f}"
#                 )
#             except Exception as e:
#                 pass  # Silent fail for emergent field logging
# 
#     def _log_resources(self, resource_usage: Dict[str, float]):
#         """Log resource usage each time we poll it (default every 5s)."""
#         self.logger.info(
#             f"Resource Usage => CPU: {resource_usage['cpu_percent']:.1f}%, "
#             f"Memory: {resource_usage['memory_percent']:.1f}%, "
#             f"Avail: {resource_usage['memory_available']:.0f} MB"
#         )
# 
#     async def handle_command(self, command: str):
#         """Process user commands and generate responses."""
#         try:
#             command = command.lower().strip()
# 
#             if command == "status":
#                 metrics = {
#                     'distinction': getattr(self.agent, 'distinction_level', 0),
#                     'coherence': getattr(self.agent.quantum_state, 'phase_coherence', 0)
#                         if hasattr(self.agent, 'quantum_state') else 0,
#                     'stability': getattr(self.agent, 'stability_factor', 0),
#                     'surplus_stability': getattr(self.agent.surplus_dynamics.surplus_state, 'stability', 0)
#                         if (hasattr(self.agent, 'surplus_dynamics') and
#                             hasattr(self.agent.surplus_dynamics, 'surplus_state')) else 0,
#                     'surplus_values': getattr(self.agent.surplus_dynamics.surplus_state, 'values', {})
#                         if (hasattr(self.agent, 'surplus_dynamics') and
#                             hasattr(self.agent.surplus_dynamics, 'surplus_state')) else {},
#                     'steps': getattr(self.agent, 'step_counter', 0)
#                 }
# 
#                 # Add emergent potential field data
#                 if hasattr(self.agent, 'emergent_potential_field'):
#                     try:
#                         field_state = self.agent.emergent_potential_field.get_field_state()
#                         metrics['emergent_potential'] = field_state.get('total_potential', 0.0)
#                         metrics['emergence_probability'] = field_state.get('emergence_probability', 0.0)
#                         metrics['emergence_active'] = field_state.get('emergence_active', False)
#                     except Exception:
#                         pass  # Silent fail
# 
#                 self.response_queue.put(f"\nCurrent Status:\n{self._format_metrics(metrics)}")
# 
#             elif command == "emergence":
#                 # Enhanced emergence command
#                 emergence_metrics = {}
#                 if (hasattr(self.agent, 'surplus_dynamics') and
#                     hasattr(self.agent.surplus_dynamics, 'get_emergence_metrics')):
#                     emergence_metrics = self.agent.surplus_dynamics.get_emergence_metrics()
# 
#                 tracker_metrics = self.emergence_tracker.get_emergence_summary()
#                 if tracker_metrics:
#                     emergence_metrics['dimensional_emergence'] = tracker_metrics
# 
#                 detailed_analysis = self.emergence_tracker.get_detailed_analysis()
#                 if detailed_analysis and detailed_analysis.get('total_patterns', 0) > 0:
#                     emergence_metrics['pattern_analysis'] = detailed_analysis
# 
#                 # Add dimension monitor statistics
#                 if self.dimension_monitor.dimension_changes:
#                     emergence_metrics['dimension_changes'] = len(self.dimension_monitor.dimension_changes)
#                     emergence_metrics['last_dimensionality'] = self.dimension_monitor.last_dimensionality
# 
#                 # Add emergent potential field data
#                 if hasattr(self.agent, 'emergent_potential_field'):
#                     try:
#                         field_state = self.agent.emergent_potential_field.get_field_state()
#                         emergence_metrics['potential_field'] = {
#                             'total_potential': field_state.get('total_potential', 0.0),
#                             'threshold': field_state.get('threshold', 0.0),
#                             'emergence_probability': field_state.get('emergence_probability', 0.0),
#                             'emergence_active': field_state.get('emergence_active', False)
#                         }
#                     except Exception as e:
#                         emergence_metrics['potential_field_error'] = str(e)
# 
#                 # Add statistics from our tracker
#                 emergence_metrics['stats'] = {
#                     'events': self.emergence_stats['events'],
#                     'dimensions_reached': list(self.emergence_stats['dimensions_reached']),
#                     'total_duration': self.emergence_stats['total_duration'],
#                     'peak_resource_usage': self.emergence_stats['peak_resource_usage']
#                 }
# 
#                 if emergence_metrics:
#                     self.response_queue.put(f"\nEmergence Metrics:\n{self._format_metrics(emergence_metrics)}")
#                 else:
#                     self.response_queue.put("\nNo emergence metrics available yet.")
# 
#             elif command == "resources":
#                 # Enhanced resource command
#                 if self.resource_history:
#                     latest = self.resource_history[-1]
#                     avg_cpu = sum(r['cpu_percent'] for r in self.resource_history) / len(self.resource_history)
#                     avg_memory = sum(r['memory_percent'] for r in self.resource_history) / len(self.resource_history)
#                     peak_cpu = max(r['cpu_percent'] for r in self.resource_history)
#                     peak_memory = max(r['memory_percent'] for r in self.resource_history)
# 
#                     emergence_info = ""
#                     if self.emergence_tracker.is_emergence_active:
#                         emergence_summary = self.emergence_tracker.get_emergence_summary()
#                         emergence_info = (
#                             f"\n\nEmergence Resource Impact:"
#                             f"\n  CPU Correlation: {emergence_summary.get('resource_correlation', 0):.3f}"
#                             f"\n  Average Impact: {emergence_summary.get('resource_impact', 0):.2f}%"
#                             f"\n  Adaptation Multiplier: {self.adaptation_params['dimension_support_multiplier']:.2f}"
#                             f"\n  Processing Interval: {self.adaptation_params['processing_interval']:.3f}s"
#                         )
# 
#                     resource_report = (
#                         f"\nResource Usage:"
#                         f"\n  Current CPU: {latest['cpu_percent']:.1f}%"
#                         f"\n  Current Memory: {latest['memory_percent']:.1f}%"
#                         f"\n  Available Memory: {latest['memory_available']:.0f} MB"
#                         f"\n  Average CPU: {avg_cpu:.1f}%"
#                         f"\n  Average Memory: {avg_memory:.1f}%"
#                         f"\n  Peak CPU: {peak_cpu:.1f}%"
#                         f"\n  Peak Memory: {peak_memory:.1f}%"
#                         f"{emergence_info}"
#                     )
#                     self.response_queue.put(resource_report)
#                 else:
#                     self.response_queue.put("\nNo resource data available yet.")
# 
#             elif command == "patterns":
#                 # Show emergence patterns
#                 pattern_analysis = self.emergence_tracker.get_detailed_analysis()
#                 if pattern_analysis and pattern_analysis.get('total_patterns', 0) > 0:
#                     pattern_report = ["\nEmergence Pattern Analysis:"]
#                     pattern_report.append(f"Total Patterns: {pattern_analysis['total_patterns']}")
#                     pattern_report.append(f"Total Events: {pattern_analysis['total_events']}")
#                     pattern_report.append(f"Dominant Pattern: {pattern_analysis.get('dominant_pattern', 'None')}")
# 
#                     if pattern_analysis.get('common_sequences'):
#                         pattern_report.append(f"Common Sequence: {pattern_analysis['common_sequences']}")
#                         pattern_report.append(f"Sequence Frequency: {pattern_analysis['sequence_frequency']}")
# 
#                     pattern_report.append("\nPattern Details:")
# 
#                     # Sort patterns by count for better display
#                     sorted_patterns = sorted(
#                         pattern_analysis.get('pattern_details', {}).items(),
#                         key=lambda x: x[1].get('count', 0),
#                         reverse=True
#                     )
# 
#                     for shape, details in sorted_patterns:
#                         pattern_report.append(f"\n  Pattern: {shape}")
#                         pattern_report.append(f"    Count: {details.get('count', 0)}")
#                         pattern_report.append(f"    Duration: {details.get('duration', 0):.2f}s")
#                         pattern_report.append(f"    Avg CPU Impact: {details.get('avg_cpu_impact', 0):.2f}%")
#                         pattern_report.append(f"    Stability Mean: {details.get('stability_mean', 0):.4f}")
#                         pattern_report.append(f"    Coherence Mean: {details.get('coherence_mean', 0):.4f}")
#                         pattern_report.append(f"    Distinction Mean: {details.get('distinction_mean', 0):.4f}")
# 
#                     self.response_queue.put("\n".join(pattern_report))
#                 else:
#                     self.response_queue.put("\nNo emergence patterns detected yet.")
# 
#             elif command == "dimensions":
#                 # Show dimensional emergence details
#                 dim_changes = self.dimension_monitor.dimension_changes
#                 dim_transitions = self.dimension_monitor.shape_transitions
#                 dim_stats = self.dimension_monitor.get_transition_statistics()
# 
#                 if dim_changes or dim_transitions:
#                     dim_report = ["\nDimensional Emergence Analysis:"]
#                     dim_report.append(f"Current Dimensionality: {self.dimension_monitor.last_dimensionality}D")
#                     dim_report.append(f"Total Dimension Changes: {len(dim_changes)}")
#                     dim_report.append(f"Total Shape Transitions: {len(dim_transitions)}")
# 
#                     # Add statistics
#                     if dim_stats:
#                         dim_report.append(f"Most Common Dimensionality: {dim_stats.get('most_common_dimensionality', 'Unknown')}")
#                         dim_report.append(f"Most Common Transition: {dim_stats.get('most_common_transition', 'Unknown')}")
# 
#                         if 'dimensionality_proportions' in dim_stats:
#                             dim_report.append("\nDimensionality Distribution:")
#                             for dim, prop in dim_stats['dimensionality_proportions'].items():
#                                 dim_report.append(f"  {dim}D: {prop*100:.1f}%")
# 
#                     # Recent transitions
#                     if dim_transitions:
#                         dim_report.append("\nRecent Transitions:")
#                         for i, transition in enumerate(dim_transitions[-5:]):
#                             dim_report.append(f"  {i+1}. {transition['old_dimensionality']}D → {transition['new_dimensionality']}D")
#                             dim_report.append(f"     Shape: {transition['shape']}")
#                             dim_report.append(f"     Source: {transition['tag']}")
#                             if 'timestamp' in transition:
#                                 time_str = time.strftime('%H:%M:%S', time.localtime(transition['timestamp']))
#                                 dim_report.append(f"     Time: {time_str}")
# 
#                     self.response_queue.put("\n".join(dim_report))
#                 else:
#                     self.response_queue.put("\nNo dimensional changes detected yet.")
# 
#             elif command == "symbolic":
#                 # Get symbolic analysis
#                 analysis = self._get_symbolic_analysis()
#                 self.response_queue.put(analysis)
# 
#             elif command == "potential":
#                 # Get emergent potential field data
#                 if hasattr(self.agent, 'emergent_potential_field'):
#                     try:
#                         field_state = self.agent.emergent_potential_field.get_field_state()
#                         field_data = self.agent.get_emergent_potential_visualization()
# 
#                         field_report = ["\nEmergent Potential Field:"]
#                         field_report.append(f"Total Potential: {field_state.get('total_potential', 0.0):.4f}")
#                         field_report.append(f"Threshold: {field_state.get('threshold', 0.0):.4f}")
#                         field_report.append(f"Emergence Probability: {field_state.get('emergence_probability', 0.0):.4f}")
#                         field_report.append(f"Field Intensity: {field_state.get('field_intensity', 1.0):.2f}")
#                         field_report.append(f"Stability Factor: {field_state.get('stability_factor', 1.0):.2f}")
#                         field_report.append(f"Emergence Active: {'Yes' if field_state.get('emergence_active', False) else 'No'}")
#                         field_report.append(f"Emergence Count: {field_state.get('emergence_count', 0)}")
# 
#                         # Add component information
#                         if 'components' in field_data and field_data['components']:
#                             field_report.append("\nTop Components:")
#                             for i, comp in enumerate(field_data['components'][:5]):  # Show top 5
#                                 field_report.append(f"  {i+1}. {comp['id']} ({comp['type']}): {comp['potential']:.4f}")
# 
#                         # Add emergence events
#                         if 'emergence_events' in field_data and field_data['emergence_events']:
#                             field_report.append("\nRecent Emergence Events:")
#                             for i, event in enumerate(field_data['emergence_events'][-3:]):  # Show latest 3
#                                 field_report.append(f"  {i+1}. Intensity: {event['intensity']:.2f}, Potential: {event['potential']:.4f}")
# 
#                         self.response_queue.put("\n".join(field_report))
#                     except Exception as e:
#                         self.response_queue.put(f"\nError getting emergent potential field data: {e}")
#                 else:
#                     self.response_queue.put("\nEmergent potential field not available.")
# 
#             elif command == "help":
#                 help_text = """
#                 Available Commands:
#                 - status: Get current agent metrics
#                 - emergence: Get detailed emergence metrics and analysis
#                 - resources: Get current resource usage
#                 - patterns: Show emergence pattern analysis
#                 - dimensions: Show dimensional emergence details
#                 - symbolic: Show analysis of symbolic expressions and emergence patterns
#                 - potential: Show emergent potential field status and components
#                 - help: Show this help message
#                 - exit: Stop the simulation
#                 """
#                 self.response_queue.put(help_text)
# 
#             elif command == "exit":
#                 self.running = False
#                 self.response_queue.put("Shutting down simulation...")
# 
#             else:
#                 self.response_queue.put(f"Unknown command: {command}")
# 
#         except Exception as e:
#             self.response_queue.put(f"Error processing command: {e}")
#             traceback.print_exc()
# 
#     def _format_metrics(self, metrics: Dict[str, Any], indent: int = 0) -> str:
#         """Format a dictionary of metrics into a multi-line string for display."""
#         if not metrics:
#             return "(No metrics to display)"
# 
#         lines = []
#         indent_str = " " * indent
# 
#         for k, v in metrics.items():
#             if isinstance(v, dict):
#                 # Recursively format nested dictionaries with indentation
#                 lines.append(f"{indent_str}{k}:")
#                 lines.append(self._format_metrics(v, indent + 2))
#             elif isinstance(v, list) and len(v) > 0 and isinstance(v[0], dict):
#                 # Handle lists of dictionaries (like history items)
#                 lines.append(f"{indent_str}{k}:")
#                 for i, item in enumerate(v[:3]):  # Show only first 3 items
#                     lines.append(f"{indent_str}  Item {i+1}:")
#                     lines.append(self._format_metrics(item, indent + 4))
#                 if len(v) > 3:
#                     lines.append(f"{indent_str}  ... ({len(v) - 3} more items)")
#             elif isinstance(v, (int, float)) and not isinstance(v, bool):
#                 # Format numbers with proper precision
#                 if abs(v) < 0.01 or abs(v) > 1000:
#                     lines.append(f"{indent_str}{k}: {v:.6g}")
#                 else:
#                     lines.append(f"{indent_str}{k}: {v:.4f}")
#             else:
#                 # Format everything else
#                 lines.append(f"{indent_str}{k}: {v}")
# 
#         return "\n".join(lines)
# 
# def visualize_emergence_potential(agent, display_type='potential'):
#     """Helper function to display emergence potential visualization in Colab"""
#     try:
#         import matplotlib.pyplot as plt
# 
#         if hasattr(agent, 'visualize_emergent_field'):
#             fig = agent.visualize_emergent_field(display_type)
#             if fig:
#                 plt.show()
#         elif hasattr(agent, 'get_emergent_potential_visualization'):
#             data = agent.get_emergent_potential_visualization()
#             print(f"Emergent Potential Field Status:")
#             print(f"- Total Potential: {data['field_state']['total_potential']:.4f}")
#             print(f"- Threshold: {data['field_state']['threshold']:.4f}")
#             print(f"- Emergence Probability: {data['field_state'].get('emergence_probability', 0.0):.4f}")
#             print(f"- Emergence Active: {'Yes' if data['field_state']['emergence_active'] else 'No'}")
#             print(f"- Components: {len(data['components'])}")
#             print(f"- Emergence Events: {len(data['emergence_events'])}")
# 
#             # Create a simple bar chart of components
#             if 'components' in data and data['components']:
#                 plt.figure(figsize=(10, 6))
#                 components = data['components'][:10]  # Top 10 components
#                 labels = [f"{c['id'][:10]}..." if len(c['id']) > 10 else c['id'] for c in components]
#                 values = [c['potential'] for c in components]
#                 colors = ['blue' if c['type'] == 'surplus' else
#                           'green' if c['type'] == 'quantum' else
#                           'orange' if c['type'] == 'cognitive' else 'gray'
#                           for c in components]
# 
#                 plt.bar(range(len(components)), values, color=colors)
#                 plt.xticks(range(len(components)), labels, rotation=45, ha='right')
#                 plt.title('Top Contributors to Emergent Potential')
#                 plt.ylabel('Potential')
#                 plt.tight_layout()
#                 plt.show()
#         else:
#             print("Agent does not have emergence potential visualization capabilities")
#     except ImportError:
#         print("Matplotlib is required for visualization")
#     except Exception as e:
#         print(f"Error in visualization: {e}")
#         traceback.print_exc()
# 
# def run_interactive_simulation(agent):
#     """Run the simulation with interactive command handling."""
#     simulation = InteractiveSimulation(agent)
# 
#     try:
#         # Create event loop
#         loop = asyncio.new_event_loop()
#         asyncio.set_event_loop(loop)
# 
#         # Start simulation in separate thread
#         sim_thread = threading.Thread(
#             target=lambda: loop.run_until_complete(simulation.run_simulation()),
#             daemon=True
#         )
#         sim_thread.start()
#         simulation.sim_thread = sim_thread
# 
#         simulation.logger.info("\nInteractive Émile-3K Simulation")
#         simulation.logger.info("Type 'help' for available commands")
#         simulation.logger.info("Use 'emergence' to analyze dimensional emergence")
# 
#         # Main input loop
#         while simulation.running:
#             try:
#                 command = None  # Initialize command at the start of each loop
# 
#                 # Use more robust input handling
#                 if sys.platform != 'win32':
#                     # For Unix systems, use select for non-blocking input
#                     readable, _, _ = select.select([sys.stdin], [], [], 0.1)
#                     if readable:
#                         command = sys.stdin.readline().strip()
#                         if command:
#                             simulation.command_queue.put(command)
#                 else:
#                     # For Windows, just use regular input with occasional checks for simulation status
#                     try:
#                         command = input(">> ")
#                         if command:
#                             simulation.command_queue.put(command)
#                     except EOFError:
#                         simulation.running = False
#                         break
# 
#                 # Check if simulation is still running
#                 if not simulation.running:
#                     break
# 
#                 # Wait briefly for responses
#                 time.sleep(0.1)
#                 while not simulation.response_queue.empty():
#                     response = simulation.response_queue.get()
#                     if response:
#                         simulation.logger.info(response)
# 
#                 # Make sure command is defined before checking it
#                 if command and command.lower() == "exit":
#                     break
# 
#             except (KeyboardInterrupt, EOFError):
#                 simulation.logger.info("\nGraceful shutdown initiated...")
#                 simulation.running = False
#                 break
#             except Exception as input_error:
#                 simulation.logger.error(f"Error in input handling: {input_error}")
#                 time.sleep(0.5)  # Brief pause before retrying
# 
#         # Wait for thread to terminate with timeout
#         if sim_thread.is_alive():
#             print("Waiting for simulation thread to terminate...")
#             sim_thread.join(timeout=5.0)
# 
#             # Force exit if still alive
#             if sim_thread.is_alive():
#                 print("Simulation thread did not terminate gracefully. Forcing exit...")
#                 if hasattr(simulation, 'sim_task') and simulation.sim_task:
#                     try:
#                         loop.call_soon_threadsafe(simulation.sim_task.cancel)
#                     except Exception as e:
#                         print(f"Error cancelling simulation task: {e}")
# 
#         # Clean up event loop
#         try:
#             pending = asyncio.all_tasks(loop)
#             for task in pending:
#                 task.cancel()
# 
#             # Wait for a short time for cancellation
#             if pending:
#                 loop.run_until_complete(asyncio.wait(pending, timeout=1.0))
# 
#             loop.run_until_complete(loop.shutdown_asyncgens())
#             loop.close()
#         except (RuntimeError, AttributeError) as e:
#             print(f"Error during cleanup: {e}")
# 
#     except Exception as e:
#         simulation.logger.error(f"Error in simulation runner: {e}")
#         traceback.print_exc()  # Goes to log file
#     finally:
#         simulation.logger.info("Simulation runner terminated")
# 
# 
# if __name__ == "__main__":
#     from agent_classes import EnhancedSingleAgentFinalEvolution
#     import logging
#     import traceback
#     logger = setup_logging()
#     try:
#         logger = setup_logging()
#         logger.info("\n🔄 Initializing agent...")
#         agent = EnhancedSingleAgentFinalEvolution(num_qubits=4)
#         logger.info("✅ Agent initialized successfully")
#     except Exception as init_error:
#         logger = logging.getLogger("emile4.simulation")
#         logger.error(f"❌ Error initializing agent: {init_error}")
#         traceback.print_exc()  # Goes to log file
#         exit(1)
# 
#     # Run interactive simulation
#     run_interactive_simulation(agent)
# 
# 
# 
# 
#

"""# Modules for future consideration, refinement, and integration. Not currently implemented.

## Semantic_ML (Conceptual Semantic Trainer - to be integrated in future iterations)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile semantic_trainer.py
# # -*- coding: utf-8 -*-
# """
# Enhanced & Integrated Émile5SemanticML Trainer
# Implements improvements for stability, efficiency, and enhanced numeric integration
# with direct Émile simulation connection.
# """
# 
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import os
# import glob
# import spacy
# import time
# import subprocess
# from collections import Counter
# import nltk
# import re
# import numpy as np
# from nltk.tokenize import word_tokenize
# from nltk.corpus import stopwords
# from sentence_transformers import SentenceTransformer
# from sklearn.metrics.pairwise import cosine_similarity
# from qiskit import QuantumCircuit, transpile
# from qiskit_aer import AerSimulator
# from transformers import AutoTokenizer, AutoModel
# import random
# import threading
# import matplotlib.pyplot as plt
# from datetime import datetime
# import json
# 
# # ================================
# # 1. CONFIGURATIONS & DATA LOADING
# # ================================
# 
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# MODEL_NAME = "bert-base-uncased"  # Or any other suitable BERT model
# MODEL_PATH = "/content/emile_semantic_ml_mini.pt"  # Path to save the trained model
# CACHE_DIR = "/content/cache"  # Directory to cache embeddings
# LOG_DIR = "/content/logs"  # Directory for new simulation logs
# EMILE_SCRIPT = "simulation_runner_logs.py"  # Path to Émile simulation script
# 
# # Create necessary directories
# for directory in [CACHE_DIR, LOG_DIR]:
#     os.makedirs(directory, exist_ok=True)
# 
# # --- Load Pretrained Models ---
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# transformer_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
# transformer_model.eval()
# 
# try:
#     spacy_model = spacy.load("en_core_web_sm")
# except OSError:
#     print("Downloading en_core_web_sm model...")
#     spacy.cli.download("en_core_web_sm")
#     spacy_model = spacy.load("en_core_web_sm")
# 
# sentence_model = SentenceTransformer('all-MiniLM-L6-v2').to(DEVICE)
# sentence_model.eval()
# 
# # --- Define Paths and Load Data ---
# data_path = "/content/"  # Path for training data
# log_paths = glob.glob(os.path.join(LOG_DIR, "emile*_sim_*.log"))  # Find all Émile log files
# if not log_paths:
#     log_paths = [os.path.join(LOG_DIR, f"emile5_sim_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log")]
# training_files = glob.glob(os.path.join(data_path, "*.txt"))
# 
# def read_text(file_path):
#     try:
#         with open(file_path, "r", encoding="utf-8") as f:
#             return f.read()
#     except Exception as e:
#         print(f"Error reading file {file_path}: {e}")
#         return ""
# 
# training_data = {os.path.basename(f): read_text(f) for f in training_files}
# 
# nltk.download("punkt", quiet=True)
# nltk.download("stopwords", quiet=True)
# stop_words = set(stopwords.words("english"))
# 
# # Initialize Tracking Variables
# training_metrics = {
#     "initial_losses": [],
#     "recursive_losses": [],
#     "coherence_scores": [],
#     "numeric_integration_scores": [],
#     "timestamps": []
# }
# 
# # ================================
# # 2. ENHANCED SEMIOTIC DISTINCTION EXTRACTOR
# # ================================
# 
# class SemioticExtractor(nn.Module):
#     """
#     Enhanced semantic model with numeric value integration capabilities.
#     Supports both old (text_fc/numeric_fc) and new (fc1/fc1_numeric) naming conventions.
#     """
#     def __init__(self, hidden_dim=384, numeric_dim=8):  # Use 384 instead of 256
#         super().__init__()
#         self.encoder = transformer_model
# 
#         # Text processing pathway
#         self.fc1 = nn.Linear(self.encoder.config.hidden_size, hidden_dim)
#         self.activation = nn.Tanh()
#         self.dropout = nn.Dropout(0.1)
# 
#         # Numeric processing pathway
#         self.numeric_enabled = numeric_dim > 0
#         if self.numeric_enabled:
#             self.fc1_numeric = nn.Linear(numeric_dim, hidden_dim // 2)
#             self.fc_combined = nn.Linear(hidden_dim + (hidden_dim // 2), 384)  # Change to 384
#         else:
#             self.fc2 = nn.Linear(hidden_dim, 384)  # Change to 384
# 
#         # Final normalization
#         self.norm = nn.LayerNorm(384)  # Change to 384
# 
#     def forward(self, input_ids, attention_mask, numeric_values=None):
#         with torch.no_grad():
#             outputs = self.encoder(input_ids, attention_mask=attention_mask)
#         pooled_output = outputs.last_hidden_state[:, 0, :]
# 
#         # Process text features
#         text_features = self.activation(self.fc1(pooled_output))
#         text_features = self.dropout(text_features)
# 
#         # Process and combine with numeric features if provided
#         if self.numeric_enabled and numeric_values is not None:
#             numeric_features = self.activation(self.fc1_numeric(numeric_values))
#             combined_features = torch.cat([text_features, numeric_features], dim=1)
#             output = self.fc_combined(combined_features)
#         else:
#             output = self.fc2(text_features) if hasattr(self, 'fc2') else text_features
# 
#         return self.norm(output)
# 
# # ================================
# # 3. IMPROVED QUANTUM SEMANTIC STABILIZER
# # ================================
# 
# def quantum_semantic_stabilization(num_qubits=4, shots=1024):
#     """Enhanced quantum stabilization with measurement shots parameter"""
#     from qiskit import QuantumCircuit, transpile
#     from qiskit_aer import AerSimulator
# 
#     qc = QuantumCircuit(num_qubits)
# 
#     # Apply Hadamard gates for superposition
#     for i in range(num_qubits):
#         qc.h(i)
# 
#     # Add entanglement for more complex stabilization
#     for i in range(num_qubits-1):
#         qc.cx(i, i+1)
# 
#     # Add phase kickback
#     for i in range(num_qubits):
#         qc.rz(np.pi/4, i)
# 
#     qc.measure_all()
# 
#     simulator = AerSimulator()
#     compiled_circuit = transpile(qc, simulator)
#     result = simulator.run(compiled_circuit, shots=shots).result()
#     return result.get_counts()
# 
# # ================================
# # 4. ENHANCED NLP THEME EXTRACTION WITH NUMERIC RECOGNITION
# # ================================
# 
# def extract_themes(text, top_n=20, min_word_length=3, extract_numeric=True):
#     """
#     Extract themes with improved filtering and weighting,
#     and optional numeric value extraction.
#     """
#     # Process with spaCy
#     doc = spacy_model(text)
# 
#     # Extract lemmatized words
#     words = [token.lemma_.lower() for token in doc
#              if token.is_alpha and
#              not token.is_stop and
#              len(token.text) >= min_word_length and
#              token.pos_ in ['NOUN', 'VERB', 'ADJ']]  # Focus on meaningful POS
# 
#     word_freq = Counter(words)
#     top_words = word_freq.most_common(top_n)
# 
#     result = {"word_themes": top_words}
# 
#     # Optional numeric extraction
#     if extract_numeric:
#         numeric_values = []
#         numeric_patterns = {}
# 
#         # Extract numbers via regex
#         numbers = re.findall(r'\b\d+\.?\d*\b', text)
#         numeric_values.extend([float(num) for num in numbers])
# 
#         # Potential numeric-related terms
#         numeric_modifiers = [
#             "increase", "decrease", "oscillate", "accelerate",
#             "decelerate", "threshold", "critical", "harmonic",
#             "resonant", "quantize", "continuous", "discrete"
#         ]
# 
#         numeric_transformations = [
#             "multiply", "divide", "amplify", "attenuate",
#             "exponential", "logarithmic", "scale", "normalize",
#             "bound", "unbound", "fraction", "integral"
#         ]
# 
#         for term in numeric_modifiers:
#             count = len(re.findall(rf'\b{term}\w*\b', text.lower()))
#             if count > 0:
#                 numeric_patterns[term] = count
# 
#         for term in numeric_transformations:
#             count = len(re.findall(rf'\b{term}\w*\b', text.lower()))
#             if count > 0:
#                 numeric_patterns[term] = count
# 
#         result["numeric_values"] = numeric_values
#         result["numeric_patterns"] = numeric_patterns
# 
#         # Basic stats
#         if numeric_values:
#             result["numeric_stats"] = {
#                 "count": len(numeric_values),
#                 "mean": np.mean(numeric_values),
#                 "std": np.std(numeric_values),
#                 "min": min(numeric_values),
#                 "max": max(numeric_values)
#             }
# 
#     return result
# 
# symbolic_themes = {file: extract_themes(content) for file, content in training_data.items()}
# 
# # ================================
# # 5. IMPROVED SENTENCE EMBEDDING & CONCEPT MAPPING
# # ================================
# 
# def precompute_embeddings(text_data, cache_path=None, extract_numeric=True):
#     """
#     Precompute and optionally cache sentence embeddings.
#     Also can extract numeric values from sentences.
#     """
#     sentences = [sent.strip() for sent in text_data.split(".") if sent.strip()]
# 
#     if cache_path and os.path.exists(cache_path):
#         try:
#             cached_data = torch.load(cache_path)
#             print(f"Loaded cached embeddings from {cache_path}")
#             return (cached_data['sentences'],
#                     cached_data['embeddings'],
#                     cached_data.get('numeric_values', None))
#         except Exception as e:
#             print(f"Could not load cache: {e}")
# 
#     batch_size = 32
#     all_embeddings = []
# 
#     numeric_values = None
#     if extract_numeric:
#         numeric_values = []
#         for sentence in sentences:
#             numbers = re.findall(r'\b\d+\.?\d*\b', sentence)
#             for num in numbers:
#                 try:
#                     numeric_values.append({
#                         'value': float(num),
#                         'sentence_idx': sentences.index(sentence)
#                     })
#                 except ValueError:
#                     pass
# 
#     for i in range(0, len(sentences), batch_size):
#         batch = sentences[i:i+batch_size]
#         if not batch:
#             continue
#         batch_embeddings = sentence_model.encode(batch, convert_to_tensor=True)
#         all_embeddings.append(batch_embeddings)
# 
#     if not all_embeddings:
#         return [], torch.tensor([]), None
# 
#     embeddings = torch.cat(all_embeddings, dim=0)
# 
#     if cache_path:
#         os.makedirs(os.path.dirname(cache_path), exist_ok=True)
#         cache_data = {
#             'sentences': sentences,
#             'embeddings': embeddings
#         }
#         if numeric_values:
#             cache_data['numeric_values'] = numeric_values
# 
#         torch.save(cache_data, cache_path)
#         print(f"Saved embeddings to cache at {cache_path}")
# 
#     return sentences, embeddings, numeric_values
# 
# sentence_embeddings = {}
# numeric_data = {}
# for file, content in training_data.items():
#     cache_path = os.path.join(CACHE_DIR, f"{file}.emb")
#     sentences, embeddings, numeric_values = precompute_embeddings(content, cache_path)
#     sentence_embeddings[file] = (sentences, embeddings)
#     if numeric_values:
#         numeric_data[file] = numeric_values
# 
# def find_related_concepts(query, top_n=5):
#     """Find semantically related concepts using the sentence embeddings."""
#     query_embedding = sentence_model.encode([query], convert_to_tensor=True).to(DEVICE)
#     results = []
# 
#     for file, (sentences, embeddings) in sentence_embeddings.items():
#         if len(sentences) == 0 or embeddings.shape[0] == 0:
#             continue
# 
#         if embeddings.device != query_embedding.device:
#             embeddings = embeddings.to(query_embedding.device)
# 
#         similarities = cosine_similarity(query_embedding.cpu().numpy(),
#                                          embeddings.cpu().numpy())[0]
# 
#         top_indices = np.argsort(similarities)[-3:][::-1]
#         for idx in top_indices:
#             if idx < len(sentences):
#                 results.append((file, sentences[idx], similarities[idx]))
# 
#     results = sorted(results, key=lambda x: x[2], reverse=True)
#     return results[:top_n]
# 
# # ================================
# # 6. IMPROVED NUMERIC-SEMANTIC INTEGRATION
# # ================================
# 
# def extract_numeric_features(text, max_features=8):
#     """
#     Extract numeric features from text for input to the semantic model.
#     """
#     features = []
# 
#     numbers = re.findall(r'\b\d+\.?\d*\b', text)
#     raw_values = []
#     for num in numbers:
#         try:
#             raw_values.append(float(num))
#         except ValueError:
#             pass
# 
#     if raw_values:
#         features.extend([
#             len(raw_values),
#             np.mean(raw_values),
#             np.std(raw_values) if len(raw_values) > 1 else 0.0,
#             max(raw_values),
#             min(raw_values),
#             np.median(raw_values),
#             np.percentile(raw_values, 25) if len(raw_values) > 3 else min(raw_values),
#             np.percentile(raw_values, 75) if len(raw_values) > 3 else max(raw_values)
#         ])
# 
#     features = features[:max_features]
#     if len(features) < max_features:
#         features.extend([0.0] * (max_features - len(features)))
# 
#     return torch.tensor(features, dtype=torch.float32)
# 
# def calculate_numeric_integration_score(text, numeric_values):
#     """
#     Calculate how well numeric values are integrated with semantic context.
#     """
#     if not numeric_values:
#         return 0.0
# 
#     context_indicators = [
#         "value", "measure", "rate", "score", "level", "threshold",
#         "percent", "amount", "quantity", "frequency", "probability",
#         "equals", "is", "was", "reached", "exceeded", "dropped",
#         "increased", "decreased", "accelerated", "decelerated",
#         "equals", "=", "approximately", "about", "around", "nearly"
#     ]
#     context_score = 0.0
#     for indicator in context_indicators:
#         if indicator in text.lower():
#             context_score += 0.1
#     context_score = min(1.0, context_score)
# 
#     unit_indicators = [
#         "%", "percent", "kg", "meters", "seconds", "minutes", "hours",
#         "days", "watts", "joules", "degrees", "Hz", "hertz", "bytes",
#         "bps", "bits", "pixels", "frames", "cycles", "iterations"
#     ]
#     unit_score = 0.0
#     for unit in unit_indicators:
#         if unit in text:
#             unit_score += 0.2
#     unit_score = min(1.0, unit_score)
# 
#     transform_indicators = [
#         "times", "multiplied", "divided", "plus", "minus", "added",
#         "subtracted", "increased by", "decreased by", "factor of",
#         "scaled", "normalized", "averaged", "mean", "median", "deviation"
#     ]
#     transform_score = 0.0
#     for transform in transform_indicators:
#         if transform in text.lower():
#             transform_score += 0.15
#     transform_score = min(1.0, transform_score)
# 
#     final_score = 0.4 * context_score + 0.3 * unit_score + 0.3 * transform_score
#     return final_score
# 
# # ================================
# # 7. IMPROVED TRAINING AND REFINEMENT
# # ================================
# 
# def train_semiotic_model(model, training_data, epochs=5, batch_size=16, learning_rate=1e-5, save_interval=2):
#     """Training loop that uses CosineEmbeddingLoss and numeric integration tracking."""
#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
#     scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2)
#     loss_function = nn.CosineEmbeddingLoss()
# 
#     best_loss = float('inf')
# 
#     for epoch in range(epochs):
#         print(f"Epoch {epoch + 1}/{epochs}")
#         model.train()
#         total_loss = 0
#         batch_count = 0
# 
#         data_items = list(training_data.items())
#         random.shuffle(data_items)
# 
#         for i in range(0, len(data_items), batch_size):
#             batch_data = dict(data_items[i:i + min(batch_size, len(data_items) - i)])
#             if not batch_data:
#                 continue
# 
#             batch_text = " ".join(batch_data.values())
#             if not batch_text.strip():
#                 continue
# 
#             inputs = tokenizer(batch_text, return_tensors="pt", padding=True,
#                                truncation=True, max_length=512).to(DEVICE)
# 
#             numeric_features = extract_numeric_features(batch_text).to(DEVICE)
# 
#             # Expand dims if needed
#             batch_size_actual = inputs['input_ids'].shape[0]
#             if batch_size_actual > 1:
#                 numeric_features = numeric_features.unsqueeze(0).expand(batch_size_actual, -1)
# 
#             outputs = model(inputs['input_ids'], inputs['attention_mask'], numeric_features)
# 
#             with torch.no_grad():
#                 sentence_transformer_embeddings = sentence_model.encode(batch_text, convert_to_tensor=True).to(DEVICE)
# 
#             if outputs.shape[0] != sentence_transformer_embeddings.shape[0]:
#                 min_size = min(outputs.shape[0], sentence_transformer_embeddings.shape[0])
#                 outputs = outputs[:min_size]
#                 sentence_transformer_embeddings = sentence_transformer_embeddings[:min_size]
# 
#             target = torch.ones(outputs.shape[0]).to(DEVICE)
#             loss = loss_function(outputs, sentence_transformer_embeddings, target)
# 
#             optimizer.zero_grad()
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#             optimizer.step()
# 
#             total_loss += loss.item()
#             batch_count += 1
# 
#             numeric_score = calculate_numeric_integration_score(batch_text, extract_numeric_features(batch_text).tolist())
#             training_metrics['numeric_integration_scores'].append(numeric_score)
# 
#             training_metrics['initial_losses'].append(loss.item())
#             training_metrics['timestamps'].append(time.time())
# 
#             if batch_count % 5 == 0:
#                 print(f"  Batch {batch_count}: Loss = {loss.item():.4f}, Numeric Integration = {numeric_score:.4f}")
# 
#         avg_loss = total_loss / max(1, batch_count)
#         print(f"  Average Loss: {avg_loss:.4f}")
#         scheduler.step(avg_loss)
# 
#         if epoch % save_interval == 0 or avg_loss < best_loss:
#             if avg_loss < best_loss:
#                 best_loss = avg_loss
#                 save_model(model, MODEL_PATH.replace('.pt', '_best.pt'))
#                 print(f"  New best model saved (loss: {best_loss:.4f})")
#             else:
#                 save_model(model)
#                 print(f"  Model saved at epoch {epoch+1}")
# 
# 
# def run_emile_simulation():
#     """Runs the Émile simulation in a separate process, logs to a new file."""
#     timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
#     log_file = os.path.join(LOG_DIR, f"emile4_sim_{timestamp}.log")
# 
#     global log_paths
#     log_paths = [log_file]
# 
#     print(f"🚀 Starting Émile simulation. Logging to: {log_file}")
#     log_handle = open(log_file, 'w', encoding='utf-8')
# 
#     try:
#         process = subprocess.Popen(
#             ["python", EMILE_SCRIPT],
#             stdout=log_handle,
#             stderr=subprocess.STDOUT,
#             universal_newlines=True
#         )
#         return process, log_file, log_handle
#     except Exception as e:
#         log_handle.close()
#         print(f"Error starting Émile simulation: {e}")
#         return None, log_file, None
# 
# 
# def refine_semiotic_model_with_simulation(model, update_interval=10, batch_size=16, learning_rate=1e-5, runtime=1800):
#     """
#     Runs Émile simulation while training on new Symbolic Expression lines in the logs.
#     """
#     print("\n🔄 Starting integrated Émile simulation and semantic model training")
#     emile_process, log_file, log_handle = run_emile_simulation()
# 
#     if emile_process is None:
#         print("❌ Failed to start Émile simulation. Aborting integrated training.")
#         return
# 
#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
#     loss_function = nn.CosineEmbeddingLoss()
# 
#     model.train()
# 
#     start_time = time.time()
#     last_position = 0
#     batch_text = []
#     batch_numeric = []
#     batch_count = 0
#     total_loss = 0
#     symbolic_expressions_count = 0
#     numeric_integration_score_sum = 0.0
# 
#     try:
#         while time.time() - start_time < runtime and emile_process.poll() is None:
#             time.sleep(0.1)
#             with open(log_file, "r", encoding="utf-8") as f:
#                 f.seek(last_position)
#                 new_lines = f.readlines()
#                 last_position = f.tell()
# 
#             for line in new_lines:
#                 if "Symbolic Expression:" in line:
#                     symbolic_content = line.split("Symbolic Expression:", 1)[1].strip()
# 
#                     numeric_values = {}
#                     numeric_matches = re.findall(r'(\w+)=(\d+\.?\d*)', line)
#                     for key, value in numeric_matches:
#                         try:
#                             numeric_values[key] = float(value)
#                         except ValueError:
#                             pass
# 
#                     if symbolic_content:
#                         batch_text.append(symbolic_content)
#                         batch_numeric.append(numeric_values)
#                         symbolic_expressions_count += 1
#                         print(f"📝 Collected symbolic expression: \"{symbolic_content[:50]}...\"")
# 
#                         if numeric_values:
#                             numeric_str = ", ".join([f"{k}={v}" for k, v in numeric_values.items()])
#                             print(f"   📊 With numeric values: {numeric_str}")
# 
#             if len(batch_text) >= update_interval:
#                 batch_count += 1
#                 print(f"\n🔍 Processing batch {batch_count} with {len(batch_text)} symbolic expressions")
# 
#                 text_data = " ".join(batch_text)
#                 inputs = tokenizer(text_data, return_tensors="pt", padding=True,
#                                    truncation=True, max_length=512).to(DEVICE)
# 
#                 numeric_features = extract_numeric_features(text_data).to(DEVICE)
#                 batch_size_actual = inputs['input_ids'].shape[0]
#                 if batch_size_actual > 1:
#                     numeric_features = numeric_features.unsqueeze(0).expand(batch_size_actual, -1)
# 
#                 outputs = model(inputs['input_ids'], inputs['attention_mask'], numeric_features)
# 
#                 with torch.no_grad():
#                     ref_embeds = sentence_model.encode(text_data, convert_to_tensor=True).to(DEVICE)
# 
#                 if outputs.shape[0] != ref_embeds.shape[0]:
#                     min_size = min(outputs.shape[0], ref_embeds.shape[0])
#                     outputs = outputs[:min_size]
#                     ref_embeds = ref_embeds[:min_size]
# 
#                 target = torch.ones(outputs.shape[0]).to(DEVICE)
#                 loss = loss_function(outputs, ref_embeds, target)
# 
#                 if torch.isnan(loss) or torch.isinf(loss):
#                     print("⚠️ Warning: NaN/Inf loss detected. Skipping batch.")
#                     batch_text = []
#                     batch_numeric = []
#                     continue
# 
#                 optimizer.zero_grad()
#                 loss.backward()
#                 torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#                 optimizer.step()
# 
#                 numeric_score = calculate_numeric_integration_score(text_data, extract_numeric_features(text_data).tolist())
#                 numeric_integration_score_sum += numeric_score
# 
#                 current_loss = loss.item()
#                 total_loss += current_loss
#                 training_metrics['recursive_losses'].append(current_loss)
#                 training_metrics['numeric_integration_scores'].append(numeric_score)
#                 training_metrics['timestamps'].append(time.time())
# 
#                 print(f"📊 Batch {batch_count} Loss: {current_loss:.4f}, Numeric Integration: {numeric_score:.4f}")
# 
#                 if batch_count % 5 == 0:
#                     coherence = evaluate_semantic_coherence(model, batch_text[:5])
#                     training_metrics['coherence_scores'].append(coherence)
#                     print(f"🧠 Current semantic coherence: {coherence:.4f}")
# 
#                 if batch_count % 5 == 0:
#                     save_model(model, MODEL_PATH.replace('.pt', f'_sim_{batch_count}.pt'))
#                     print(f"💾 Model checkpoint saved at batch {batch_count}")
# 
#                 batch_text = []
#                 batch_numeric = []
# 
#         if len(batch_text) > 0:
#             save_model(model, MODEL_PATH.replace('.pt', '_final.pt'))
# 
#         runtime_elapsed = time.time() - start_time
#         avg_loss = total_loss / max(1, batch_count)
#         avg_numeric_score = numeric_integration_score_sum / max(1, batch_count)
# 
#         print(f"\n✅ Integrated training complete after {runtime_elapsed:.1f} seconds")
#         print(f"📊 Processed {symbolic_expressions_count} symbolic expressions in {batch_count} batches")
#         print(f"📈 Average loss: {avg_loss:.4f}, Average numeric integration: {avg_numeric_score:.4f}")
# 
#     except Exception as e:
#         print(f"❌ Error during integrated training: {e}")
#         import traceback
#         traceback.print_exc()
# 
#     finally:
#         if emile_process and emile_process.poll() is None:
#             emile_process.terminate()
#             print("🛑 Émile simulation terminated")
# 
#         if log_handle:
#             log_handle.close()
# 
#         save_model(model, MODEL_PATH.replace('.pt', '_integrated_final.pt'))
#         print("💾 Final integrated model saved")
# 
#         plot_training_metrics()
# 
#         return symbolic_expressions_count, batch_count, runtime_elapsed
# 
# 
# def evaluate_semantic_coherence(model, query_texts, reference_texts=None):
#     """
#     Evaluates how close the model embeddings are to a "ground truth" (SentenceTransformer) reference.
#     """
#     model.eval()
# 
#     if reference_texts is None:
#         reference_texts = list(training_data.values())
# 
#     query_embeddings = []
#     for text in query_texts:
#         inputs = tokenizer(text, return_tensors="pt", padding=True, truncation=True, max_length=512).to(DEVICE)
#         numeric_features = extract_numeric_features(text).to(DEVICE)
# 
#         with torch.no_grad():
#             output = model(inputs['input_ids'], inputs['attention_mask'], numeric_features)
#         query_embeddings.append(output)
# 
#     if query_embeddings:
#         query_embeddings = torch.cat(query_embeddings, dim=0)
#     else:
#         return 0.0
# 
#     reference_embeddings = sentence_model.encode(reference_texts, convert_to_tensor=True).to(DEVICE)
# 
#     similarities = []
#     for q_emb in query_embeddings:
#         q_emb = q_emb.unsqueeze(0)
#         cos_sims = torch.nn.functional.cosine_similarity(q_emb, reference_embeddings)
#         max_sim = torch.max(cos_sims).item()
#         similarities.append(max_sim)
# 
#     return sum(similarities) / len(similarities) if similarities else 0.0
# 
# 
# def plot_training_metrics():
#     """
#     Plot training metrics over time with numeric integration scores.
#     """
#     fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 15))
# 
#     # Losses
#     if training_metrics['initial_losses']:
#         ax1.plot(range(len(training_metrics['initial_losses'])),
#                  training_metrics['initial_losses'],
#                  label='Initial Training Loss')
# 
#     if training_metrics['recursive_losses']:
#         ax1.plot(range(len(training_metrics['recursive_losses'])),
#                  training_metrics['recursive_losses'],
#                  label='Recursive Training Loss')
# 
#     ax1.set_title('Loss over Training Iterations')
#     ax1.set_xlabel('Batch')
#     ax1.set_ylabel('Loss')
#     ax1.legend()
#     ax1.grid(True)
# 
#     # Coherence
#     if training_metrics['coherence_scores']:
#         ax2.plot(range(len(training_metrics['coherence_scores'])),
#                  training_metrics['coherence_scores'],
#                  label='Semantic Coherence',
#                  color='green')
#         ax2.set_title('Semantic Coherence over Training')
#         ax2.set_xlabel('Evaluation Point')
#         ax2.set_ylabel('Coherence Score')
#         ax2.legend()
#         ax2.grid(True)
# 
#     # Numeric Integration
#     if training_metrics['numeric_integration_scores']:
#         ax3.plot(range(len(training_metrics['numeric_integration_scores'])),
#                  training_metrics['numeric_integration_scores'],
#                  label='Numeric Integration',
#                  color='purple')
#         window_size = min(10, len(training_metrics['numeric_integration_scores']))
#         if window_size > 1:
#             rolling_avg = np.convolve(training_metrics['numeric_integration_scores'],
#                                       np.ones(window_size)/window_size,
#                                       mode='valid')
#             ax3.plot(range(window_size-1, len(training_metrics['numeric_integration_scores'])),
#                      rolling_avg,
#                      label=f'{window_size}-point Moving Average',
#                      color='blue',
#                      linestyle='--')
# 
#         ax3.set_title('Numeric Integration over Training')
#         ax3.set_xlabel('Batch')
#         ax3.set_ylabel('Integration Score')
#         ax3.legend()
#         ax3.grid(True)
# 
#     plt.tight_layout()
#     timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#     filename = os.path.join(LOG_DIR, f"training_metrics_{timestamp}.png")
#     plt.savefig(filename)
#     print(f"📊 Training metrics plot saved to {filename}")
#     plt.close(fig)
# 
# 
# def refine_semiotic_model(model, log_file, update_interval=10, batch_size=16, learning_rate=1e-5):
#     """
#     Optimized log-based training with improved monitoring and numeric integration
#     (for existing log files).
#     """
#     print(f"Starting recursive training on {log_file}...")
#     optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)
#     loss_function = nn.CosineEmbeddingLoss()
# 
#     last_position = 0
#     batch_text = []
#     batch_numeric = []
#     batch_count = 0
#     total_loss = 0
#     save_counter = 0
#     numeric_integration_score_sum = 0.0
# 
#     model.train()
# 
#     def stream_log(filepath, start_pos=0):
#         with open(filepath, 'r', encoding='utf-8') as f:
#             f.seek(start_pos)
#             while True:
#                 line = f.readline()
#                 if not line:
#                     break
#                 yield line, f.tell()
# 
#     for line, last_position in stream_log(log_file, last_position):
#         if "Symbolic Expression:" in line:
#             symbolic_content = line.split("Symbolic Expression:", 1)[1].strip()
#             numeric_values = {}
#             numeric_matches = re.findall(r'(\w+)=(\d+\.?\d*)', line)
#             for key, value in numeric_matches:
#                 try:
#                     numeric_values[key] = float(value)
#                 except ValueError:
#                     pass
# 
#             if symbolic_content:
#                 batch_text.append(symbolic_content)
#                 batch_numeric.append(numeric_values)
# 
#         if len(batch_text) >= update_interval:
#             batch_count += 1
#             save_counter += 1
#             print(f"  Processing batch {batch_count} from log...")
# 
#             text_data = " ".join(batch_text)
#             inputs = tokenizer(text_data, return_tensors="pt", padding=True,
#                                truncation=True, max_length=512).to(DEVICE)
# 
#             numeric_features = extract_numeric_features(text_data).to(DEVICE)
#             batch_size_actual = inputs['input_ids'].shape[0]
#             if batch_size_actual > 1:
#                 numeric_features = numeric_features.unsqueeze(0).expand(batch_size_actual, -1)
# 
#             outputs = model(inputs['input_ids'], inputs['attention_mask'], numeric_features)
# 
#             with torch.no_grad():
#                 sentence_transformer_embeddings = sentence_model.encode(text_data, convert_to_tensor=True).to(DEVICE)
# 
#             if outputs.shape[0] != sentence_transformer_embeddings.shape[0]:
#                 min_size = min(outputs.shape[0], sentence_transformer_embeddings.shape[0])
#                 outputs = outputs[:min_size]
#                 sentence_transformer_embeddings = sentence_transformer_embeddings[:min_size]
# 
#             target = torch.ones(outputs.shape[0]).to(DEVICE)
#             loss = loss_function(outputs, sentence_transformer_embeddings, target)
# 
#             if torch.isnan(loss) or torch.isinf(loss):
#                 print("    WARNING: NaN/Inf loss detected. Skipping batch.")
#                 batch_text = []
#                 batch_numeric = []
#                 continue
# 
#             optimizer.zero_grad()
#             loss.backward()
#             torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
#             optimizer.step()
# 
#             numeric_score = calculate_numeric_integration_score(text_data, extract_numeric_features(text_data).tolist())
#             numeric_integration_score_sum += numeric_score
# 
#             current_loss = loss.item()
#             training_metrics['recursive_losses'].append(current_loss)
#             training_metrics['numeric_integration_scores'].append(numeric_score)
#             training_metrics['timestamps'].append(time.time())
# 
#             print(f"    Batch Loss: {current_loss:.4f}, Numeric Integration: {numeric_score:.4f}")
#             total_loss += current_loss
#             batch_text = []
#             batch_numeric = []
# 
#             if save_counter >= 5:
#                 save_model(model)
#                 print(f"    Model saved after {save_counter} batches")
#                 save_counter = 0
# 
#     if save_counter > 0:
#         save_model(model)
#         avg_loss = total_loss / max(1, batch_count)
#         avg_numeric_score = numeric_integration_score_sum / max(1, batch_count)
#         print(f"Final model saved with average loss: {avg_loss:.4f}, average numeric integration: {avg_numeric_score:.4f}")
# 
# 
# def extract_numeric_properties_from_logs(log_file):
#     """
#     Extracts numeric properties from simulation logs for training data enrichment.
#     """
#     print(f"Extracting numeric properties from {log_file}...")
# 
#     numeric_series = {
#         'surplus': [],
#         'distinction': [],
#         'coherence': [],
#         'entropy': [],
#         'dimensionality': [],
#         'timestamps': []
#     }
# 
#     symbolic_expressions = []
#     try:
#         with open(log_file, 'r', encoding='utf-8') as f:
#             lines = f.readlines()
#         current_time = time.time()
#         time_offset = 0
# 
#         for line in lines:
#             timestamp_match = re.search(r'\[([\d\-\s:.]+)\]', line)
#             if timestamp_match:
#                 try:
#                     timestamp_str = timestamp_match.group(1)
#                     for fmt in ['%Y-%m-%d %H:%M:%S.%f', '%Y-%m-%d %H:%M:%S',
#                                 '%H:%M:%S.%f', '%H:%M:%S']:
#                         try:
#                             dt = datetime.strptime(timestamp_str, fmt)
#                             time_offset = (dt - datetime(1970, 1, 1)).total_seconds()
#                             break
#                         except ValueError:
#                             continue
#                 except Exception:
#                     time_offset += 0.1
#             else:
#                 time_offset += 0.1
# 
#             timestamp = current_time - (len(lines) - lines.index(line)) * 0.1
# 
#             if "Symbolic Expression:" in line:
#                 expr = line.split("Symbolic Expression:", 1)[1].strip()
#                 symbolic_expressions.append({
#                     'expression': expr,
#                     'timestamp': timestamp
#                 })
#                 numeric_matches = re.findall(r'(\w+)=(\d+\.?\d*)', line)
#                 for key, value in numeric_matches:
#                     try:
#                         value = float(value)
#                         if key in numeric_series:
#                             numeric_series[key].append(value)
#                             if len(numeric_series[key]) > len(numeric_series['timestamps']):
#                                 numeric_series['timestamps'].append(timestamp)
#                     except ValueError:
#                         pass
# 
#             for key in numeric_series.keys():
#                 if key == 'timestamps':
#                     continue
#                 pattern = fr'{key}\s*[=:]\s*(\d+\.?\d*)'
#                 match = re.search(pattern, line, re.IGNORECASE)
#                 if match:
#                     try:
#                         value = float(match.group(1))
#                         numeric_series[key].append(value)
#                         if len(numeric_series[key]) > len(numeric_series['timestamps']):
#                             numeric_series['timestamps'].append(timestamp)
#                     except ValueError:
#                         pass
# 
#         total_values = sum(len(values) for k, values in numeric_series.items() if k != 'timestamps')
#         print(f"Extracted {total_values} numeric values and {len(symbolic_expressions)} symbolic expressions")
# 
#         stats = {}
#         for key, values in numeric_series.items():
#             if key == 'timestamps' or not values:
#                 continue
#             stats[key] = {
#                 'count': len(values),
#                 'min': min(values),
#                 'max': max(values),
#                 'mean': sum(values) / len(values),
#                 'std': np.std(values)
#             }
# 
#         return {
#             'time_series': numeric_series,
#             'symbolic_expressions': symbolic_expressions,
#             'stats': stats
#         }
# 
#     except Exception as e:
#         print(f"Error extracting numeric properties: {e}")
#         import traceback
#         traceback.print_exc()
#         return {
#             'time_series': numeric_series,
#             'symbolic_expressions': symbolic_expressions,
#             'stats': {},
#             'error': str(e)
#         }
# 
# def create_training_dataset_with_numeric_values(log_data, output_folder=None):
#     """
#     Creates an enhanced training dataset with numeric values from log data.
#     """
#     if output_folder is None:
#         output_folder = CACHE_DIR
# 
#     os.makedirs(output_folder, exist_ok=True)
# 
#     expressions = log_data.get('symbolic_expressions', [])
#     time_series = log_data.get('time_series', {})
# 
#     training_texts = []
#     numeric_features = []
# 
#     for expr_data in expressions:
#         expr = expr_data['expression']
#         timestamp = expr_data['timestamp']
# 
#         closest_values = {}
#         for key, values in time_series.items():
#             if key == 'timestamps' or not values:
#                 continue
#             timestamps = time_series['timestamps']
#             if not timestamps:
#                 continue
#             closest_idx = min(range(len(timestamps)), key=lambda i: abs(timestamps[i] - timestamp))
#             if closest_idx < len(values):
#                 closest_values[key] = values[closest_idx]
# 
#         features = []
#         for key in ['surplus', 'distinction', 'coherence', 'entropy', 'dimensionality']:
#             if key in closest_values:
#                 features.append(closest_values[key])
#             else:
#                 features.append(0.0)
# 
#         if features:  # If we have any
#             for key in ['surplus', 'distinction', 'coherence']:
#                 if key in time_series and len(time_series[key]) > 5:
#                     recent_values = time_series[key][-5:]
#                     features.append(np.var(recent_values))
#                 else:
#                     features.append(0.0)
# 
#         while len(features) < 8:
#             features.append(0.0)
# 
#         training_texts.append(expr)
#         numeric_features.append(features)
# 
#     if output_folder:
#         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#         text_file = os.path.join(output_folder, f'training_texts_{timestamp}.txt')
#         with open(text_file, 'w', encoding='utf-8') as f:
#             f.write('\n'.join(training_texts))
# 
#         features_file = os.path.join(output_folder, f'numeric_features_{timestamp}.npy')
#         np.save(features_file, np.array(numeric_features))
# 
#         print(f"Saved {len(training_texts)} training examples to {output_folder}")
# 
#     return training_texts, numeric_features
# 
# # --- Improved Helper Functions ---
# 
# def save_model(model, path=MODEL_PATH):
#     """Saves the trained model plus numeric config info."""
#     try:
#         numeric_enabled = hasattr(model, 'numeric_enabled') and model.numeric_enabled
#         numeric_dim = model.fc1_numeric.in_features if numeric_enabled else 0
# 
#         torch.save({
#             'model_state_dict': model.state_dict(),
#             'model_config': {
#                 'hidden_dim': model.fc1.out_features,
#                 'numeric_dim': numeric_dim,
#                 'numeric_enabled': numeric_enabled,
#                 'output_dim': 256
#             },
#             'training_metrics': {
#                 'numeric_integration_scores': training_metrics.get('numeric_integration_scores', []),
#                 'coherence_scores': training_metrics.get('coherence_scores', [])
#             },
#             'timestamp': time.strftime("%Y%m%d-%H%M%S")
#         }, path)
#         print(f"Model saved to {path}")
#     except Exception as e:
#         print(f"Error saving model: {e}")
#         alt_path = os.path.join(os.path.dirname(path), f"backup_model_{time.strftime('%Y%m%d_%H%M%S')}.pt")
#         try:
#             torch.save(model.state_dict(), alt_path)
#             print(f"Model saved to alternate location: {alt_path}")
#         except Exception as e2:
#             print(f"Could not save to alternate location either: {e2}")
# 
# def load_model(path=MODEL_PATH):
#     """
#     Loads a previously trained model with numeric integration support.
#     """
#     try:
#         checkpoint = torch.load(path)
# 
#         if isinstance(checkpoint, dict) and 'model_state_dict' in checkpoint:
#             config = checkpoint.get('model_config', {})
#             hidden_dim = config.get('hidden_dim', 384)
#             numeric_dim = config.get('numeric_dim', 0)
#             numeric_enabled = config.get('numeric_enabled', False)
# 
#             print(f"Loading model with config: hidden_dim={hidden_dim}, numeric_dim={numeric_dim}, numeric_enabled={numeric_enabled}")
# 
#             model = SemioticExtractor(hidden_dim=hidden_dim, numeric_dim=numeric_dim).to(DEVICE)
#             model.load_state_dict(checkpoint['model_state_dict'])
# 
#             if 'training_metrics' in checkpoint:
#                 metrics = checkpoint['training_metrics']
#                 if 'numeric_integration_scores' in metrics:
#                     training_metrics['numeric_integration_scores'] = metrics['numeric_integration_scores']
#                 if 'coherence_scores' in metrics:
#                     training_metrics['coherence_scores'] = metrics['coherence_scores']
#         else:
#             print("Loading legacy model format (state dict only)")
#             model = SemioticExtractor().to(DEVICE)
#             model.load_state_dict(checkpoint)
# 
#         model.eval()
#         print(f"Model loaded from {path}")
#         return model
#     except Exception as e:
#         print(f"Error loading model from {path}: {e}")
#         backup_pattern = os.path.join(os.path.dirname(path), "backup_model_*.pt")
#         backups = sorted(glob.glob(backup_pattern), reverse=True)
# 
#         if backups:
#             print(f"Attempting to load latest backup: {backups[0]}")
#             try:
#                 model = SemioticExtractor().to(DEVICE)
#                 model.load_state_dict(torch.load(backups[0]))
#                 model.eval()
#                 print(f"Successfully loaded backup from {backups[0]}")
#                 return model
#             except Exception as e2:
#                 print(f"Could not load backup either: {e2}")
# 
#         print("Creating new model instead.")
#         return SemioticExtractor().to(DEVICE)
# 
# # ================================
# # 8. NUMERIC INTEGRATION TESTING
# # ================================
# 
# def test_numeric_integration(model, test_expressions=None):
#     """
#     Test how the model handles numeric values in text.
#     """
#     model.eval()
# 
#     if test_expressions is None:
#         test_expressions = [
#             "Coherence stabilizes within ontological field [coherence=0.85].",
#             "Flux dissolves across phase space [entropy=0.72].",
#             "Distinction emerges through complexity [distinction=0.67, surplus=3.5].",
#             "Increasing surplus=4.2 amplifies dimensional emergence.",
#             "Equilibrium aligns with stability [threshold=2.8 normalized].",
#             "Oscillating coherence=0.45 within attractor dynamics.",
#             "Recursion stabilizes within feedback [dimensionality=4]."
#         ]
# 
#     results = []
# 
#     for expr in test_expressions:
#         inputs = tokenizer(expr, return_tensors="pt", padding=True,
#                            truncation=True, max_length=512).to(DEVICE)
#         numeric_features = extract_numeric_features(expr).to(DEVICE)
# 
#         with torch.no_grad():
#             output_with_numeric = model(inputs['input_ids'], inputs['attention_mask'], numeric_features)
# 
#         zero_numeric = torch.zeros_like(numeric_features).to(DEVICE)
#         with torch.no_grad():
#             output_without_numeric = model(inputs['input_ids'], inputs['attention_mask'], zero_numeric)
# 
#         reference_embedding = sentence_model.encode([expr], convert_to_tensor=True).to(DEVICE)
# 
#         sim_with_numeric_tensor = torch.nn.functional.cosine_similarity(
#             output_with_numeric,  # shape (1, 384)
#             reference_embedding,  # shape (1, 384)
#             dim=1
#         )
#         sim_with_numeric = sim_with_numeric_tensor[0].item()
# 
#         sim_without_numeric_tensor = torch.nn.functional.cosine_similarity(
#             output_without_numeric,  # shape (1, 384)
#             reference_embedding,     # shape (1, 384)
#             dim=1
#         )
#         sim_without_numeric = sim_without_numeric_tensor[0].item()
# 
#         numeric_impact = sim_with_numeric - sim_without_numeric
# 
#         integration_score = calculate_numeric_integration_score(expr, extract_numeric_features(expr).tolist())
#         numeric_impact = sim_with_numeric - sim_without_numeric
# 
#         results.append({
#             'expression': expr,
#             'similarity_with_numeric': sim_with_numeric,
#             'similarity_without_numeric': sim_without_numeric,
#             'numeric_impact': numeric_impact,
#             'integration_score': integration_score,
#             'numeric_features': extract_numeric_features(expr).tolist()
#         })
# 
#     avg_impact = sum(r['numeric_impact'] for r in results) / len(results)
#     avg_integration = sum(r['integration_score'] for r in results) / len(results)
# 
#     print(f"\n==== Numeric Integration Test Results ====")
#     print(f"Average numeric impact: {avg_impact:.4f}")
#     print(f"Average integration score: {avg_integration:.4f}")
#     print("\nDetailed results:")
# 
#     for i, r in enumerate(results):
#         print(f"\n{i+1}. Expression: {r['expression']}")
#         print(f"   Integration score: {r['integration_score']:.4f}")
#         print(f"   Numeric impact: {r['numeric_impact']:.4f}")
# 
#     return {
#         'results': results,
#         'avg_impact': avg_impact,
#         'avg_integration': avg_integration
#     }
# 
# # ================================
# # 9. MAIN EXECUTION
# # ================================
# 
# if __name__ == "__main__":
#     for directory in [CACHE_DIR, LOG_DIR]:
#         os.makedirs(directory, exist_ok=True)
# 
#     print("Émile5 Semantic ML Trainer")
#     print("=" * 40)
#     print(f"Device: {DEVICE}")
#     print(f"Model name: {MODEL_NAME}")
#     print(f"Model path: {MODEL_PATH}")
# 
#     if os.path.exists(MODEL_PATH):
#         print(f"Loading existing model from {MODEL_PATH}")
#         model = load_model(MODEL_PATH)
#     else:
#         print("Creating new semantic model with numeric integration capabilities")
#         model = SemioticExtractor(hidden_dim=384, numeric_dim=8).to(DEVICE)
# 
#     import sys
#     run_simulation = "--sim" in sys.argv or "-s" in sys.argv
#     run_training = "--train" in sys.argv or "-t" in sys.argv
#     run_testing = "--test" in sys.argv or "-e" in sys.argv
#     epochs = 5
# 
#     for i, arg in enumerate(sys.argv):
#         if arg == "--epochs" or arg == "-e":
#             if i + 1 < len(sys.argv):
#                 try:
#                     epochs = int(sys.argv[i + 1])
#                 except ValueError:
#                     pass
# 
#     if run_training:
#         print(f"\nTraining semantic model for {epochs} epochs")
#         if training_data:
#             train_semiotic_model(model, training_data, epochs=epochs)
#         else:
#             print("❌ No training data found. Please add .txt files to the data directory.")
# 
#     if run_simulation:
#         print("\nRunning integrated simulation and training")
#         refine_semiotic_model_with_simulation(model)
# 
#     if run_testing or not (run_training or run_simulation):
#         print("\nTesting numeric integration")
#         test_numeric_integration(model)
# 
#     print("\nTraining complete!")
#     print(f"Final model saved to {MODEL_PATH}")
# 
#     # Plot final training metrics
#     plot_training_metrics()
# 
# 
# def visualize_semantic_numeric_relationship(model, log_data, save_path=None):
#     """
#     Visualizes the relationship between symbolic expressions and numeric values.
#     """
#     model.eval()
# 
#     expressions = log_data.get('symbolic_expressions', [])
#     time_series = log_data.get('time_series', {})
# 
#     if not expressions or not time_series:
#         print("Insufficient data for visualization")
#         return
# 
#     keys_to_plot = []
#     for key in ['coherence', 'distinction', 'entropy', 'surplus']:
#         if key in time_series and len(time_series[key]) >= 5:
#             keys_to_plot.append(key)
# 
#     if not keys_to_plot:
#         print("No numeric time series with sufficient data points found")
#         return
# 
#     expression_texts = [e['expression'] for e in expressions]
#     expression_embeddings = []
# 
#     for text in expression_texts:
#         inputs = tokenizer(text, return_tensors="pt", padding=True,
#                            truncation=True, max_length=512).to(DEVICE)
#         numeric_features = extract_numeric_features(text).to(DEVICE)
# 
#         with torch.no_grad():
#             embedding = model(inputs['input_ids'], inputs['attention_mask'], numeric_features)
#             expression_embeddings.append(embedding.cpu().numpy())
# 
#     expression_embeddings = np.vstack(expression_embeddings)
# 
#     from sklearn.decomposition import PCA
#     pca = PCA(n_components=2)
#     embedding_2d = pca.fit_transform(expression_embeddings)
# 
#     fig, axes = plt.subplots(len(keys_to_plot), 1, figsize=(12, 4 * len(keys_to_plot)))
#     if len(keys_to_plot) == 1:
#         axes = [axes]
# 
#     for i, key in enumerate(keys_to_plot):
#         ax = axes[i]
#         values = time_series[key]
#         timestamps = time_series['timestamps'][:len(values)]
# 
#         ax.plot(range(len(values)), values, 'b-', label=f'{key} values')
# 
#         for j, expr in enumerate(expressions):
#             idx = find_closest_timestamp_index(expr['timestamp'], timestamps)
#             if idx is not None and idx < len(values):
#                 ax.scatter(idx, values[idx], c='red', s=50, zorder=5)
#                 if j % max(1, len(expressions) // 5) == 0:
#                     short_expr = expr['expression'][:30] + "..." if len(expr['expression'])>30 else expr['expression']
#                     ax.annotate(short_expr, (idx, values[idx]),
#                                 textcoords="offset points",
#                                 xytext=(0, 10),
#                                 ha='center',
#                                 fontsize=8,
#                                 bbox=dict(boxstyle="round,pad=0.3", fc="white", alpha=0.8))
# 
#         ax.set_title(f'{key.capitalize()} Values Over Time with Expression Markers')
#         ax.set_xlabel('Time Steps')
#         ax.set_ylabel(f'{key.capitalize()} Value')
#         ax.grid(True)
#         ax.legend()
# 
#     fig2, ax2 = plt.subplots(figsize=(10, 8))
#     timestamps = [e['timestamp'] for e in expressions]
#     min_time, max_time = min(timestamps), max(timestamps)
#     normalized_times = [(t - min_time) / (max_time - min_time) if max_time > min_time else 0.5 for t in timestamps]
# 
#     scatter = ax2.scatter(embedding_2d[:, 0], embedding_2d[:, 1],
#                           c=normalized_times, cmap='viridis',
#                           s=100, alpha=0.8)
# 
#     for i, expr in enumerate(expressions):
#         if i % max(1, len(expressions) // 10) == 0:
#             short_expr = expr['expression'][:20] + "..." if len(expr['expression'])>20 else expr['expression']
#             ax2.annotate(short_expr, (embedding_2d[i, 0], embedding_2d[i, 1]),
#                          textcoords="offset points",
#                          xytext=(5, 5),
#                          ha='left',
#                          fontsize=8,
#                          bbox=dict(boxstyle="round,pad=0.3", fc="white", alpha=0.7))
# 
#     ax2.set_title('Semantic Space of Expressions (PCA Projection)')
#     ax2.set_xlabel('Principal Component 1')
#     ax2.set_ylabel('Principal Component 2')
#     ax2.grid(True)
# 
#     cbar = plt.colorbar(scatter)
#     cbar.set_label('Time (normalized)')
#     plt.tight_layout()
# 
#     if save_path:
#         fig_path = save_path.replace('.png', '_timeseries.png')
#         fig2_path = save_path.replace('.png', '_semantic.png')
#         fig.savefig(fig_path)
#         fig2.savefig(fig2_path)
#         print(f"Visualizations saved to {fig_path} and {fig2_path}")
#     else:
#         plt.show()
# 
#     plt.close(fig)
#     plt.close(fig2)
# 
# 
# def find_closest_timestamp_index(target, timestamps):
#     if not timestamps:
#         return None
#     return min(range(len(timestamps)), key=lambda i: abs(timestamps[i] - target))
# 
# 
# def plot_numeric_integration_progress(training_metrics, save_path=None):
#     """
#     Plots the progress of numeric integration during training.
#     """
#     if 'numeric_integration_scores' not in training_metrics or not training_metrics['numeric_integration_scores']:
#         print("No numeric integration scores available for plotting")
#         return
# 
#     fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
# 
#     scores = training_metrics['numeric_integration_scores']
#     ax1.plot(range(len(scores)), scores, 'purple-', label='Numeric Integration')
# 
#     if len(scores) > 5:
#         window_size = min(10, len(scores)//2)
#         if window_size > 1:
#             avg = np.convolve(scores, np.ones(window_size)/window_size, mode='valid')
#             ax1.plot(range(window_size-1, len(scores)), avg, 'r--',
#                      label=f'{window_size}-point Moving Average')
# 
#     ax1.set_title('Numeric Integration Progress')
#     ax1.set_xlabel('Training Step')
#     ax1.set_ylabel('Integration Score')
#     ax1.grid(True)
#     ax1.legend()
# 
#     if 'recursive_losses' in training_metrics and training_metrics['recursive_losses']:
#         losses = training_metrics['recursive_losses']
#         losses = losses[:len(scores)]
# 
#         ax2.scatter(losses, scores, alpha=0.6, c=range(len(scores)), cmap='viridis')
#         ax2.set_title('Loss vs. Numeric Integration')
#         ax2.set_xlabel('Loss')
#         ax2.set_ylabel('Numeric Integration Score')
#         ax2.grid(True)
# 
#         norm = plt.Normalize(0, len(scores))
#         sm = plt.cm.ScalarMappable(cmap='viridis', norm=norm)
#         sm.set_array([])
#         cbar = plt.colorbar(sm, ax=ax2)
#         cbar.set_label('Training Step')
# 
#         from scipy import stats
#         if len(losses) > 1:
#             slope, intercept, r_value, p_value, std_err = stats.linregress(losses, scores)
#             x_line = np.array([min(losses), max(losses)])
#             y_line = slope * x_line + intercept
#             ax2.plot(x_line, y_line, 'r--', label=f'r={r_value:.2f}')
#             ax2.legend()
# 
#     plt.tight_layout()
#     if save_path:
#         plt.savefig(save_path)
#         print(f"Numeric integration progress plot saved to {save_path}")
#     else:
#         plt.show()
# 
#     plt.close(fig)
# 
# # Add to main script to use these functions
# if __name__ == "__main__":
#     if 'run_testing' in globals() and run_testing and log_paths:
#         print("\nAnalyzing simulation logs for numeric values...")
#         log_data = extract_numeric_properties_from_logs(log_paths[0])
# 
#         training_texts, numeric_features = create_training_dataset_with_numeric_values(log_data)
# 
#         print("\nVisualizing semantic-numeric relationship...")
#         timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
#         viz_path = os.path.join(LOG_DIR, f"semantic_numeric_viz_{timestamp}.png")
#         visualize_semantic_numeric_relationship(model, log_data, viz_path)
# 
#         num_progress_path = os.path.join(LOG_DIR, f"numeric_integration_progress_{timestamp}.png")
#         plot_numeric_integration_progress(training_metrics, save_path=num_progress_path)
#

"""### A: Semantic ML Mini"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile semantic_ml_mini.py
# 
# import os
# import re
# import torch
# import torch.nn as nn
# import torch.optim as optim
# import random
# import time
# from datetime import datetime
# import numpy as np
# 
# from transformers import AutoTokenizer, AutoModel
# from sentence_transformers import SentenceTransformer
# from
# 
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# MODEL_NAME = "bert-base-uncased"
# MODEL_PATH = "/content/emile_semantic_ml_mini.pt"
# 
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# transformer_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
# transformer_model.eval()
# 
# sentence_model = SentenceTransformer("all-MiniLM-L6-v2").to(DEVICE)
# sentence_model.eval()
# 
# class LogSemioticExtractor(nn.Module):
#     def __init__(self, hidden_dim=384, numeric_dim=8):
#         super().__init__()
#         self.encoder = transformer_model
#         self.text_fc = nn.Linear(self.encoder.config.hidden_size, hidden_dim)
#         self.activation = nn.Tanh()
#         self.dropout = nn.Dropout(0.1)
# 
#         self.numeric_enabled = numeric_dim > 0
#         if self.numeric_enabled:
#             self.numeric_fc = nn.Linear(numeric_dim, hidden_dim // 2)
#             self.fc_combined = nn.Linear(hidden_dim + (hidden_dim // 2), 384) # match ST dimension
#         else:
#             self.fc_direct = nn.Linear(hidden_dim, 384)
# 
#         self.norm = nn.LayerNorm(384)
# 
#     def forward(self, input_ids, attention_mask, numeric_values=None):
#         with torch.no_grad():
#             outputs = self.encoder(input_ids, attention_mask=attention_mask)
#         pooled_output = outputs.last_hidden_state[:, 0, :]
# 
#         text_features = self.activation(self.text_fc(pooled_output))
#         text_features = self.dropout(text_features)
# 
#         if self.numeric_enabled and numeric_values is not None:
#             numeric_feats = self.activation(self.numeric_fc(numeric_values))
#             if numeric_feats.ndim == 1:
#                 numeric_feats = numeric_feats.unsqueeze(0)
#             combined = torch.cat([text_features, numeric_feats], dim=1)
#             out = self.fc_combined(combined)
#         else:
#             out = self.fc_direct(text_features) if hasattr(self, 'fc_direct') else text_features
# 
#         return self.norm(out)
# 
# def extract_log_numeric_features(line: str, max_features=8):
#     """
#     Simple numeric extraction from a log line.
#     You can refine to parse (ms), CPU usage, etc.
#     """
#     numbers = re.findall(r'\b\d+\.?\d*\b', line)
#     raw = []
#     for num in numbers:
#         try:
#             raw.append(float(num))
#         except ValueError:
#             pass
# 
#     features = []
#     if raw:
#         features.extend([
#             len(raw),
#             np.mean(raw),
#             np.std(raw) if len(raw) > 1 else 0.0,
#             max(raw),
#             min(raw),
#             np.median(raw),
#             np.percentile(raw, 25) if len(raw)>3 else min(raw),
#             np.percentile(raw, 75) if len(raw)>3 else max(raw)
#         ])
#     # pad or truncate
#     features = features[:max_features]
#     if len(features) < max_features:
#         features.extend([0.0]*(max_features-len(features)))
#     return torch.tensor(features, dtype=torch.float32)
# 
# def build_log_dataset(log_paths):
#     data = []
#     for p in log_paths:
#         if not os.path.exists(p):
#             print(f"Missing: {p}")
#             continue
#         with open(p, 'r', encoding='utf-8') as f:
#             lines = f.readlines()
#         for line in lines:
#             line_text = line.strip()
#             if line_text:
#                 data.append(line_text)
#     return data
# 
# def train_on_logs(model, log_data, epochs=3, batch_size=16, lr=1e-5):
#     optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
#     loss_fn = nn.CosineEmbeddingLoss()
# 
#     random.shuffle(log_data)
#     model.train()
# 
#     for epoch in range(epochs):
#         print(f"Epoch {epoch+1}/{epochs}")
#         total_loss = 0.0
#         count = 0
# 
#         for i in range(0, len(log_data), batch_size):
#             chunk = log_data[i:i+batch_size]
#             if not chunk:
#                 continue
# 
#             batch_loss = 0.0
#             subcount = 0
# 
#             for line_text in chunk:
#                 inputs = tokenizer(line_text, return_tensors="pt", truncation=True, max_length=512).to(DEVICE)
#                 numeric_vals = extract_log_numeric_features(line_text).to(DEVICE)
# 
#                 outputs = model(inputs['input_ids'], inputs['attention_mask'], numeric_vals.unsqueeze(0))
# 
#                 with torch.no_grad():
#                     ref_emb = sentence_model.encode([line_text], convert_to_tensor=True).to(DEVICE)
# 
#                 # shape match
#                 min_sz = min(outputs.shape[0], ref_emb.shape[0])
#                 out_batch = outputs[:min_sz]
#                 ref_batch = ref_emb[:min_sz]
# 
#                 target = torch.ones(out_batch.shape[0], device=DEVICE)
#                 loss = loss_fn(out_batch, ref_batch, target)
# 
#                 optimizer.zero_grad()
#                 loss.backward()
#                 optimizer.step()
# 
#                 batch_loss += loss.item()
#                 subcount += 1
# 
#             if subcount>0:
#                 total_loss += batch_loss / subcount
#                 count += 1
# 
#         if count>0:
#             print(f"  Avg Loss: {total_loss / count:.4f}")
#         else:
#             print("  No valid lines")
# 
#     print("Done training logs")
# 
# if __name__ == "__main__":
#     # Example usage
#     log_files = [
#        "/content/logs/emile4_sim_20250228_044253.log",
#        # etc.
#     ]
#     log_data = build_log_dataset(log_files)
# 
#     # either load or new
#     if os.path.exists(MODEL_PATH):
#         cpt = torch.load(MODEL_PATH)
#         model = LogSemioticExtractor().to(DEVICE)
#         model.load_state_dict(cpt['model_state_dict'])
#     else:
#         model = LogSemioticExtractor(hidden_dim=384, numeric_dim=8).to(DEVICE)
# 
#     train_on_logs(model, log_data, epochs=2, batch_size=8, lr=1e-5)
# 
#     torch.save({'model_state_dict': model.state_dict()}, MODEL_PATH)
#     print(f"Saved to {MODEL_PATH}")
#

"""### B: Semantic ML Flash"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile semantic_ml_flash.py
# 
# import os
# import re
# import random
# import time
# import numpy as np
# 
# import torch
# import torch.nn as nn
# import torch.optim as optim
# 
# from transformers import AutoTokenizer, AutoModel
# from sentence_transformers import SentenceTransformer
# 
# # ================================
# # 1. CONFIGURATIONS
# # ================================
# DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")
# MODEL_NAME = "bert-base-uncased"
# MODEL_PATH = "/content/emile_semantic_ml_flash.pt"  # Where to save
# LOG_DIR = "/content/logs"  # Directory holding emile logs
# 
# # If you have a second reference model
# REFERENCE_MODEL = "/content/emile_semantic_ml_mini.pt"
# 
# # Create output directories if needed
# os.makedirs(LOG_DIR, exist_ok=True)
# 
# # ================================
# # 2. PRETRAINED MODELS
# # ================================
# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
# transformer_model = AutoModel.from_pretrained(MODEL_NAME).to(DEVICE)
# transformer_model.eval()
# 
# sentence_model = SentenceTransformer(REFERENCE_MODEL).to(DEVICE)
# sentence_model.eval()
# 
# # ================================
# # 3. CUSTOM MODEL
# # ================================
# 
# class EmileLogExtractor(nn.Module):
#     """
#     Merges text embeddings from a BERT-like model with numeric features
#     into a final 384-dim embedding (matching e.g. sentence transformers).
#     """
#     def __init__(self, hidden_dim=384, numeric_dim=8):
#         super().__init__()
#         self.encoder = transformer_model
# 
#         # Text path
#         self.fc_text = nn.Linear(self.encoder.config.hidden_size, hidden_dim)
#         self.activation = nn.Tanh()
#         self.dropout = nn.Dropout(0.1)
# 
#         # Numeric path
#         self.numeric_enabled = numeric_dim > 0
#         if self.numeric_enabled:
#             self.fc_numeric = nn.Linear(numeric_dim, hidden_dim // 2)
#             self.fc_combined = nn.Linear(hidden_dim + hidden_dim // 2, 384)
#         else:
#             # Just go directly to 384 if no numeric features
#             self.fc_direct = nn.Linear(hidden_dim, 384)
# 
#         self.norm = nn.LayerNorm(384)
# 
#     def forward(self, input_ids, attention_mask, numeric_values=None):
#         # Pass text through BERT
#         with torch.no_grad():
#             outputs = self.encoder(input_ids, attention_mask=attention_mask)
#         # Use [CLS] embedding
#         text_embedding = outputs.last_hidden_state[:, 0, :]  # shape: (batch, 768)
# 
#         # Map down
#         text_features = self.activation(self.fc_text(text_embedding))
#         text_features = self.dropout(text_features)  # shape: (batch, hidden_dim)
# 
#         if self.numeric_enabled and numeric_values is not None:
#             numeric_proj = self.activation(self.fc_numeric(numeric_values))
#             if numeric_proj.ndim == 1:  # if shape (dim,) => make it (1,dim)
#                 numeric_proj = numeric_proj.unsqueeze(0)
#             combined = torch.cat([text_features, numeric_proj], dim=1)
#             out = self.fc_combined(combined)  # (batch, 384)
#         else:
#             # direct path
#             out = self.fc_direct(text_features) if hasattr(self, 'fc_direct') else text_features
# 
#         return self.norm(out)  # final shape: (batch, 384)
# 
# # ================================
# # 4. LOG PARSING FOR NUMERIC FEATURES
# # ================================
# def extract_targeted_numeric_features(line: str):
#     """
#     Parse known patterns from your logs.
#     Example:
#      - "Pass: HighLevelSynthesis - 0.04816 (ms)"
#      - "Resource Usage => CPU: 8.7%, Memory: 5.2%, Avail: 80998 MB"
#      - "Distinction: 0.647, Coherence: 0.052, Entropy: 0.869"
#      - etc.
# 
#     We'll keep it simple with some placeholders:
#     """
#     # Initialize numeric features
#     # Suppose we want 8 "slots"
#     # slot 0: pass_time_ms
#     # slot 1: CPU usage
#     # slot 2: memory usage
#     # slot 3: distinction
#     # slot 4: coherence
#     # slot 5: surplus
#     # slot 6: random_value
#     # slot 7: something else
#     feats = [0.0]*8
# 
#     # 1) Look for pass time in (ms)
#     match_pass = re.search(r'Pass:\s*\S+\s*-\s*([\d.]+)\s*\(ms\)', line)
#     if match_pass:
#         feats[0] = float(match_pass.group(1))
# 
#     # 2) Resource usage
#     match_cpu = re.search(r'CPU:\s*([\d.]+)%', line)
#     if match_cpu:
#         feats[1] = float(match_cpu.group(1))
# 
#     match_mem = re.search(r'Memory:\s*([\d.]+)%', line)
#     if match_mem:
#         feats[2] = float(match_mem.group(1))
# 
#     # 3) Distinction, Coherence, Surplus
#     match_dist = re.search(r'Distinction:\s*([\d.]+)', line)
#     if match_dist:
#         feats[3] = float(match_dist.group(1))
# 
#     match_coh = re.search(r'Coherence:\s*([\d.]+)', line)
#     if match_coh:
#         feats[4] = float(match_coh.group(1))
# 
#     match_surp = re.search(r'surplus.?=?([\d.]+)', line)
#     if match_surp:
#         feats[5] = float(match_surp.group(1))
# 
#     # 4) A random example: "random_value=1.234"
#     match_rand = re.search(r'random_value.?=?([\d.]+)', line)
#     if match_rand:
#         feats[6] = float(match_rand.group(1))
# 
#     # We can store something else in slot 7 if we want, or keep it zero.
# 
#     return torch.tensor(feats, dtype=torch.float32)
# 
# # ================================
# # 5. BUILD DATASET FROM LOGS
# # ================================
# def gather_log_lines(log_dir):
#     """
#     Collect lines from all .log files in the directory.
#     """
#     lines_collected = []
#     for fname in os.listdir(log_dir):
#         if fname.endswith(".log"):
#             path = os.path.join(log_dir, fname)
#             with open(path, "r", encoding="utf-8") as f:
#                 for line in f:
#                     line = line.strip()
#                     if line:
#                         lines_collected.append(line)
#     return lines_collected
# 
# # ================================
# # 6. TRAINING FUNCTION
# # ================================
# def train_logs_only(model, log_lines, epochs=3, batch_size=16, lr=1e-5, max_length=512):
#     """
#     Each line => text + numeric => model => embedding =>
#     compare with reference embedding => CosineEmbeddingLoss.
#     """
#     optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=0.01)
#     loss_fn = nn.CosineEmbeddingLoss()
# 
#     random.shuffle(log_lines)
# 
#     for epoch in range(epochs):
#         print(f"Epoch {epoch+1}/{epochs}")
#         model.train()
#         total_loss = 0.0
#         num_batches = 0
# 
#         for i in range(0, len(log_lines), batch_size):
#             chunk = log_lines[i : i+batch_size]
#             if not chunk:
#                 continue
# 
#             # We'll accumulate loss across the chunk
#             batch_loss = 0.0
#             subcount = 0
# 
#             for line in chunk:
#                 # 1) Tokenize text
#                 inputs = tokenizer(line, return_tensors="pt",
#                                    truncation=True, max_length=max_length).to(DEVICE)
# 
#                 # 2) Extract numeric features
#                 numeric_feats = extract_targeted_numeric_features(line).to(DEVICE)
#                 # Expand to shape (batch=1, numeric_dim=8)
#                 numeric_feats = numeric_feats.unsqueeze(0)
# 
#                 # 3) Forward pass
#                 outputs = model(
#                     inputs['input_ids'],
#                     inputs['attention_mask'],
#                     numeric_feats
#                 )  # shape: (1, 384)
# 
#                 # 4) Reference embedding from your domain reference model
#                 with torch.no_grad():
#                     ref_emb = sentence_model.encode([line], convert_to_tensor=True).to(DEVICE)
#                     # shape: (1, 384)
# 
#                 # 5) cos embed loss
#                 # shape check
#                 if outputs.shape[0] != ref_emb.shape[0]:
#                     min_sz = min(outputs.shape[0], ref_emb.shape[0])
#                     outputs = outputs[:min_sz]
#                     ref_emb = ref_emb[:min_sz]
# 
#                 target = torch.ones(outputs.shape[0], device=DEVICE)
#                 loss = loss_fn(outputs, ref_emb, target)
# 
#                 # 6) Backprop
#                 optimizer.zero_grad()
#                 loss.backward()
#                 optimizer.step()
# 
#                 batch_loss += loss.item()
#                 subcount += 1
# 
#             if subcount>0:
#                 total_loss += (batch_loss/subcount)
#                 num_batches += 1
# 
#         if num_batches>0:
#             avg_loss = total_loss / num_batches
#             print(f"  Avg Loss: {avg_loss:.4f}")
#         else:
#             print("  No valid lines processed.")
# 
#     print("Training complete on logs-only data.")
# 
# # ================================
# # 7. MAIN EXECUTION
# # ================================
# if __name__ == "__main__":
#     # 1) Gather log lines
#     lines = gather_log_lines(LOG_DIR)
#     print(f"Collected {len(lines)} lines from logs at {LOG_DIR}.")
# 
#     if not lines:
#         print("No log lines found. Exiting.")
#         exit(0)
# 
#     # 2) Create or load model
#     if os.path.exists(MODEL_PATH):
#         print(f"Loading existing model from {MODEL_PATH} ...")
#         checkpoint = torch.load(MODEL_PATH)
#         # If you want to handle config, do so; else, assume default dims
#         model = EmileLogExtractor(hidden_dim=384, numeric_dim=8).to(DEVICE)
#         model.load_state_dict(checkpoint['model_state_dict'])
#         model.eval()
#     else:
#         print("Creating new logs-only model ...")
#         model = EmileLogExtractor(hidden_dim=384, numeric_dim=8).to(DEVICE)
# 
#     # 3) Train
#     train_logs_only(model, lines, epochs=2, batch_size=16, lr=1e-5)
# 
#     # 4) Save
#     tosave = {
#         'model_state_dict': model.state_dict(),
#         'timestamp': time.strftime("%Y%m%d-%H%M%S")
#     }
#     torch.save(tosave, MODEL_PATH)
#     print(f"Model saved to {MODEL_PATH}")
#

"""# Modules For Immediate Integration

## Agent Interface (For Immediate Intetgration)
"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile agent_interface.py
# """
# Agent Interface Module
# ---------------------
# This module provides adapter functions to ensure proper integration between
# the Émile-4 agent architecture and the semantic components.
# """
# 
# import sys
# import os
# import traceback
# from typing import Dict, Any, Optional
# 
# class AgentSemanticInterface:
#     """
#     Provides a clean interface for connecting agents with semantic models
#     by adapting method signatures and handling parameter mismatches.
#     """
# 
#     @staticmethod
#     def adapt_symbolic_generator(agent, semantic_integration):
#         """
#         Creates and attaches adapted symbolic generation methods to the agent
#         that handle parameter conversions between different method signatures.
# 
#         Args:
#             agent: The agent to modify
#             semantic_integration: The semantic integration component to connect
# 
#         Returns:
#             bool: True if successful, False otherwise
#         """
#         try:
#             # Store original methods for reference
#             agent._original_symbolic_generator = getattr(agent, 'generate_symbolic_expression', None)
#             if not agent._original_symbolic_generator:
#                 agent._original_symbolic_generator = getattr(agent, '_generate_symbolic_expression', None)
# 
#             # Create proper adapter based on agent interface
#             def symbolic_adapter(surplus=None, distinction=None, coherence=None, **kwargs):
#                 """
#                 Adapter to handle different parameter signatures.
# 
#                 This adapter can be called with either:
#                 - surplus, distinction, coherence (old interface)
#                 - direct kwargs matching the semantic integration interface
#                 """
#                 try:
#                     # Get current quantum metrics
#                     quantum_metrics = agent.quantum_state.get_quantum_metrics()
# 
#                     # If called with positional args or explicit params
#                     if surplus is not None and distinction is not None and coherence is not None:
#                         # Generate expression with semantic integration
#                         expression = semantic_integration.generate_symbolic_expression(
#                             surplus=surplus,
#                             distinction=distinction,
#                             coherence=coherence,
#                             quantum_metrics=quantum_metrics,
#                             **kwargs
#                         )
#                         return expression
#                     else:
#                         # If called without params, use current agent state
#                         if hasattr(agent, 'surplus_dynamics') and hasattr(agent.surplus_dynamics, 'surplus_state'):
#                             current_surplus = agent.surplus_dynamics.surplus_state.total_surplus()
#                         else:
#                             current_surplus = 1.0  # Default
# 
#                         if hasattr(agent, 'distinction_level'):
#                             current_distinction = agent.distinction_level
#                         else:
#                             current_distinction = 0.5  # Default
# 
#                         current_coherence = quantum_metrics.get('phase_coherence', 0.5)
# 
#                         # Call with current state
#                         expression = semantic_integration.generate_symbolic_expression(
#                             surplus=current_surplus,
#                             distinction=current_distinction,
#                             coherence=current_coherence,
#                             quantum_metrics=quantum_metrics,
#                             **kwargs
#                         )
#                         return expression
#                 except Exception as e:
#                     print(f"Error in symbolic adapter: {e}")
#                     traceback.print_exc()
# 
#                     # Fallback to original generator if available
#                     if agent._original_symbolic_generator:
#                         try:
#                             return agent._original_symbolic_generator()
#                         except:
#                             pass
# 
#                     # Last resort - return a simple expression
#                     return "Flux aligns with stability."
# 
#             # Attach adapter to agent
#             setattr(agent, 'generate_symbolic_expression', symbolic_adapter)
# 
#             # For internal methods that might be looking for _generate_symbolic_expression
#             if hasattr(agent, '_generate_symbolic_expression'):
#                 setattr(agent, '_generate_symbolic_expression', symbolic_adapter)
# 
#             return True
#         except Exception as e:
#             print(f"Error adapting symbolic generator: {e}")
#             traceback.print_exc()
#             return False
# 
# def create_fallback_symbolic_generator(agent):
#     """
#     Creates a fallback symbolic expression generator when semantic model is not available.
# 
#     Args:
#         agent: The agent to attach the fallback generator to
# 
#     Returns:
#         A function that generates fallback symbolic expressions
#     """
#     try:
#         # Import SymbolicOutput for fallback
#         from symbolic_output import SymbolicOutput
#         symbolic_system = SymbolicOutput()
# 
#         def fallback_generator(*args, **kwargs):
#             """Fallback symbolic generator when semantic model is not available"""
#             # Handle different call signatures
#             if len(args) >= 3:
#                 # Traditional call with positional arguments (surplus, distinction, coherence)
#                 return symbolic_system.generate_symbolic_expression(args[0], args[1], args[2])
#             elif all(k in kwargs for k in ['surplus', 'distinction', 'coherence']):
#                 # Call with keyword arguments
#                 return symbolic_system.generate_symbolic_expression(
#                     kwargs['surplus'],
#                     kwargs['distinction'],
#                     kwargs['coherence']
#                 )
#             else:
#                 # If called without enough parameters, extract from agent state
#                 if hasattr(agent, 'surplus_dynamics') and hasattr(agent.surplus_dynamics, 'surplus_state'):
#                     surplus = agent.surplus_dynamics.surplus_state.total_surplus()
#                 else:
#                     surplus = 1.5  # Default value
# 
#                 if hasattr(agent, 'distinction_level'):
#                     distinction = agent.distinction_level
#                 else:
#                     distinction = 0.5  # Default value
# 
#                 if hasattr(agent, 'quantum_state') and hasattr(agent.quantum_state, 'phase_coherence'):
#                     coherence = agent.quantum_state.phase_coherence
#                 else:
#                     coherence = 0.7  # Default value
# 
#                 return symbolic_system.generate_symbolic_expression(
#                     surplus, distinction, coherence
#                 )
# 
#         return fallback_generator
#     except Exception as e:
#         print(f"Error creating fallback generator: {e}")
#         # Last resort simple function
#         return lambda *args, **kwargs: "Flux aligns with stability."

"""## Semantic Integration (For Immediate Integration)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile semantic_integration.py
# 
# """
# Semantic Integration Module for Émile-4 Simulation
# --------------------------------------------------
# This module integrates the SemanticEnhancedOutput with the existing Émile architecture,
# providing bi-directional learning between symbolic expression and quantum/surplus dynamics.
# """
# 
# import os
# import time
# import logging
# import numpy as np
# import torch
# from typing import Dict, List, Optional, Tuple, Any, Union
# from collections import deque
# import json
# 
# # Import the SemanticEnhancedOutput class
# from semantic_enhanced_output import SemanticEnhancedOutput
# 
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.semantic_integration")
# 
# class SemanticIntegrationManager:
#     """
#     Manages the integration between the quantum/surplus system and the semantic model,
#     creating a bi-directional learning loop where:
#     1. The quantum/surplus system informs semantic expression generation
#     2. Semantic expressions provide feedback to the quantum/surplus system
#     """
#     def __init__(self,
#                  semantic_model_path=None,
#                  enable_bidirectional=True,
#                  device="cuda" if torch.cuda.is_available() else "cpu"):
#         """
#         Initialize the semantic integration manager.
# 
#         Args:
#             semantic_model_path: Path to the semantic model checkpoint (optional)
#             enable_bidirectional: Whether to enable bidirectional learning
#             device: Device for model computation
#         """
#         logger.info("Initializing SemanticIntegrationManager")
# 
#         # Create semantic output generator
#         self.symbolic_output = SemanticEnhancedOutput(
#             semantic_model_path=semantic_model_path,
#             device=device
#         )
# 
#         # Integration settings
#         self.enable_bidirectional = enable_bidirectional
#         self.device = device
#         self.semantic_model_loaded = self.symbolic_output.semantic_model is not None
# 
#         # Bidirectional learning parameters
#         self.feedback_strength = 0.2
#         self.semantic_coherence_threshold = 0.7
#         self.adaptation_rate = 0.05
# 
#         # History tracking
#         self.expression_history = []
#         self.coherence_history = deque(maxlen=100)
#         self.distinction_history = deque(maxlen=100)
#         self.integration_metrics = {
#             'semantic_coherence': [],
#             'quantum_influence': [],
#             'feedback_strength': [],
#             'expression_complexity': []
#         }
# 
#         # Statistics
#         self.total_expressions = 0
#         self.total_feedback_cycles = 0
#         self.emergence_events = 0
# 
#         # Integration state
#         self.last_expression_time = time.time()
#         self.last_expression = None
#         self.last_expression_components = {}
#         self.last_expression_coherence = 0.0
# 
#         logger.info(f"Semantic integration initialized. Model loaded: {self.semantic_model_loaded}")
# 
#     def generate_symbolic_expression(self,
#                                     surplus: float,
#                                     distinction: float,
#                                     coherence: float,
#                                     entropy: Optional[float] = None,
#                                     dimensionality: Optional[int] = None) -> str:
#         """
#         Generate a symbolic expression based on the current system state,
#         using the semantic-enhanced output module.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             entropy: Optional entropy metric
#             dimensionality: Optional detected dimensionality
# 
#         Returns:
#             A symbolic expression representing the current state
#         """
#         try:
#             # Generate expression using semantic model
#             expression = self.symbolic_output.generate_symbolic_expression(
#                 surplus=surplus,
#                 distinction=distinction,
#                 coherence=coherence,
#                 entropy=entropy,
#                 dimensionality=dimensionality
#             )
# 
#             # Get expression components for feedback loop
#             last_entry = self.symbolic_output.expression_history[-1] if self.symbolic_output.expression_history else None
# 
#             if last_entry:
#                 self.last_expression_components = last_entry.get('components', {})
#                 self.last_expression_coherence = self.symbolic_output._calculate_semantic_coherence(expression)
# 
#             # Update tracking
#             self.last_expression = expression
#             self.last_expression_time = time.time()
#             self.total_expressions += 1
# 
#             # Store in history with metrics
#             self.expression_history.append({
#                 'expression': expression,
#                 'metrics': {
#                     'surplus': surplus,
#                     'distinction': distinction,
#                     'coherence': coherence,
#                     'entropy': entropy,
#                     'dimensionality': dimensionality
#                 },
#                 'components': self.last_expression_components,
#                 'semantic_coherence': self.last_expression_coherence,
#                 'timestamp': self.last_expression_time
#             })
# 
#             # Update history collections
#             self.coherence_history.append(coherence)
#             self.distinction_history.append(distinction)
# 
#             # Update integration metrics
#             self.integration_metrics['semantic_coherence'].append(self.last_expression_coherence)
# 
#             logger.debug(f"Generated expression: {expression}")
#             return expression
# 
#         except Exception as e:
#             logger.error(f"Error generating symbolic expression: {e}")
#             return "System integration in flux."  # Safe fallback
# 
#     def handle_post_emergence(self,
#                           surplus: float,
#                           distinction: float,
#                           coherence: float,
#                           dimensionality: Optional[int] = None,
#                           entropy: Optional[float] = None) -> str:
#         """
#         Handle post-emergence expression generation with semantic enhancement.
# 
#         Args:
#             surplus: Current cognitive surplus level
#             distinction: Current distinction level
#             coherence: Current phase coherence
#             dimensionality: Optional detected dimensionality
#             entropy: Optional entropy metric
# 
#         Returns:
#             A symbolic expression representing the emergent state
#         """
#         try:
#             # Generate emergent expression
#             expression = self.symbolic_output.handle_post_emergence(
#                 surplus=surplus,
#                 distinction=distinction,
#                 coherence=coherence,
#                 dimensionality=dimensionality,
#                 entropy=entropy
#             )
# 
#             # Track emergence event
#             self.emergence_events += 1
#             self.last_expression = expression
#             self.last_expression_time = time.time()
# 
#             # Store in history
#             self.expression_history.append({
#                 'expression': expression,
#                 'type': 'emergence',
#                 'metrics': {
#                     'surplus': surplus,
#                     'distinction': distinction,
#                     'coherence': coherence,
#                     'entropy': entropy,
#                     'dimensionality': dimensionality
#                 },
#                 'timestamp': self.last_expression_time
#             })
# 
#             logger.info(f"Generated emergence expression: {expression}")
#             return expression
# 
#         except Exception as e:
#             logger.error(f"Error handling post-emergence: {e}")
#             return self.generate_symbolic_expression(surplus, distinction, coherence)
# 
#     def calculate_feedback_parameters(self) -> Dict[str, float]:
#         """
#         Calculate feedback parameters for the bidirectional learning loop.
# 
#         Returns:
#             Dictionary of feedback parameters
#         """
#         try:
#             # Default parameters
#             feedback = {
#                 'coherence_adjustment': 0.0,
#                 'distinction_adjustment': 0.0,
#                 'semantic_coherence': self.last_expression_coherence,
#                 'feedback_strength': self.feedback_strength
#             }
# 
#             # Skip if bidirectional learning is disabled
#             if not self.enable_bidirectional:
#                 return feedback
# 
#             # Skip if no expression history
#             if not self.expression_history:
#                 return feedback
# 
#             # Get semantic coherence from last expression
#             semantic_coherence = self.last_expression_coherence
# 
#             # Calculate historical averages if available
#             coherence_avg = np.mean(list(self.coherence_history)) if self.coherence_history else 0.5
#             distinction_avg = np.mean(list(self.distinction_history)) if self.distinction_history else 0.5
# 
#             # Calculate adjustments based on semantic coherence
#             # Higher coherence means the expression is semantically consistent
#             if semantic_coherence > self.semantic_coherence_threshold:
#                 # Encourage stability in high-coherence states
#                 coherence_adjustment = self.adaptation_rate * (semantic_coherence - self.semantic_coherence_threshold)
#                 distinction_adjustment = 0.0  # Maintain distinction
#             else:
#                 # Encourage exploration in low-coherence states
#                 coherence_adjustment = -self.adaptation_rate * (self.semantic_coherence_threshold - semantic_coherence)
#                 distinction_adjustment = self.adaptation_rate * 0.5  # Increase distinction slightly
# 
#             # Scale adjustments based on history stability
#             if len(self.coherence_history) > 10:
#                 coherence_stability = 1.0 - np.std(list(self.coherence_history)[-10:])
#                 distinction_stability = 1.0 - np.std(list(self.distinction_history)[-10:])
# 
#                 coherence_adjustment *= coherence_stability
#                 distinction_adjustment *= distinction_stability
# 
#             # Update feedback
#             feedback['coherence_adjustment'] = coherence_adjustment
#             feedback['distinction_adjustment'] = distinction_adjustment
#             feedback['semantic_coherence'] = semantic_coherence
# 
#             # Log the feedback cycle
#             self.total_feedback_cycles += 1
# 
#             return feedback
# 
#         except Exception as e:
#             logger.error(f"Error calculating feedback parameters: {e}")
#             return {
#                 'coherence_adjustment': 0.0,
#                 'distinction_adjustment': 0.0,
#                 'semantic_coherence': 0.5,
#                 'feedback_strength': 0.0
#             }
# 
#     def get_vocabulary_status(self) -> Dict[str, Any]:
#         """
#         Returns the current vocabulary status, including dynamic expansions
#         and semantic coherence metrics.
# 
#         Returns:
#             Dictionary containing vocabulary statistics
#         """
#         try:
#             # Count original and dynamic vocabulary
#             orig_counts = {
#                 'descriptors': len(self.state_descriptors),
#                 'relations': len(self.relations),
#                 'concepts': len(self.surplus_concepts),
#                 'modifiers': len(self.modifiers),
#                 'secondary': len(self.secondary_concepts),
#                 'numeric_modifiers': len(self.numeric_modifiers),
#                 'numeric_transformations': len(self.numeric_transformations)
#             }
# 
#             dynamic_counts = {
#                 'descriptors': len(self.dynamic_state_descriptors),
#                 'relations': len(self.dynamic_relations),
#                 'concepts': len(self.dynamic_surplus_concepts),
#                 'modifiers': len(self.dynamic_modifiers),
#                 'secondary': len(self.dynamic_secondary_concepts),
#                 'numeric_modifiers': len(self.dynamic_numeric_modifiers),
#                 'numeric_transformations': len(self.dynamic_numeric_transformations)
#             }
# 
#             # Calculate added terms
#             added_terms = {
#                 'descriptors': list(set(self.dynamic_state_descriptors) - set(self.state_descriptors)),
#                 'relations': list(set(self.dynamic_relations) - set(self.relations)),
#                 'concepts': list(set(self.dynamic_surplus_concepts) - set(self.surplus_concepts)),
#                 'modifiers': list(set(self.dynamic_modifiers) - set(self.modifiers)),
#                 'secondary': list(set(self.dynamic_secondary_concepts) - set(self.secondary_concepts)),
#                 'numeric_modifiers': list(set(self.dynamic_numeric_modifiers) - set(self.numeric_modifiers)),
#                 'numeric_transformations': list(set(self.dynamic_numeric_transformations) - set(self.numeric_transformations))
#             }
# 
#             # Get top coherence terms if semantic model is used
#             top_coherence = {}
#             if self.semantic_model:
#                 for vocab_type in ['descriptors', 'relations', 'concepts', 'modifiers',
#                                   'secondary', 'numeric_modifiers', 'numeric_transformations']:
#                     if vocab_type in self.vocabulary_coherence:
#                         # Sort by coherence and get top 5
#                         sorted_terms = sorted(
#                             self.vocabulary_coherence[vocab_type].items(),
#                             key=lambda x: x[1],
#                             reverse=True
#                         )[:5]
#                         top_coherence[vocab_type] = sorted_terms
# 
#             # Calculate numeric integration statistics
#             numeric_stats = {}
#             if hasattr(self, 'numeric_memory') and len(self.numeric_memory) > 0:
#                 # Calculate frequency of numeric inclusion
#                 expressions_with_numeric = sum(1 for e in self.expression_history
#                                             if 'components' in e and e['components'].get('numeric_value_str'))
# 
#                 numeric_stats = {
#                     'inclusion_rate': expressions_with_numeric / max(1, len(self.expression_history)),
#                     'num_memory_size': len(self.numeric_memory),
#                     'trends_tracked': len(self.numeric_trends) if hasattr(self, 'numeric_trends') else 0,
#                     'numeric_influence': self.numeric_influence
#                 }
# 
#                 # Add trend summary if available
#                 if hasattr(self, 'numeric_trends') and self.numeric_trends:
#                     trend_summary = {
#                         'increasing': sum(1 for t in self.numeric_trends.values() if t.get('increasing', False)),
#                         'decreasing': sum(1 for t in self.numeric_trends.values() if t.get('decreasing', False)),
#                         'oscillating': sum(1 for t in self.numeric_trends.values() if t.get('oscillating', False)),
#                         'stable': sum(1 for t in self.numeric_trends.values() if t.get('stable', False))
#                     }
#                     numeric_stats['trend_summary'] = trend_summary
# 
#                 # Add frequency analysis for numeric components if available
#                 if 'numeric_modifiers' in self.frequency_analysis:
#                     top_modifiers = sorted(
#                         self.frequency_analysis['numeric_modifiers'].items(),
#                         key=lambda x: x[1],
#                         reverse=True
#                     )[:3]
#                     numeric_stats['top_modifiers'] = top_modifiers
# 
#                 if 'numeric_transformations' in self.frequency_analysis:
#                     top_transforms = sorted(
#                         self.frequency_analysis['numeric_transformations'].items(),
#                         key=lambda x: x[1],
#                         reverse=True
#                     )[:3]
#                     numeric_stats['top_transforms'] = top_transforms
# 
#                 if 'selected_metrics' in self.frequency_analysis:
#                     top_metrics = sorted(
#                         self.frequency_analysis['selected_metrics'].items(),
#                         key=lambda x: x[1],
#                         reverse=True
#                     )[:3]
#                     numeric_stats['top_metrics'] = top_metrics
# 
#             return {
#                 'original_counts': orig_counts,
#                 'dynamic_counts': dynamic_counts,
#                 'added_terms': added_terms,
#                 'top_coherence_terms': top_coherence,
#                 'numeric_stats': numeric_stats,
#                 'dynamic_vocabulary_enabled': self.dynamic_vocabulary_enabled,
#                 'semantic_model_active': self.semantic_model is not None,
#                 'expression_complexity': float(self.expression_complexity),
#                 'vocabulary_sizes': {
#                     'total_original': sum(orig_counts.values()),
#                     'total_dynamic': sum(dynamic_counts.values()),
#                     'total_added': sum(len(terms) for terms in added_terms.values())
#                 }
#             }
# 
#         except Exception as e:
#             print(f"Error getting vocabulary status: {e}")
#             return {
#                 'error': str(e),
#                 'semantic_model_active': self.semantic_model is not None
#             }
# 
#     def get_integration_status(self) -> Dict[str, Any]:
#         """
#         Get the current status of semantic integration.
# 
#         Returns:
#             Dictionary with integration status
#         """
#         try:
#             # Calculate basic stats
#             if self.integration_metrics['semantic_coherence']:
#                 avg_coherence = np.mean(self.integration_metrics['semantic_coherence'])
#                 coherence_trend = np.mean(np.diff(self.integration_metrics['semantic_coherence'][-10:])) if len(self.integration_metrics['semantic_coherence']) > 10 else 0.0
#             else:
#                 avg_coherence = 0.0
#                 coherence_trend = 0.0
# 
#             # Get vocabulary status
#             vocab_status = self.symbolic_output.get_vocabulary_status()
# 
#             # Prepare status report
#             status = {
#                 'semantic_model_loaded': self.semantic_model_loaded,
#                 'bidirectional_enabled': self.enable_bidirectional,
#                 'total_expressions': self.total_expressions,
#                 'emergence_events': self.emergence_events,
#                 'total_feedback_cycles': self.total_feedback_cycles,
#                 'avg_semantic_coherence': avg_coherence,
#                 'coherence_trend': coherence_trend,
#                 'vocabulary_status': vocab_status,
#                 'last_expression_time': self.last_expression_time,
#                 'feedback_strength': self.feedback_strength,
#                 'adaptation_rate': self.adaptation_rate
#             }
# 
#             # Add most recent expression
#             if self.last_expression:
#                 status['last_expression'] = self.last_expression
# 
#             # Add emergence analysis if available
#             if self.emergence_events > 0:
#                 emergence_analysis = self.symbolic_output.analyze_emergence_patterns()
#                 status['emergence_analysis'] = emergence_analysis
# 
#             return status
# 
#         except Exception as e:
#             logger.error(f"Error getting integration status: {e}")
#             return {
#                 'error': str(e),
#                 'semantic_model_loaded': self.semantic_model_loaded
#             }
# 
#     def export_semantic_knowledge(self, filepath=None) -> Optional[str]:
#         """
#         Export semantic knowledge to a file.
# 
#         Args:
#             filepath: Optional filepath, or None to use automatic name
# 
#         Returns:
#             Path to exported file, or None if export failed
#         """
#         try:
#             # Use symbolic_output's export function
#             return self.semantic_enhanced_output.export_semantic_knowledge(filepath)
#         except Exception as e:
#             logger.error(f"Error exporting semantic knowledge: {e}")
#             return None
# 
#     def import_semantic_knowledge(self, filepath: str) -> bool:
#         """
#         Import semantic knowledge from a file.
# 
#         Args:
#             filepath: Path to the semantic knowledge file
# 
#         Returns:
#             Boolean indicating success
#         """
#         try:
#             # Use symbolic_output's import function
#             return self.symbolic_output.import_semantic_knowledge(filepath)
#         except Exception as e:
#             logger.error(f"Error importing semantic knowledge: {e}")
#             return False
# 
# # Adapter function for legacy code compatibility
# def generate_symbolic_expression(
#     surplus: float,
#     distinction: float,
#     coherence: float,
#     entropy: Optional[float] = None,
#     dimensionality: Optional[int] = None,
#     semantic_manager: Optional[SemanticIntegrationManager] = None
# ) -> str:
#     """
#     Adapter function to generate symbolic expressions using either the
#     semantic integration manager (if provided) or legacy fallback.
# 
#     Args:
#         surplus: Current cognitive surplus level
#         distinction: Current distinction level
#         coherence: Current phase coherence
#         entropy: Optional entropy metric
#         dimensionality: Optional detected dimensionality
#         semantic_manager: Optional semantic integration manager
# 
#     Returns:
#         A symbolic expression representing the current state
#     """
#     try:
#         if semantic_manager is not None:
#             return semantic_manager.generate_symbolic_expression(
#                 surplus=surplus,
#                 distinction=distinction,
#                 coherence=coherence,
#                 entropy=entropy,
#                 dimensionality=dimensionality
#             )
#         else:
#             # Legacy fallback if no semantic manager is provided
#             from symbolic_output import SymbolicOutput
#             symbolic_system = SymbolicOutput()
#             return symbolic_system.generate_symbolic_expression(
#                 surplus=surplus,
#                 distinction=distinction,
#                 coherence=coherence,
#                 entropy=entropy,
#                 dimensionality=dimensionality
#             )
#     except Exception as e:
#         logger.error(f"Error in adapter function: {e}")
#         return "System in flux."  # Safe fallback

"""### Quantum Semantic Coupling (For Immediate Integration)"""

# Commented out IPython magic to ensure Python compatibility.
# %%writefile quantum_semantic_coupling.py
# """
# Quantum-Semantic Coupling Module for Émile-4 Simulation
# ------------------------------------------------------
# This module handles the integration between quantum phase processing and
# semantic expression generation, creating a bidirectional feedback loop.
# """
# 
# import logging
# import numpy as np
# import torch
# from typing import Dict, List, Optional, Tuple, Any, Union
# from collections import deque
# 
# # Configure logging
# logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
# logger = logging.getLogger("emile4.quantum_semantic_coupling")
# 
# class QuantumSemanticCoupling:
#     """
#     Manages bidirectional coupling between quantum phase processing and
#     semantic expression, allowing the quantum state to influence expression
#     and expression coherence to feed back into quantum processing.
#     """
#     def __init__(self):
#         """Initialize quantum-semantic coupling system."""
#         self.coupling_strength = 0.3  # How strongly quantum state influences semantics
#         self.feedback_strength = 0.2  # How strongly semantics influence quantum state
# 
#         # Tracking parameters
#         self.coherence_coupling = 0.0
#         self.phase_coupling = 0.0
#         self.distinction_coupling = 0.0
# 
#         # Momentum tracking
#         self.coherence_momentum = 0.0
#         self.phase_momentum = 0.0
#         self.distinction_momentum = 0.0
# 
#         # History tracking
#         self.coupling_history = deque(maxlen=100)
#         self.feedback_history = deque(maxlen=100)
# 
#         # Adaptation parameters
#         self.adaptation_rate = 0.05
#         self.min_coupling = 0.1
#         self.max_coupling = 0.5
# 
#         logger.info("Quantum-Semantic coupling initialized")
# 
#     def calculate_semantic_coherence(self,
#                                     phase_coherence: float,
#                                     quantum_metrics: Dict[str, float]) -> float:
#         """
#         Calculate semantic coherence based on quantum metrics
# 
#         Args:
#             phase_coherence: Current phase coherence
#             quantum_metrics: Dictionary of quantum state metrics
# 
#         Returns:
#             Semantic coherence value
#         """
#         try:
#             # Extract relevant metrics with defaults
#             normalized_entropy = quantum_metrics.get('normalized_entropy', 0.5)
#             phase = quantum_metrics.get('phase', 0.0)
#             phase_distinction = quantum_metrics.get('phase_distinction', 0.5)
# 
#             # Calculate base semantic coherence
#             # High coherence, low entropy leads to higher semantic coherence
#             base_coherence = phase_coherence * (1.0 - normalized_entropy)
# 
#             # Apply phase influence
#             phase_factor = 0.5 + 0.5 * np.sin(phase)  # Oscillates between 0-1
# 
#             # Apply distinction coupling
#             distinction_factor = 0.5 + 0.5 * phase_distinction
# 
#             # Calculate final coherence with coupling strengths
#             semantic_coherence = (
#                 0.5 * base_coherence +
#                 0.25 * phase_factor * self.phase_coupling +
#                 0.25 * distinction_factor * self.distinction_coupling
#             )
# 
#             # Ensure valid range
#             semantic_coherence = np.clip(semantic_coherence, 0.0, 1.0)
# 
#             return float(semantic_coherence)
# 
#         except Exception as e:
#             logger.error(f"Error calculating semantic coherence: {e}")
#             return 0.5  # Default middle value
# 
#     def calculate_quantum_feedback(self,
#                                  semantic_coherence: float,
#                                  expression_components: Dict[str, Any]) -> Dict[str, float]:
#         """
#         Calculate quantum feedback parameters based on semantic expression
# 
#         Args:
#             semantic_coherence: Current semantic coherence value
#             expression_components: Components of the semantic expression
# 
#         Returns:
#             Dictionary of feedback parameters
#         """
#         try:
#             # Initialize feedback parameters
#             feedback = {
#                 'phase_adjustment': 0.0,
#                 'coherence_adjustment': 0.0,
#                 'distinction_adjustment': 0.0
#             }
# 
#             # Skip feedback if expression components are missing
#             if not expression_components:
#                 return feedback
# 
#             # Extract expression components
#             descriptor = expression_components.get('descriptor', '')
#             relation = expression_components.get('relation', '')
#             concept = expression_components.get('concept', '')
# 
#             # Calculate descriptor-based phase adjustment
#             # Different descriptors influence the quantum phase differently
#             phase_map = {
#                 'Flux': 0.1,
#                 'Equilibrium': -0.1,
#                 'Distinction': 0.2,
#                 'Recursion': 0.15,
#                 'Convergence': -0.15,
#                 'Divergence': 0.25,
#                 'Resonance': -0.2,
#                 'Coherence': -0.1,
#                 'Entanglement': 0.3,
#                 'Superposition': 0.25,
#                 'Bifurcation': 0.2,
#                 'Integration': -0.15
#             }
# 
#             # Calculate feedback based on expression components
#             phase_adj = phase_map.get(descriptor, 0.0) * self.feedback_strength
# 
#             # Higher semantic coherence reinforces quantum coherence
#             coherence_adj = (semantic_coherence - 0.5) * self.feedback_strength
# 
#             # Certain concepts influence distinction level
#             distinction_map = {
#                 'distinction': 0.2,
#                 'emergence': 0.15,
#                 'complexity': 0.1,
#                 'recursion': 0.1,
#                 'entropy': -0.1,
#                 'stability': -0.15
#             }
# 
#             # Check if any distinction-affecting concept is in the expression
#             distinction_adj = 0.0
#             for key, value in distinction_map.items():
#                 if key in concept.lower():
#                     distinction_adj += value * self.feedback_strength
# 
#             # Limit adjustment ranges
#             phase_adj = np.clip(phase_adj, -0.1, 0.1)
#             coherence_adj = np.clip(coherence_adj, -0.05, 0.05)
#             distinction_adj = np.clip(distinction_adj, -0.05, 0.05)
# 
#             # Update feedback parameters
#             feedback['phase_adjustment'] = phase_adj
#             feedback['coherence_adjustment'] = coherence_adj
#             feedback['distinction_adjustment'] = distinction_adj
# 
#             # Record feedback
#             self.feedback_history.append({
#                 'semantic_coherence': semantic_coherence,
#                 'phase_adjustment': phase_adj,
#                 'coherence_adjustment': coherence_adj,
#                 'distinction_adjustment': distinction_adj
#             })
# 
#             return feedback
# 
#         except Exception as e:
#             logger.error(f"Error calculating quantum feedback: {e}")
#             return {
#                 'phase_adjustment': 0.0,
#                 'coherence_adjustment': 0.0,
#                 'distinction_adjustment': 0.0
#             }
# 
#     def update_coupling_parameters(self, phase_coherence: float, distinction_level: float):
#         """
#         Update coupling parameters based on system state
# 
#         Args:
#             phase_coherence: Current phase coherence
#             distinction_level: Current distinction level
#         """
#         try:
#             # Update coherence coupling
#             target_coherence_coupling = np.clip(phase_coherence,
#                                               self.min_coupling,
#                                               self.max_coupling)
#             self.coherence_coupling = (
#                 0.9 * self.coherence_coupling +
#                 0.1 * target_coherence_coupling
#             )
# 
#             # Update distinction coupling
#             target_distinction_coupling = np.clip(distinction_level,
#                                                 self.min_coupling,
#                                                 self.max_coupling)
#             self.distinction_coupling = (
#                 0.9 * self.distinction_coupling +
#                 0.1 * target_distinction_coupling
#             )
# 
#             # Update phase coupling (tends toward middle value)
#             # This creates a balanced influence of phase on semantics
#             target_phase_coupling = 0.3
#             self.phase_coupling = (
#                 0.9 * self.phase_coupling +
#                 0.1 * target_phase_coupling
#             )
# 
#             # Update momenta
#             self.coherence_momentum = 0.9 * self.coherence_momentum + 0.1 * (target_coherence_coupling - self.coherence_coupling)
#             self.distinction_momentum = 0.9 * self.distinction_momentum + 0.1 * (target_distinction_coupling - self.distinction_coupling)
# 
#             # Record coupling state
#             self.coupling_history.append({
#                 'coherence_coupling': self.coherence_coupling,
#                 'distinction_coupling': self.distinction_coupling,
#                 'phase_coupling': self.phase_coupling,
#                 'coherence_momentum': self.coherence_momentum,
#                 'distinction_momentum': self.distinction_momentum
#             })
# 
#         except Exception as e:
#             logger.error(f"Error updating coupling parameters: {e}")
# 
#     def get_coupling_metrics(self) -> Dict[str, float]:
#         """
#         Get current coupling metrics.
# 
#         Returns:
#             Dictionary of coupling metrics
#         """
#         try:
#             metrics = {
#                 'coherence_coupling': float(self.coherence_coupling),
#                 'distinction_coupling': float(self.distinction_coupling),
#                 'phase_coupling': float(self.phase_coupling),
#                 'coherence_momentum': float(self.coherence_momentum),
#                 'distinction_momentum': float(self.distinction_momentum),
#                 'coupling_strength': float(self.coupling_strength),
#                 'feedback_strength': float(self.feedback_strength)
#             }
# 
#             # Add trend metrics if history exists
#             if len(self.coupling_history) > 10:
#                 recent = list(self.coupling_history)[-10:]
# 
#                 # Calculate coupling stability (1.0 = stable, 0.0 = unstable)
#                 coherence_variance = np.var([r['coherence_coupling'] for r in recent])
#                 distinction_variance = np.var([r['distinction_coupling'] for r in recent])
# 
#                 stability = 1.0 - np.clip(coherence_variance + distinction_variance, 0.0, 1.0)
#                 metrics['coupling_stability'] = float(stability)
# 
#                 # Calculate feedback effectiveness if feedback history exists
#                 if len(self.feedback_history) > 10:
#                     recent_feedback = list(self.feedback_history)[-10:]
# 
#                     # Effective feedback has consistent direction and appropriate magnitude
#                     coherence_adj_consistency = np.mean([r['coherence_adjustment'] for r in recent_feedback])
#                     coherence_adj_variance = np.var([r['coherence_adjustment'] for r in recent_feedback])
# 
#                     # Higher consistency magnitude and lower variance indicates more effective feedback
#                     effectiveness = np.abs(coherence_adj_consistency) / (coherence_adj_variance + 0.01)
#                     metrics['feedback_effectiveness'] = float(np.clip(effectiveness, 0.0, 1.0))
# 
#             return metrics
# 
#         except Exception as e:
#             logger.error(f"Error getting coupling metrics: {e}")
#             return {
#                 'coherence_coupling': self.coherence_coupling,
#                 'distinction_coupling': self.distinction_coupling,
#                 'phase_coupling': self.phase_coupling,
#                 'error': str(e)
#             }
#

"""# 24. Run Émile5"""

#!python simulation_runner_logs.py